{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import dotenv\n",
    "import os\n",
    "from libraries.client_stashapp import get_stashapp_client, StashAppClient\n",
    "from libraries.StashDbClient import StashDbClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "stash_client = StashAppClient()\n",
    "stash_raw_client = get_stashapp_client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging scenes with same StashDB ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce functions\n",
    "def compare_and_merge_scenes(scenes):\n",
    "    \"\"\"Compare and merge multiple scenes, returning a merged version with the most complete metadata\"\"\"\n",
    "    \n",
    "    # Helper function to format value for display\n",
    "    def format_value(val):\n",
    "        if isinstance(val, list):\n",
    "            return f\"[{len(val)} items]\" if len(val) > 3 else str(val)\n",
    "        return str(val) if val is not None else \"None\"\n",
    "    \n",
    "    # Helper function to get tag names for display\n",
    "    def get_tag_names(tags):\n",
    "        return sorted([t['name'] for t in tags])\n",
    "    \n",
    "    # Helper function to get performer names for display\n",
    "    def get_performer_names(performers):\n",
    "        return sorted([p.get('name', f\"ID: {p['id']}\") for p in performers])\n",
    "    \n",
    "    # Fields to compare (excluding technical details and paths)\n",
    "    fields_to_compare = [\n",
    "        'title', 'code', 'details', 'director', 'date',\n",
    "        'rating100', 'organized', 'o_counter', 'organized', 'studio_id',\n",
    "        'gallery_ids', 'play_duration', 'play_count'\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Scene Comparison ===\\n\")\n",
    "    \n",
    "    # Compare basic fields\n",
    "    merged = {}\n",
    "    for field in fields_to_compare:\n",
    "        values = [scene.get(field) for scene in scenes]\n",
    "        \n",
    "        # For numerical fields, handle None values specially\n",
    "        if field in ['o_counter', 'play_count', 'play_duration', 'rating100']:\n",
    "            if all(v is None for v in values):\n",
    "                merged[field] = None\n",
    "            else:\n",
    "                merged[field] = max(v or 0 for v in values)\n",
    "        else:\n",
    "            # Choose the first non-None value\n",
    "            merged[field] = next((v for v in values if v is not None), None)\n",
    "        \n",
    "        # Only show detailed comparison if values differ\n",
    "        if len(set(str(v) for v in values)) > 1:\n",
    "            print(f\"{field}:\")\n",
    "            for i, val in enumerate(values, 1):\n",
    "                print(f\"  Scene {i}: {format_value(val)}\")\n",
    "            print(f\"  Merged: {format_value(merged[field])}\\n\")\n",
    "        else:\n",
    "            print(f\"{field}: {format_value(values[0])}\")\n",
    "    \n",
    "    print(\"\\n=== Special Fields ===\\n\")\n",
    "    \n",
    "    # Handle tags\n",
    "    all_tags = []\n",
    "    print(\"tags:\")\n",
    "    for i, scene in enumerate(scenes, 1):\n",
    "        tags = scene.get('tags', [])\n",
    "        all_tags.extend(tags)\n",
    "        print(f\"  Scene {i}: {get_tag_names(tags)}\")\n",
    "    \n",
    "    merged_tags = list({t['id']: t for t in all_tags}.values())\n",
    "    print(f\"  Merged: {get_tag_names(merged_tags)}\\n\")\n",
    "    merged['tag_ids'] = [t['id'] for t in merged_tags]\n",
    "    \n",
    "    # Handle performers\n",
    "    all_performers = []\n",
    "    print(\"performers:\")\n",
    "    for i, scene in enumerate(scenes, 1):\n",
    "        performers = scene.get('performers', [])\n",
    "        all_performers.extend(performers)\n",
    "        print(f\"  Scene {i}: {get_performer_names(performers)}\")\n",
    "    \n",
    "    merged_performers = list({p['id']: p for p in all_performers}.values())\n",
    "    print(f\"  Merged: {get_performer_names(merged_performers)}\\n\")\n",
    "    merged['performer_ids'] = [p['id'] for p in merged_performers]\n",
    "    \n",
    "    # Handle other special fields\n",
    "    special_fields = {\n",
    "        'stash_ids': lambda scenes: list({(s['endpoint'], s['stash_id']): s \n",
    "                                        for scene in scenes \n",
    "                                        for s in scene.get('stash_ids', [])}.values()),\n",
    "        'scene_markers': lambda scenes: sorted([m for scene in scenes \n",
    "                                              for m in scene.get('scene_markers', [])],\n",
    "                                             key=lambda m: m['seconds']),\n",
    "        'o_history': lambda scenes: sorted(list(set([h for scene in scenes \n",
    "                                                   for h in scene.get('o_history', [])]))),\n",
    "        'play_history': lambda scenes: sorted(list(set([h for scene in scenes \n",
    "                                                      for h in scene.get('play_history', [])]))),\n",
    "        'urls': lambda scenes: sorted(list(set([u for scene in scenes \n",
    "                                              for u in scene.get('urls', [])])))\n",
    "    }\n",
    "    \n",
    "    for field, merge_func in special_fields.items():\n",
    "        values = [scene.get(field, []) for scene in scenes]\n",
    "        merged_items = merge_func(scenes)\n",
    "        \n",
    "        if any(len(v) != len(values[0]) for v in values) or len(merged_items) != len(values[0]):\n",
    "            print(f\"{field}:\")\n",
    "            for i, items in enumerate(values, 1):\n",
    "                print(f\"  Scene {i}: {format_value(items) if len(items) <= 3 else f'{len(items)} items'}\")\n",
    "            print(f\"  Merged: {format_value(merged_items) if len(merged_items) <= 3 else f'{len(merged_items)} items'}\\n\")\n",
    "        else:\n",
    "            print(f\"{field}: {len(values[0])} items\")\n",
    "        \n",
    "        merged[field] = merged_items\n",
    "    \n",
    "    # Keep other fields from first scene that we haven't explicitly handled\n",
    "    for key in scenes[0]:\n",
    "        if key not in merged and key not in ['files', 'paths', 'sceneStreams', 'tags', 'performers']:\n",
    "            merged[key] = scenes[0][key]\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic info for all scenes\n",
    "all_scenes_basic_info = stash_raw_client.find_scenes({}, fragment=\"id title studio { id name } stash_ids { endpoint stash_id }\")\n",
    "\n",
    "# Map the data with explicit type handling\n",
    "all_scenes_basic_info_mapped = [\n",
    "    {\n",
    "        \"stashapp_id\": str(scene['id']),  # Ensure ID is string\n",
    "        \"stashapp_title\": str(scene['title']) if scene['title'] else \"\",  # Handle None titles\n",
    "        \"stashapp_studio_id\": str(scene['studio']['id']) if scene['studio'] else None,  # Ensure studio ID is string\n",
    "        \"stashapp_studio_name\": str(scene['studio']['name']) if scene['studio'] else None,\n",
    "        \"stashapp_stashdb_id\": next((str(stash_id['stash_id']) for stash_id in scene['stash_ids'] \n",
    "                           if stash_id['endpoint'] == 'https://stashdb.org/graphql'), None)\n",
    "    }\n",
    "    for scene in all_scenes_basic_info\n",
    "]\n",
    "\n",
    "# Create DataFrame with increased schema inference length\n",
    "all_scenes_basic_info_mapped_df = pl.DataFrame(\n",
    "    all_scenes_basic_info_mapped,\n",
    "    infer_schema_length=10000  # Increase schema inference length\n",
    ")\n",
    "\n",
    "scenes_with_dupe_stashdb_ids_basic_info = all_scenes_basic_info_mapped_df.filter(pl.col('stashapp_stashdb_id').is_in(all_scenes_basic_info_mapped_df.group_by('stashapp_stashdb_id').agg(pl.len().alias('count')).filter(pl.col('count') > 1).get_column('stashapp_stashdb_id'))).sort('stashapp_stashdb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupe_stashdb_ids_basic_info\n",
    "print(f\"Studios with duplicate scenes: {scenes_with_dupe_stashdb_ids_basic_info.select(pl.col('stashapp_studio_id')).unique().to_series().to_list()}\")\n",
    "\n",
    "studio_id = scenes_with_dupe_stashdb_ids_basic_info.select(pl.col('stashapp_studio_id')).unique().to_series().to_list()[0]\n",
    "print(f\"Studio ID: {studio_id}\")\n",
    "\n",
    "all_scenes = stash_client.find_scenes_by_studio(studio_id)\n",
    "print(f\"Number of scenes: {len(all_scenes)}\")\n",
    "\n",
    "scenes_with_dupe_stashdb_ids = all_scenes.filter(pl.col('stashapp_stashdb_id').is_in(all_scenes.group_by('stashapp_stashdb_id').agg(pl.len().alias('count')).filter(pl.col('count') > 1).get_column('stashapp_stashdb_id'))).sort('stashapp_stashdb_id')\n",
    "print(f\"Number of scenes with duplicate StashDB IDs: {len(scenes_with_dupe_stashdb_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_scenes = scenes_with_dupe_stashdb_ids.group_by('stashapp_stashdb_id').agg([\n",
    "    pl.col('stashapp_id').alias('scene_ids'),\n",
    "    pl.col('stashapp_title').alias('titles')\n",
    "])\n",
    "\n",
    "grouped_scenes = grouped_scenes\n",
    "grouped_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge scenes with same StashDB ID\n",
    "for group in grouped_scenes.iter_rows(named=True):\n",
    "    stashdb_id = group['stashapp_stashdb_id']\n",
    "    scene_ids = group['scene_ids']\n",
    "    titles = group['titles']\n",
    "    \n",
    "    print(f\"\\nProcessing group with StashDB ID: {stashdb_id}\")\n",
    "    print(f\"Scene titles: {titles[0]}\")  # All titles should be the same\n",
    "    print(f\"Scene IDs: {scene_ids}\")\n",
    "    \n",
    "    # Get full scene data for each ID in the group\n",
    "    scenes = [stash_raw_client.find_scene(str(scene_id)) for scene_id in scene_ids]\n",
    "    if not scenes:\n",
    "        raise ValueError(f\"No scenes found for IDs: {scene_ids}\")\n",
    "    \n",
    "    if len(scenes) < 1:\n",
    "        raise ValueError(f\"No scenes found for IDs: {scene_ids}\")\n",
    "    \n",
    "    if len(scenes) < 2:\n",
    "        raise ValueError(f\"Only one scene found for IDs: {scene_ids}\")\n",
    "    \n",
    "    # Sort scenes by ID to determine source and destination\n",
    "    sorted_scene_ids = [str(scene_id) for scene_id in sorted([int(scene['id']) for scene in scenes])]\n",
    "    destination_scene_id = sorted_scene_ids[0]\n",
    "    source_scene_ids = sorted_scene_ids[1:]\n",
    "    \n",
    "    print(f\"Merging scenes {source_scene_ids} into {destination_scene_id}\")\n",
    "    \n",
    "    # Compare and merge the scenes\n",
    "    merged_scene = compare_and_merge_scenes(scenes)\n",
    "    \n",
    "    # Prepare the merge input\n",
    "    scene_merge_input = {\n",
    "        \"source\": source_scene_ids,\n",
    "        \"destination\": destination_scene_id,\n",
    "        \"values\": {\n",
    "            \"id\": destination_scene_id,\n",
    "            \"title\": merged_scene['title'],\n",
    "            \"code\": merged_scene['code'],\n",
    "            \"details\": merged_scene['details'],\n",
    "            \"director\": merged_scene['director'],\n",
    "            \"urls\": merged_scene['urls'],\n",
    "            \"date\": merged_scene['date'],\n",
    "            \"rating100\": merged_scene['rating100'],\n",
    "            \"o_counter\": merged_scene['o_counter'],\n",
    "            \"organized\": merged_scene['organized'],\n",
    "            \"gallery_ids\": merged_scene['gallery_ids'] if 'gallery_ids' in merged_scene and merged_scene['gallery_ids'] is not None else [],\n",
    "            \"performer_ids\": merged_scene['performer_ids'],\n",
    "            \"tag_ids\": merged_scene['tag_ids'],\n",
    "            \"stash_ids\": merged_scene['stash_ids'],\n",
    "            \"play_duration\": merged_scene['play_duration'],\n",
    "            \"play_count\": merged_scene['play_count'],\n",
    "        },\n",
    "        \"play_history\": True, \n",
    "        \"o_history\": True,\n",
    "    }\n",
    "    \n",
    "    # Execute the merge\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        mutation SceneMerge($merge_input: SceneMergeInput!) {\n",
    "            sceneMerge(input: $merge_input) {\n",
    "                id\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        result = stash_raw_client.call_GQL(query, {\"merge_input\": scene_merge_input})\n",
    "        print(f\"Successfully merged scenes: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error merging scenes: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding scenes with multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "oshash_and_duration_match_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: OSHASH And Duration Match\" })\n",
    "duration_match_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: Duration Match\" })\n",
    "scenes_with_multiple_versions = stash_raw_client.find_tag({ \"name\": \"Scene: Multiple Versions\" })\n",
    "duration_mismatch_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: Duration Mismatch\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupes = stash_raw_client.find_scenes({ \n",
    "  \"file_count\": {\n",
    "    \"modifier\": \"GREATER_THAN\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"tags\": {\n",
    "    \"value\": [],\n",
    "    \"modifier\": \"INCLUDES\",\n",
    "    \"excludes\": [scenes_with_multiple_versions['id']]\n",
    "  }\n",
    "}, fragment=\"id title date studio { name } files { id duration path width height size fingerprints { type value } }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene['files']):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash'), None)\n",
    "        phash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'phash'), None)\n",
    "        \n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'date': scene['date'],\n",
    "            'studio_name': scene['studio']['name'] if scene['studio'] else None,\n",
    "            'file_id': file['id'],\n",
    "            'file_path': file['path'],\n",
    "            'resolution_width': file['width'],\n",
    "            'resolution_height': file['height'],\n",
    "            'size': file['size'],\n",
    "            'duration': file['duration'],\n",
    "            'oshash': oshash,\n",
    "            'phash': phash,\n",
    "            'is_primary': i == 0  # True if this is the first file in the scene's files list\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create Polars DataFrame\n",
    "scenes_with_multiple_files_df = pl.DataFrame(file_records)\n",
    "scenes_with_multiple_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting files where OSHAS is identical to another file in the scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Filter to only show rows that have matching oshash values with other rows\n",
    "duplicate_files_df = scenes_with_multiple_files_df.filter(\n",
    "    pl.col('oshash').is_in(\n",
    "        scenes_with_multiple_files_df.group_by('oshash')\n",
    "        .agg(pl.len().alias('dupe_count'))  # Changed to use len() and a unique alias\n",
    "        .filter(pl.col('dupe_count') > 1)\n",
    "        .get_column('oshash')\n",
    "    )\n",
    ").sort(['oshash', 'file_id'])\n",
    "\n",
    "duplicate_files_df = duplicate_files_df.with_columns(pl.col('file_path').map_elements(lambda x: os.path.exists(x), return_dtype=pl.Boolean).alias('file_path_exists'))\n",
    "\n",
    "print(\"\\nFiles with duplicate oshash values:\")\n",
    "print(duplicate_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by scene_id and oshash to find duplicates within each scene\n",
    "grouped_files = duplicate_files_df.group_by(['scene_id', 'oshash']).agg([\n",
    "    pl.col('file_id').min().alias('keep_file_id'),  # File to keep\n",
    "    pl.col('file_id').alias('all_file_ids'),        # All file IDs\n",
    "    pl.col('file_path').alias('all_file_paths'),    # All file paths\n",
    "    pl.col('size').alias('all_file_sizes'),         # File sizes\n",
    "    pl.col('file_path_exists').alias('all_file_exists'),  # Path existence check\n",
    "    pl.col('is_primary').alias('all_is_primary')    # Primary file flags\n",
    "])\n",
    "\n",
    "# Lists to store kept and deleted file information\n",
    "kept_files = []\n",
    "files_to_delete = []\n",
    "\n",
    "# Print summary and collect file information\n",
    "print(\"\\nFiles to be deleted:\")\n",
    "for row in grouped_files.iter_rows(named=True):\n",
    "    file_ids = row['all_file_ids']\n",
    "    file_paths = row['all_file_paths']\n",
    "    file_sizes = row['all_file_sizes']\n",
    "    is_primary = row['all_is_primary']\n",
    "    \n",
    "    # Find primary file index if it exists\n",
    "    primary_indices = [i for i, p in enumerate(is_primary) if p]\n",
    "    if primary_indices:\n",
    "        # Keep the primary file\n",
    "        keep_index = primary_indices[0]\n",
    "        keep_file_id = file_ids[keep_index]\n",
    "    else:\n",
    "        # If no primary file, keep the one with lowest file_id\n",
    "        keep_file_id = row['keep_file_id']\n",
    "        keep_index = file_ids.index(keep_file_id)\n",
    "    \n",
    "    kept_files.append({\n",
    "        'scene_id': row['scene_id'],\n",
    "        'file_id': keep_file_id,\n",
    "        'file_path': file_paths[keep_index],\n",
    "        'size': file_sizes[keep_index],\n",
    "        'is_primary': is_primary[keep_index]\n",
    "    })\n",
    "    \n",
    "    # Get indices of files to delete (all except kept file, never delete primary files)\n",
    "    delete_indices = [i for i, (file_id, p) in enumerate(zip(file_ids, is_primary)) \n",
    "                     if file_id != keep_file_id and not p]\n",
    "    \n",
    "    if delete_indices:  # Only show if there are files to delete\n",
    "        primary_status = \" (Primary)\" if is_primary[keep_index] else \"\"\n",
    "        print(f\"\\nScene {row['scene_id']} - Keeping{primary_status}: {file_paths[keep_index]} (ID: {keep_file_id}, Size: {file_sizes[keep_index]:,} bytes) Exists: {os.path.exists(file_paths[keep_index])}\")\n",
    "        for idx in delete_indices:\n",
    "            print(f\"  Will delete: {file_paths[idx]} (ID: {file_ids[idx]}, Size: {file_sizes[idx]:,} bytes) Exists: {os.path.exists(file_paths[idx])}\")\n",
    "            files_to_delete.append({\n",
    "                'scene_id': row['scene_id'],\n",
    "                'file_id': file_ids[idx],\n",
    "                'file_path': file_paths[idx],\n",
    "                'size': file_sizes[idx],\n",
    "                'is_primary': is_primary[idx]\n",
    "            })\n",
    "\n",
    "# Calculate total space that would be freed\n",
    "total_space = sum(file['size'] for file in files_to_delete)\n",
    "print(f\"\\nTotal space that would be freed: {total_space:,} bytes ({total_space/1024/1024/1024:.2f} GB)\")\n",
    "print(f\"Number of files to delete: {len(files_to_delete)}\")\n",
    "\n",
    "# Create DataFrames for kept and deleted files\n",
    "kept_files_df = pl.DataFrame(kept_files)\n",
    "delete_files_df = pl.DataFrame(files_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in delete_files_df.iter_rows(named=True):\n",
    "    stash_raw_client.destroy_files(row['file_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding scenes where durations do not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene['files']):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash'), None)\n",
    "        phash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'phash'), None)\n",
    "        \n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'date': scene['date'],\n",
    "            'studio_name': scene['studio']['name'] if scene['studio'] else None,\n",
    "            'file_id': file['id'],\n",
    "            'file_path': file['path'],\n",
    "            'resolution_width': file['width'],\n",
    "            'resolution_height': file['height'],\n",
    "            'size': file['size'],\n",
    "            'duration': round(file['duration']),  # Round duration to nearest second\n",
    "            'oshash': oshash,\n",
    "            'phash': phash,\n",
    "            'is_primary': i == 0  # True if this is the first file in the scene's files list\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create Polars DataFrame\n",
    "scenes_with_multiple_files_df = pl.DataFrame(file_records)\n",
    "\n",
    "# Group by scene_id and find scenes where durations don't match\n",
    "duration_mismatches = scenes_with_multiple_files_df.group_by('scene_id').agg([\n",
    "    pl.col('title').first().alias('title'),\n",
    "    pl.col('duration').n_unique().alias('unique_durations'),\n",
    "    pl.col('duration').alias('all_durations'),\n",
    "    pl.col('file_id').alias('all_file_ids'),\n",
    "    pl.col('file_path').alias('all_file_paths'),\n",
    "    pl.col('size').alias('all_file_sizes'),\n",
    "    pl.col('is_primary').alias('all_is_primary')\n",
    "]).filter(\n",
    "    pl.col('unique_durations') > 1  # Only keep scenes with different durations\n",
    ").sort('scene_id')\n",
    "\n",
    "# Print summary of mismatched files\n",
    "print(\"\\nScenes with duration mismatches:\")\n",
    "for row in duration_mismatches.iter_rows(named=True):\n",
    "    print(f\"\\nScene {row['scene_id']} - {row['title']}\")\n",
    "    \n",
    "    for i, (duration, file_id, file_path, size, is_primary) in enumerate(zip(\n",
    "        row['all_durations'], \n",
    "        row['all_file_ids'], \n",
    "        row['all_file_paths'],\n",
    "        row['all_file_sizes'],\n",
    "        row['all_is_primary']\n",
    "    )):\n",
    "        primary_status = \" (Primary)\" if is_primary else \"\"\n",
    "        print(f\"  File{primary_status}: {file_path}\")\n",
    "        print(f\"    Duration: {duration}s, ID: {file_id}, Size: {size:,} bytes\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTotal scenes with duration mismatches: {len(duration_mismatches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in duration_mismatches.iter_rows(named=True):\n",
    "    refreshed_scene = stash_raw_client.find_scene(row['scene_id'])\n",
    "    print(f\"Scene {row['scene_id']} - {row['title']}\")\n",
    "    existing_tag_ids = [tag['id'] for tag in refreshed_scene['tags']]\n",
    "    if duration_mismatch_tag['id'] not in existing_tag_ids:\n",
    "        new_tag_ids = [*existing_tag_ids, duration_mismatch_tag['id']]\n",
    "        stash_raw_client.update_scene({ \"id\": row['scene_id'], \"tag_ids\": new_tag_ids })\n",
    "        print(f\"Added duration mismatch tag to scene {row['scene_id']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store different types of duplicates\n",
    "clear_duplicates = []  # Same hash and duration\n",
    "possible_duplicates = []  # Same duration only\n",
    "other_duplicates = []  # Different duration\n",
    "\n",
    "# Process each scene separately\n",
    "for scene in scenes_with_dupes:\n",
    "    # Create dictionaries to group files by oshash and duration for this scene\n",
    "    oshash_groups = {}\n",
    "    duration_groups = {}\n",
    "    \n",
    "    # Process files within this scene\n",
    "    for file in scene['files']:\n",
    "        assert file['fingerprints'] is not None, f\"No fingerprints for file: {file['path']}\"\n",
    "        oshash = next(fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash')\n",
    "        file_info = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'path': file['path'],\n",
    "            'duration': round(file['duration']),\n",
    "            'oshash': oshash\n",
    "        }\n",
    "        \n",
    "        # Group by oshash\n",
    "        if oshash not in oshash_groups:\n",
    "            oshash_groups[oshash] = []\n",
    "        oshash_groups[oshash].append(file_info)\n",
    "        \n",
    "        # Group by duration\n",
    "        duration_key = str(round(file['duration']))  # Round to nearest second to group files within 1s\n",
    "        if duration_key not in duration_groups:\n",
    "            duration_groups[duration_key] = []\n",
    "        duration_groups[duration_key].append(file_info)\n",
    "\n",
    "    # Check for duplicates within this scene\n",
    "    has_oshash_duration_match = False\n",
    "    has_duration_match = False\n",
    "    \n",
    "    # Find clear duplicates (same oshash and duration)\n",
    "    for oshash, files in oshash_groups.items():\n",
    "        if len(files) > 1:\n",
    "            # Check if all files in group have same duration\n",
    "            durations = {f['duration'] for f in files}\n",
    "            if len(durations) == 1:\n",
    "                has_oshash_duration_match = True\n",
    "                clear_duplicates.append({\n",
    "                    'scene_title': scene['title'],\n",
    "                    'scene_id': scene['id'],\n",
    "                    'files': files\n",
    "                })\n",
    "\n",
    "    # Find possible duplicates (same duration only)\n",
    "    for duration, files in duration_groups.items():\n",
    "        if len(files) > 1:\n",
    "            # Only include if files don't have matching oshash\n",
    "            oshashes = {file_info['oshash'] for file_info in files}\n",
    "            if len(oshashes) > 1:\n",
    "                has_duration_match = True\n",
    "                possible_duplicates.append({\n",
    "                    'scene_title': scene['title'],\n",
    "                    'scene_id': scene['id'],\n",
    "                    'files': files\n",
    "                })\n",
    "\n",
    "    # Find other duplicates (different duration)\n",
    "    for oshash, files in oshash_groups.items():\n",
    "        if len(files) > 1:\n",
    "            # Only include if files have different durations\n",
    "            durations = {f['duration'] for f in files}\n",
    "            if len(durations) > 1:\n",
    "                other_duplicates.append({\n",
    "                    'scene_title': scene['title'],\n",
    "                    'scene_id': scene['id'],\n",
    "                    'files': files\n",
    "                })\n",
    "\n",
    "    # Update tags based on current duplicate status\n",
    "    refreshed_scene = stash_raw_client.find_scene(scene['id'])\n",
    "    tag_ids = [tag['id'] for tag in refreshed_scene['tags']]\n",
    "    \n",
    "    # Handle oshash and duration match tag\n",
    "    if has_oshash_duration_match and oshash_and_duration_match_tag['id'] not in tag_ids:\n",
    "        tag_ids.append(oshash_and_duration_match_tag['id'])\n",
    "    elif not has_oshash_duration_match and oshash_and_duration_match_tag['id'] in tag_ids:\n",
    "        tag_ids.remove(oshash_and_duration_match_tag['id'])\n",
    "        \n",
    "    # Handle duration match tag\n",
    "    if has_duration_match and duration_match_tag['id'] not in tag_ids:\n",
    "        tag_ids.append(duration_match_tag['id'])\n",
    "    elif not has_duration_match and duration_match_tag['id'] in tag_ids:\n",
    "        tag_ids.remove(duration_match_tag['id'])\n",
    "        \n",
    "    # Update scene tags if they changed\n",
    "    if set(tag_ids) != set(tag['id'] for tag in refreshed_scene['tags']):\n",
    "        stash_raw_client.update_scene({ \"id\": scene['id'], \"tag_ids\": tag_ids })\n",
    "\n",
    "# Print all categories\n",
    "print(\"=== Clear duplicates (same hash and duration) ===\")\n",
    "for dupe in clear_duplicates:\n",
    "    print(f\"\\nScene: {dupe['scene_title']} (ID: {dupe['scene_id']})\")\n",
    "    for file in dupe['files']:\n",
    "        print(f\"\\\"{file['path']}\\\" (Duration: {file['duration']})\")\n",
    "\n",
    "print(\"\\n=== Possible duplicates (same duration only) ===\")\n",
    "for dupe in possible_duplicates:\n",
    "    print(f\"\\nScene: {dupe['scene_title']} (ID: {dupe['scene_id']})\")\n",
    "    for file in dupe['files']:\n",
    "        print(f\"\\\"{file['path']}\\\" (Duration: {file['duration']})\")\n",
    "\n",
    "print(\"\\n=== Other potential duplicates (different duration) ===\")\n",
    "for dupe in other_duplicates:\n",
    "    print(f\"\\nScene: {dupe['scene_title']} (ID: {dupe['scene_id']})\")\n",
    "    for file in dupe['files']:\n",
    "        print(f\"\\\"{file['path']}\\\" (Duration: {file['duration']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
