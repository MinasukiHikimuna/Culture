{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import dotenv\n",
    "import os\n",
    "from libraries.client_stashapp import get_stashapp_client, StashAppClient\n",
    "from libraries.StashDbClient import StashDbClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "stash_client = StashAppClient()\n",
    "stash_raw_client = get_stashapp_client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging scenes with same StashDB ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce functions\n",
    "def compare_and_merge_scenes(scenes):\n",
    "    \"\"\"Compare and merge multiple scenes, returning a merged version with the most complete metadata\"\"\"\n",
    "    \n",
    "    # Helper function to format value for display\n",
    "    def format_value(val):\n",
    "        if isinstance(val, list):\n",
    "            return f\"[{len(val)} items]\" if len(val) > 3 else str(val)\n",
    "        return str(val) if val is not None else \"None\"\n",
    "    \n",
    "    # Helper function to get tag names for display\n",
    "    def get_tag_names(tags):\n",
    "        return sorted([t['name'] for t in tags])\n",
    "    \n",
    "    # Helper function to get performer names for display\n",
    "    def get_performer_names(performers):\n",
    "        return sorted([p.get('name', f\"ID: {p['id']}\") for p in performers])\n",
    "    \n",
    "    # Fields to compare (excluding technical details and paths)\n",
    "    fields_to_compare = [\n",
    "        'title', 'code', 'details', 'director', 'date',\n",
    "        'rating100', 'organized', 'o_counter', 'organized', 'studio_id',\n",
    "        'gallery_ids', 'play_duration', 'play_count'\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Scene Comparison ===\\n\")\n",
    "    \n",
    "    # Compare basic fields\n",
    "    merged = {}\n",
    "    for field in fields_to_compare:\n",
    "        values = [scene.get(field) for scene in scenes]\n",
    "        \n",
    "        # For numerical fields, handle None values specially\n",
    "        if field in ['o_counter', 'play_count', 'play_duration', 'rating100']:\n",
    "            if all(v is None for v in values):\n",
    "                merged[field] = None\n",
    "            else:\n",
    "                merged[field] = max(v or 0 for v in values)\n",
    "        else:\n",
    "            # Choose the first non-None value\n",
    "            merged[field] = next((v for v in values if v is not None), None)\n",
    "        \n",
    "        # Only show detailed comparison if values differ\n",
    "        if len(set(str(v) for v in values)) > 1:\n",
    "            print(f\"{field}:\")\n",
    "            for i, val in enumerate(values, 1):\n",
    "                print(f\"  Scene {i}: {format_value(val)}\")\n",
    "            print(f\"  Merged: {format_value(merged[field])}\\n\")\n",
    "        else:\n",
    "            print(f\"{field}: {format_value(values[0])}\")\n",
    "    \n",
    "    print(\"\\n=== Special Fields ===\\n\")\n",
    "    \n",
    "    # Handle tags\n",
    "    all_tags = []\n",
    "    print(\"tags:\")\n",
    "    for i, scene in enumerate(scenes, 1):\n",
    "        tags = scene.get('tags', [])\n",
    "        all_tags.extend(tags)\n",
    "        print(f\"  Scene {i}: {get_tag_names(tags)}\")\n",
    "    \n",
    "    merged_tags = list({t['id']: t for t in all_tags}.values())\n",
    "    print(f\"  Merged: {get_tag_names(merged_tags)}\\n\")\n",
    "    merged['tag_ids'] = [t['id'] for t in merged_tags]\n",
    "    \n",
    "    # Handle performers\n",
    "    all_performers = []\n",
    "    print(\"performers:\")\n",
    "    for i, scene in enumerate(scenes, 1):\n",
    "        performers = scene.get('performers', [])\n",
    "        all_performers.extend(performers)\n",
    "        print(f\"  Scene {i}: {get_performer_names(performers)}\")\n",
    "    \n",
    "    merged_performers = list({p['id']: p for p in all_performers}.values())\n",
    "    print(f\"  Merged: {get_performer_names(merged_performers)}\\n\")\n",
    "    merged['performer_ids'] = [p['id'] for p in merged_performers]\n",
    "    \n",
    "    # Handle other special fields\n",
    "    special_fields = {\n",
    "        'stash_ids': lambda scenes: list({(s['endpoint'], s['stash_id']): s \n",
    "                                        for scene in scenes \n",
    "                                        for s in scene.get('stash_ids', [])}.values()),\n",
    "        'scene_markers': lambda scenes: sorted([m for scene in scenes \n",
    "                                              for m in scene.get('scene_markers', [])],\n",
    "                                             key=lambda m: m['seconds']),\n",
    "        'o_history': lambda scenes: sorted(list(set([h for scene in scenes \n",
    "                                                   for h in scene.get('o_history', [])]))),\n",
    "        'play_history': lambda scenes: sorted(list(set([h for scene in scenes \n",
    "                                                      for h in scene.get('play_history', [])]))),\n",
    "        'urls': lambda scenes: sorted(list(set([u for scene in scenes \n",
    "                                              for u in scene.get('urls', [])])))\n",
    "    }\n",
    "    \n",
    "    for field, merge_func in special_fields.items():\n",
    "        values = [scene.get(field, []) for scene in scenes]\n",
    "        merged_items = merge_func(scenes)\n",
    "        \n",
    "        if any(len(v) != len(values[0]) for v in values) or len(merged_items) != len(values[0]):\n",
    "            print(f\"{field}:\")\n",
    "            for i, items in enumerate(values, 1):\n",
    "                print(f\"  Scene {i}: {format_value(items) if len(items) <= 3 else f'{len(items)} items'}\")\n",
    "            print(f\"  Merged: {format_value(merged_items) if len(merged_items) <= 3 else f'{len(merged_items)} items'}\\n\")\n",
    "        else:\n",
    "            print(f\"{field}: {len(values[0])} items\")\n",
    "        \n",
    "        merged[field] = merged_items\n",
    "    \n",
    "    # Keep other fields from first scene that we haven't explicitly handled\n",
    "    for key in scenes[0]:\n",
    "        if key not in merged and key not in ['files', 'paths', 'sceneStreams', 'tags', 'performers']:\n",
    "            merged[key] = scenes[0][key]\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic info for all scenes\n",
    "all_scenes_basic_info = stash_raw_client.find_scenes({}, fragment=\"id title studio { id name } stash_ids { endpoint stash_id }\")\n",
    "\n",
    "# Map the data with explicit type handling\n",
    "all_scenes_basic_info_mapped = [\n",
    "    {\n",
    "        \"stashapp_id\": str(scene['id']),  # Ensure ID is string\n",
    "        \"stashapp_title\": str(scene['title']) if scene['title'] else \"\",  # Handle None titles\n",
    "        \"stashapp_studio_id\": str(scene['studio']['id']) if scene['studio'] else None,  # Ensure studio ID is string\n",
    "        \"stashapp_studio_name\": str(scene['studio']['name']) if scene['studio'] else None,\n",
    "        \"stashapp_stashdb_id\": next((str(stash_id['stash_id']) for stash_id in scene['stash_ids'] \n",
    "                           if stash_id['endpoint'] == 'https://stashdb.org/graphql'), None)\n",
    "    }\n",
    "    for scene in all_scenes_basic_info\n",
    "]\n",
    "\n",
    "# Create DataFrame with increased schema inference length\n",
    "all_scenes_basic_info_mapped_df = pl.DataFrame(\n",
    "    all_scenes_basic_info_mapped,\n",
    "    infer_schema_length=10000  # Increase schema inference length\n",
    ")\n",
    "\n",
    "scenes_with_dupe_stashdb_ids_basic_info = all_scenes_basic_info_mapped_df.filter(pl.col('stashapp_stashdb_id').is_in(all_scenes_basic_info_mapped_df.group_by('stashapp_stashdb_id').agg(pl.len().alias('count')).filter(pl.col('count') > 1).get_column('stashapp_stashdb_id'))).sort('stashapp_stashdb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupe_stashdb_ids_basic_info\n",
    "print(f\"Studios with duplicate scenes: {scenes_with_dupe_stashdb_ids_basic_info.select(pl.col('stashapp_studio_id')).unique().to_series().to_list()}\")\n",
    "\n",
    "studio_id = scenes_with_dupe_stashdb_ids_basic_info.select(pl.col('stashapp_studio_id')).unique().to_series().to_list()[0]\n",
    "print(f\"Studio ID: {studio_id}\")\n",
    "\n",
    "all_scenes = stash_client.find_scenes_by_studio(studio_id)\n",
    "print(f\"Number of scenes: {len(all_scenes)}\")\n",
    "\n",
    "scenes_with_dupe_stashdb_ids = all_scenes.filter(pl.col('stashapp_stashdb_id').is_in(all_scenes.group_by('stashapp_stashdb_id').agg(pl.len().alias('count')).filter(pl.col('count') > 1).get_column('stashapp_stashdb_id'))).sort('stashapp_stashdb_id')\n",
    "print(f\"Number of scenes with duplicate StashDB IDs: {len(scenes_with_dupe_stashdb_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_scenes = scenes_with_dupe_stashdb_ids.group_by('stashapp_stashdb_id').agg([\n",
    "    pl.col('stashapp_id').alias('scene_ids'),\n",
    "    pl.col('stashapp_title').alias('titles')\n",
    "])\n",
    "\n",
    "grouped_scenes = grouped_scenes\n",
    "grouped_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge scenes with same StashDB ID\n",
    "for group in grouped_scenes.iter_rows(named=True):\n",
    "    stashdb_id = group['stashapp_stashdb_id']\n",
    "    scene_ids = group['scene_ids']\n",
    "    titles = group['titles']\n",
    "    \n",
    "    print(f\"\\nProcessing group with StashDB ID: {stashdb_id}\")\n",
    "    print(f\"Scene titles: {titles[0]}\")  # All titles should be the same\n",
    "    print(f\"Scene IDs: {scene_ids}\")\n",
    "    \n",
    "    # Get full scene data for each ID in the group\n",
    "    scenes = [stash_raw_client.find_scene(str(scene_id)) for scene_id in scene_ids]\n",
    "    if not scenes:\n",
    "        raise ValueError(f\"No scenes found for IDs: {scene_ids}\")\n",
    "    \n",
    "    if len(scenes) < 1:\n",
    "        raise ValueError(f\"No scenes found for IDs: {scene_ids}\")\n",
    "    \n",
    "    if len(scenes) < 2:\n",
    "        raise ValueError(f\"Only one scene found for IDs: {scene_ids}\")\n",
    "    \n",
    "    # Sort scenes by ID to determine source and destination\n",
    "    sorted_scene_ids = [str(scene_id) for scene_id in sorted([int(scene['id']) for scene in scenes])]\n",
    "    destination_scene_id = sorted_scene_ids[0]\n",
    "    source_scene_ids = sorted_scene_ids[1:]\n",
    "    \n",
    "    print(f\"Merging scenes {source_scene_ids} into {destination_scene_id}\")\n",
    "    \n",
    "    # Compare and merge the scenes\n",
    "    merged_scene = compare_and_merge_scenes(scenes)\n",
    "    \n",
    "    # Prepare the merge input\n",
    "    scene_merge_input = {\n",
    "        \"source\": source_scene_ids,\n",
    "        \"destination\": destination_scene_id,\n",
    "        \"values\": {\n",
    "            \"id\": destination_scene_id,\n",
    "            \"title\": merged_scene['title'],\n",
    "            \"code\": merged_scene['code'],\n",
    "            \"details\": merged_scene['details'],\n",
    "            \"director\": merged_scene['director'],\n",
    "            \"urls\": merged_scene['urls'],\n",
    "            \"date\": merged_scene['date'],\n",
    "            \"rating100\": merged_scene['rating100'],\n",
    "            \"o_counter\": merged_scene['o_counter'],\n",
    "            \"organized\": merged_scene['organized'],\n",
    "            \"gallery_ids\": merged_scene['gallery_ids'] if 'gallery_ids' in merged_scene and merged_scene['gallery_ids'] is not None else [],\n",
    "            \"performer_ids\": merged_scene['performer_ids'],\n",
    "            \"tag_ids\": merged_scene['tag_ids'],\n",
    "            \"stash_ids\": merged_scene['stash_ids'],\n",
    "            \"play_duration\": merged_scene['play_duration'],\n",
    "            \"play_count\": merged_scene['play_count'],\n",
    "        },\n",
    "        \"play_history\": True, \n",
    "        \"o_history\": True,\n",
    "    }\n",
    "    \n",
    "    # Execute the merge\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        mutation SceneMerge($merge_input: SceneMergeInput!) {\n",
    "            sceneMerge(input: $merge_input) {\n",
    "                id\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        result = stash_raw_client.call_GQL(query, {\"merge_input\": scene_merge_input})\n",
    "        print(f\"Successfully merged scenes: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error merging scenes: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding scenes with multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "oshash_and_duration_match_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: OSHASH And Duration Match\" })\n",
    "duration_match_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: Duration Match\" })\n",
    "scenes_with_multiple_versions = stash_raw_client.find_tag({ \"name\": \"Scene: Multiple Versions\" })\n",
    "duration_mismatch_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: Duration Mismatch\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupes = stash_raw_client.find_scenes({ \n",
    "  \"file_count\": {\n",
    "    \"modifier\": \"GREATER_THAN\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"tags\": {\n",
    "    \"value\": [],\n",
    "    \"modifier\": \"INCLUDES\",\n",
    "    \"excludes\": [scenes_with_multiple_versions['id']]\n",
    "  }\n",
    "}, fragment=\"id title date studio { name } files { id duration path width height size fingerprints { type value } }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene['files']):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash'), None)\n",
    "        phash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'phash'), None)\n",
    "        \n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'date': scene['date'],\n",
    "            'studio_name': scene['studio']['name'] if scene['studio'] else None,\n",
    "            'file_id': file['id'],\n",
    "            'file_path': file['path'],\n",
    "            'resolution_width': file['width'],\n",
    "            'resolution_height': file['height'],\n",
    "            'size': file['size'],\n",
    "            'duration': file['duration'],\n",
    "            'oshash': oshash,\n",
    "            'phash': phash,\n",
    "            'is_primary': i == 0  # True if this is the first file in the scene's files list\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create Polars DataFrame\n",
    "scenes_with_multiple_files_df = pl.DataFrame(file_records)\n",
    "scenes_with_multiple_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting files where OSHASH is identical to another file in the scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Filter to only show rows that have matching oshash values with other rows\n",
    "duplicate_files_df = scenes_with_multiple_files_df.filter(\n",
    "    pl.col('oshash').is_in(\n",
    "        scenes_with_multiple_files_df.group_by('oshash')\n",
    "        .agg(pl.len().alias('dupe_count'))  # Changed to use len() and a unique alias\n",
    "        .filter(pl.col('dupe_count') > 1)\n",
    "        .get_column('oshash')\n",
    "    )\n",
    ").sort(['oshash', 'file_id'])\n",
    "\n",
    "duplicate_files_df = duplicate_files_df.with_columns(pl.col('file_path').map_elements(lambda x: os.path.exists(x), return_dtype=pl.Boolean).alias('file_path_exists'))\n",
    "\n",
    "print(\"\\nFiles with duplicate oshash values:\")\n",
    "print(duplicate_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by scene_id and oshash to find duplicates within each scene\n",
    "grouped_files = duplicate_files_df.group_by(['scene_id', 'oshash']).agg([\n",
    "    pl.col('file_id').min().alias('keep_file_id'),  # File to keep\n",
    "    pl.col('file_id').alias('all_file_ids'),        # All file IDs\n",
    "    pl.col('file_path').alias('all_file_paths'),    # All file paths\n",
    "    pl.col('size').alias('all_file_sizes'),         # File sizes\n",
    "    pl.col('file_path_exists').alias('all_file_exists'),  # Path existence check\n",
    "    pl.col('is_primary').alias('all_is_primary')    # Primary file flags\n",
    "])\n",
    "\n",
    "# Lists to store kept and deleted file information\n",
    "kept_files = []\n",
    "files_to_delete = []\n",
    "\n",
    "# Print summary and collect file information\n",
    "print(\"\\nFiles to be deleted:\")\n",
    "for row in grouped_files.iter_rows(named=True):\n",
    "    file_ids = row['all_file_ids']\n",
    "    file_paths = row['all_file_paths']\n",
    "    file_sizes = row['all_file_sizes']\n",
    "    is_primary = row['all_is_primary']\n",
    "    \n",
    "    # Find primary file index if it exists\n",
    "    primary_indices = [i for i, p in enumerate(is_primary) if p]\n",
    "    if primary_indices:\n",
    "        # Keep the primary file\n",
    "        keep_index = primary_indices[0]\n",
    "        keep_file_id = file_ids[keep_index]\n",
    "    else:\n",
    "        # If no primary file, keep the one with lowest file_id\n",
    "        keep_file_id = row['keep_file_id']\n",
    "        keep_index = file_ids.index(keep_file_id)\n",
    "    \n",
    "    kept_files.append({\n",
    "        'scene_id': row['scene_id'],\n",
    "        'file_id': keep_file_id,\n",
    "        'file_path': file_paths[keep_index],\n",
    "        'size': file_sizes[keep_index],\n",
    "        'is_primary': is_primary[keep_index]\n",
    "    })\n",
    "    \n",
    "    # Get indices of files to delete (all except kept file, never delete primary files)\n",
    "    delete_indices = [i for i, (file_id, p) in enumerate(zip(file_ids, is_primary)) \n",
    "                     if file_id != keep_file_id and not p]\n",
    "    \n",
    "    if delete_indices:  # Only show if there are files to delete\n",
    "        primary_status = \" (Primary)\" if is_primary[keep_index] else \"\"\n",
    "        print(f\"\\nScene {row['scene_id']} - Keeping{primary_status}: {file_paths[keep_index]} (ID: {keep_file_id}, Size: {file_sizes[keep_index]:,} bytes) Exists: {os.path.exists(file_paths[keep_index])}\")\n",
    "        for idx in delete_indices:\n",
    "            print(f\"  Will delete: {file_paths[idx]} (ID: {file_ids[idx]}, Size: {file_sizes[idx]:,} bytes) Exists: {os.path.exists(file_paths[idx])}\")\n",
    "            files_to_delete.append({\n",
    "                'scene_id': row['scene_id'],\n",
    "                'file_id': file_ids[idx],\n",
    "                'file_path': file_paths[idx],\n",
    "                'size': file_sizes[idx],\n",
    "                'is_primary': is_primary[idx]\n",
    "            })\n",
    "\n",
    "# Calculate total space that would be freed\n",
    "total_space = sum(file['size'] for file in files_to_delete)\n",
    "print(f\"\\nTotal space that would be freed: {total_space:,} bytes ({total_space/1024/1024/1024:.2f} GB)\")\n",
    "print(f\"Number of files to delete: {len(files_to_delete)}\")\n",
    "\n",
    "# Create DataFrames for kept and deleted files\n",
    "kept_files_df = pl.DataFrame(kept_files)\n",
    "delete_files_df = pl.DataFrame(files_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in delete_files_df.iter_rows(named=True):\n",
    "    stash_raw_client.destroy_files(row['file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_duration_mismatch_tag_but_only_one_file = stash_raw_client.find_scenes({\n",
    "    \"tags\": { \"value\": [duration_mismatch_tag[\"id\"]], \"modifier\": \"INCLUDES\" },\n",
    "    \"file_count\": { \"modifier\": \"EQUALS\", \"value\": 1 }\n",
    "}, fragment=\"id title tags { id name }\")\n",
    "scenes_with_duration_mismatch_tag_but_only_one_file_ids = [scene['id'] for scene in scenes_with_duration_mismatch_tag_but_only_one_file]\n",
    "scenes_with_duration_mismatch_tag_but_only_one_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_client.update_tags_for_scenes(\n",
    "    scenes_with_duration_mismatch_tag_but_only_one_file_ids,\n",
    "    [],\n",
    "    [duration_mismatch_tag[\"name\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding scenes where durations do not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene['files']):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash'), None)\n",
    "        phash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'phash'), None)\n",
    "        \n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'date': scene['date'],\n",
    "            'studio_name': scene['studio']['name'] if scene['studio'] else None,\n",
    "            'file_id': file['id'],\n",
    "            'file_path': file['path'],\n",
    "            'resolution_width': file['width'],\n",
    "            'resolution_height': file['height'],\n",
    "            'size': file['size'],\n",
    "            'duration': round(file['duration']),  # Round duration to nearest second\n",
    "            'oshash': oshash,\n",
    "            'phash': phash,\n",
    "            'is_primary': i == 0  # True if this is the first file in the scene's files list\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create Polars DataFrame\n",
    "scenes_with_multiple_files_df = pl.DataFrame(file_records)\n",
    "\n",
    "# Group by scene_id and find scenes where durations don't match\n",
    "duration_mismatches = scenes_with_multiple_files_df.group_by('scene_id').agg([\n",
    "    pl.col('title').first().alias('title'),\n",
    "    pl.col('duration').n_unique().alias('unique_durations'),\n",
    "    pl.col('duration').alias('all_durations'),\n",
    "    pl.col('file_id').alias('all_file_ids'),\n",
    "    pl.col('file_path').alias('all_file_paths'),\n",
    "    pl.col('size').alias('all_file_sizes'),\n",
    "    pl.col('is_primary').alias('all_is_primary')\n",
    "]).filter(\n",
    "    pl.col('unique_durations') > 1  # Only keep scenes with different durations\n",
    ").sort('scene_id')\n",
    "\n",
    "# Print summary of mismatched files\n",
    "print(\"\\nScenes with duration mismatches:\")\n",
    "for row in duration_mismatches.iter_rows(named=True):\n",
    "    print(f\"\\nScene {row['scene_id']} - {row['title']}\")\n",
    "    \n",
    "    for i, (duration, file_id, file_path, size, is_primary) in enumerate(zip(\n",
    "        row['all_durations'], \n",
    "        row['all_file_ids'], \n",
    "        row['all_file_paths'],\n",
    "        row['all_file_sizes'],\n",
    "        row['all_is_primary']\n",
    "    )):\n",
    "        primary_status = \" (Primary)\" if is_primary else \"\"\n",
    "        print(f\"  File{primary_status}: {file_path}\")\n",
    "        print(f\"    Duration: {duration}s, ID: {file_id}, Size: {size:,} bytes\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTotal scenes with duration mismatches: {len(duration_mismatches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in duration_mismatches.iter_rows(named=True):\n",
    "    refreshed_scene = stash_raw_client.find_scene(row['scene_id'])\n",
    "    print(f\"Scene {row['scene_id']} - {row['title']}\")\n",
    "    existing_tag_ids = [tag['id'] for tag in refreshed_scene['tags']]\n",
    "    if duration_mismatch_tag['id'] not in existing_tag_ids:\n",
    "        new_tag_ids = [*existing_tag_ids, duration_mismatch_tag['id']]\n",
    "        stash_raw_client.update_scene({ \"id\": row['scene_id'], \"tag_ids\": new_tag_ids })\n",
    "        print(f\"Added duration mismatch tag to scene {row['scene_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_raw_client.find_scenes({ \"tags\": { \"value\": [duration_mismatch_tag[\"id\"]], \"modifier\": \"INCLUDES\" }}, fragment=\"id title tags { id name }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find PHASH mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance_hex(hash1: str, hash2: str) -> int:\n",
    "    \"\"\"Calculate Hamming distance between two hex strings.\"\"\"\n",
    "    try:\n",
    "        bin1 = bin(int(hash1, 16))[2:].zfill(64)\n",
    "        bin2 = bin(int(hash2, 16))[2:].zfill(64)\n",
    "        return sum(b1 != b2 for b1, b2 in zip(bin1, bin2))\n",
    "    except (ValueError, TypeError):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupes = stash_raw_client.find_scenes({ \n",
    "  \"file_count\": {\n",
    "    \"modifier\": \"GREATER_THAN\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"tags\": {\n",
    "    \"value\": [],\n",
    "    \"modifier\": \"INCLUDES\",\n",
    "    \"excludes\": [scenes_with_multiple_versions['id']]\n",
    "  }\n",
    "}, fragment=\"id title date studio { name } files { id duration path width height size fingerprints { type value } }\")\n",
    "# Create Polars DataFrame with strict=False to handle mixed numeric types\n",
    "scenes_with_dupes_df = pl.DataFrame(scenes_with_dupes, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene['files']):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'oshash'), None)\n",
    "        phash = next((fp['value'] for fp in file['fingerprints'] if fp['type'] == 'phash'), None)\n",
    "        \n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            'scene_id': scene['id'],\n",
    "            'title': scene['title'],\n",
    "            'date': scene['date'],\n",
    "            'studio_name': scene['studio']['name'] if scene['studio'] else None,\n",
    "            'file_id': file['id'],\n",
    "            'file_path': file['path'],\n",
    "            'resolution_width': float(file['width']),\n",
    "            'resolution_height': float(file['height']),\n",
    "            'size': float(file['size']),\n",
    "            'duration': float(file['duration']),\n",
    "            'oshash': oshash,\n",
    "            'phash': phash,\n",
    "            'is_primary': i == 0\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create DataFrame from the flattened records\n",
    "files_df = pl.DataFrame(file_records, strict=False)\n",
    "\n",
    "# Find scenes with matching durations but differing phashes\n",
    "phash_mismatches = []\n",
    "for scene_id, group in files_df.group_by('scene_id'):\n",
    "    # Get all files for this scene\n",
    "    scene_files = group.to_dicts()\n",
    "    \n",
    "    # Skip if durations don't match\n",
    "    durations = set(file['duration'] for file in scene_files)\n",
    "    if len(durations) > 1:\n",
    "        continue\n",
    "        \n",
    "    # Compare phashes\n",
    "    has_mismatch = False\n",
    "    for i, file1 in enumerate(scene_files):\n",
    "        for file2 in scene_files[i + 1:]:\n",
    "            if hamming_distance_hex(file1['phash'], file2['phash']) > 8:\n",
    "                has_mismatch = True\n",
    "                break\n",
    "        if has_mismatch:\n",
    "            phash_mismatches.extend(scene_files)\n",
    "            break\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pl.DataFrame(phash_mismatches, strict=False).sort('scene_id')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nScenes with matching durations but differing phash values (>8 bits different):\")\n",
    "for scene_id, group in results_df.group_by('scene_id'):\n",
    "    scene_files = group.to_dicts()\n",
    "    print(f\"\\nScene {scene_id} - {scene_files[0]['title']}\")\n",
    "    \n",
    "    for file in scene_files:\n",
    "        primary_status = \" (Primary)\" if file['is_primary'] else \"\"\n",
    "        print(f\"  File{primary_status}: {file['file_path']}\")\n",
    "        print(f\"    Duration: {file['duration']}s\")\n",
    "        print(f\"    pHash: {file['phash']}\")\n",
    "        \n",
    "        if not file['is_primary']:\n",
    "            primary_file = next(f for f in scene_files if f['is_primary'])\n",
    "            hamming_dist = hamming_distance_hex(primary_file['phash'], file['phash'])\n",
    "            print(f\"    Hamming distance from primary: {hamming_dist} bits\")\n",
    "\n",
    "print(f\"\\nTotal scenes with phash mismatches: {len(set(results_df['scene_id']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making higher quality versions the primary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupes = stash_raw_client.find_scenes({ \n",
    "  \"file_count\": {\n",
    "    \"modifier\": \"GREATER_THAN\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"tags\": {\n",
    "    \"value\": [],\n",
    "    \"modifier\": \"INCLUDES\",\n",
    "    \"excludes\": [scenes_with_multiple_versions['id']]\n",
    "  }\n",
    "}, fragment=\"id title date studio { name } files { id duration path width height size fingerprints { type value } format video_codec audio_codec }\")\n",
    "# Create Polars DataFrame with strict=False to handle mixed numeric types\n",
    "scenes_with_dupes_df = pl.DataFrame(scenes_with_dupes, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Explode the files array to get one row per file\n",
    "files_df = scenes_with_dupes_df.explode('files')\n",
    "\n",
    "# Group by scene ID and check if there are different codecs within each group\n",
    "codec_analysis = (files_df\n",
    "    .select([\n",
    "        'id',\n",
    "        'title',\n",
    "        pl.col('files').struct.field('video_codec').alias('video_codec'),\n",
    "        pl.col('files').struct.field('audio_codec').alias('audio_codec')\n",
    "    ])\n",
    "    .group_by('id')\n",
    "    .agg([\n",
    "        pl.col('title').first(),\n",
    "        pl.col('video_codec').alias('video_codecs'),\n",
    "        pl.col('audio_codec').alias('audio_codecs'),\n",
    "        pl.col('video_codec').n_unique().alias('unique_video_codecs'),\n",
    "        pl.col('audio_codec').n_unique().alias('unique_audio_codecs')\n",
    "    ])\n",
    "    .filter(\n",
    "        (pl.col('unique_video_codecs') > 1) |\n",
    "        (pl.col('unique_audio_codecs') > 1)\n",
    "    )\n",
    ")\n",
    "codec_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_client.update_tags_for_scenes(\n",
    "    codec_analysis.select(\"id\").to_series().to_list(),\n",
    "    [\"Duplicate: Video or Audio Formats Differ\"],\n",
    "    []\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "codec_mismatch_tag = stash_raw_client.find_tag({ \"name\": \"Duplicate: Video or Audio Formats Differ\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_codec_mismatch_tag_but_only_one_file = stash_raw_client.find_scenes({\n",
    "    \"tags\": { \"value\": [codec_mismatch_tag[\"id\"]], \"modifier\": \"INCLUDES\" },\n",
    "    \"file_count\": { \"modifier\": \"EQUALS\", \"value\": 1 }\n",
    "}, fragment=\"id title tags { id name }\")\n",
    "scenes_with_codec_mismatch_tag_but_only_one_file_ids = [scene['id'] for scene in scenes_with_codec_mismatch_tag_but_only_one_file]\n",
    "scenes_with_codec_mismatch_tag_but_only_one_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_client.update_tags_for_scenes(\n",
    "    scenes_with_codec_mismatch_tag_but_only_one_file_ids,\n",
    "    [],\n",
    "    [codec_mismatch_tag[\"name\"]]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
