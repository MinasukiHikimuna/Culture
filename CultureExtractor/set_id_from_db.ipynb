{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Import Culture Extractor client\n",
    "import libraries.client_culture_extractor as client_culture_extractor\n",
    "\n",
    "# Import StashApp client\n",
    "from libraries.client_stashapp import StashAppClient, get_stashapp_client\n",
    "\n",
    "# Initialize Culture Extractor client\n",
    "user = os.environ.get(\"CE_DB_USERNAME\")\n",
    "pw = os.environ.get(\"CE_DB_PASSWORD\")\n",
    "host = os.environ.get(\"CE_DB_HOST\")\n",
    "port = os.environ.get(\"CE_DB_PORT\")\n",
    "db = os.environ.get(\"CE_DB_NAME\")\n",
    "\n",
    "connection_string = f\"dbname={db} user={user} password={pw} host={host} port={port}\"\n",
    "culture_extractor_client = client_culture_extractor.ClientCultureExtractor(connection_string)\n",
    "\n",
    "# Initialize Stashapp clients\n",
    "stash_client = StashAppClient()\n",
    "stash_raw_client = get_stashapp_client()\n",
    "\n",
    "# Define Culture Extractor endpoint\n",
    "CULTURE_EXTRACTOR_ENDPOINT = \"https://culture.extractor/graphql\"\n",
    "\n",
    "# DEBUG FLAG: Set to True to write intermediate outputs to JSON files\n",
    "DEBUG_WRITE_JSON = True\n",
    "DEBUG_OUTPUT_DIR = os.path.join(os.path.dirname(os.getcwd()), \".debug_outputs\")\n",
    "\n",
    "\n",
    "def write_debug_json(df: pl.DataFrame, name: str):\n",
    "    \"\"\"Write DataFrame to JSON file if DEBUG_WRITE_JSON is enabled.\"\"\"\n",
    "    if DEBUG_WRITE_JSON:\n",
    "        # Ensure debug output directory exists\n",
    "        os.makedirs(DEBUG_OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        filename = f\"set_id_from_db__{name}.json\"\n",
    "        filepath = os.path.join(DEBUG_OUTPUT_DIR, filename)\n",
    "        df.write_json(filepath)\n",
    "        print(f\"[DEBUG] Wrote {len(df)} rows to {filepath}\")\n",
    "\n",
    "\n",
    "def get_existing_ce_stash_id(existing_stash_ids: list) -> str | None:\n",
    "    \"\"\"Get existing Culture Extractor stash_id if it exists.\"\"\"\n",
    "    for sid in existing_stash_ids:\n",
    "        if sid.get(\"endpoint\") == CULTURE_EXTRACTOR_ENDPOINT:\n",
    "            return sid.get(\"stash_id\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def merge_stash_ids(\n",
    "    existing_stash_ids: list, new_endpoint: str, new_stash_id: str\n",
    ") -> list:\n",
    "    \"\"\"Merge new stash_id with existing ones, preserving other endpoints.\"\"\"\n",
    "    # Start with existing stash_ids, filtering out any existing culture.extractor entries\n",
    "    merged = [sid for sid in existing_stash_ids if sid.get(\"endpoint\") != new_endpoint]\n",
    "\n",
    "    # Add the new culture.extractor stash_id\n",
    "    merged.append({\"endpoint\": new_endpoint, \"stash_id\": new_stash_id})\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_sites",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sites from Culture Extractor\n",
    "all_ce_sites = culture_extractor_client.get_sites()\n",
    "print(f\"Found {len(all_ce_sites)} sites in Culture Extractor\")\n",
    "\n",
    "# Write debug output\n",
    "write_debug_json(all_ce_sites, \"all_ce_sites\")\n",
    "\n",
    "all_ce_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_scenes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all scenes from Stashapp with their file fingerprints and existing stash_ids\n",
    "scenes = stash_raw_client.find_scenes(\n",
    "    fragment=\"\"\"\n",
    "    id\n",
    "    title\n",
    "    stash_ids {\n",
    "        endpoint\n",
    "        stash_id\n",
    "    }\n",
    "    files {\n",
    "        id\n",
    "        basename\n",
    "        path\n",
    "        fingerprints {\n",
    "            type\n",
    "            value\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Process scenes to extract oshash from fingerprints\n",
    "results = []\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_id = scene.get(\"id\")\n",
    "    scene_title = scene.get(\"title\")\n",
    "    existing_stash_ids = scene.get(\"stash_ids\", [])\n",
    "    files = scene.get(\"files\", [])\n",
    "    \n",
    "    # Get primary file (first file)\n",
    "    if files:\n",
    "        primary_file = files[0]\n",
    "        file_basename = primary_file.get(\"basename\")\n",
    "        file_path = primary_file.get(\"path\")\n",
    "        fingerprints = primary_file.get(\"fingerprints\", [])\n",
    "        \n",
    "        # Extract oshash\n",
    "        oshash = next(\n",
    "            (fp[\"value\"] for fp in fingerprints if fp[\"type\"] == \"oshash\"),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        # Only include scenes with oshash\n",
    "        if oshash:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"scene_id\": scene_id,\n",
    "                    \"scene_title\": scene_title,\n",
    "                    \"file_basename\": file_basename,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"oshash\": oshash,\n",
    "                    \"existing_stash_ids\": existing_stash_ids,\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Create DataFrame\n",
    "scenes_df = pl.DataFrame(results)\n",
    "\n",
    "print(f\"Total scenes: {len(scenes)}\")\n",
    "print(f\"Scenes with oshash: {len(scenes_df)}\")\n",
    "\n",
    "# Write debug output\n",
    "write_debug_json(scenes_df, \"scenes_df\")\n",
    "\n",
    "scenes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_downloads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all downloads from Culture Extractor database\n",
    "# You can filter by specific site_uuid if needed, or get all sites\n",
    "\n",
    "all_downloads = []\n",
    "\n",
    "# Default to a single site for testing - change site name or set to None to process all sites\n",
    "SELECTED_SITE_NAME = \"Meana Wolf\"  # Set to None to process all sites\n",
    "\n",
    "if SELECTED_SITE_NAME:\n",
    "    selected_sites = all_ce_sites.filter(pl.col(\"ce_sites_name\") == SELECTED_SITE_NAME)\n",
    "    print(f\"Processing single site: {SELECTED_SITE_NAME}\")\n",
    "else:\n",
    "    selected_sites = all_ce_sites\n",
    "    print(f\"Processing all {len(all_ce_sites)} sites\")\n",
    "\n",
    "for site_row in selected_sites.iter_rows(named=True):\n",
    "    site_uuid = site_row[\"ce_sites_uuid\"]\n",
    "    site_name = site_row[\"ce_sites_name\"]\n",
    "    \n",
    "    print(f\"Fetching downloads for {site_name}...\")\n",
    "    site_downloads = culture_extractor_client.get_downloads(site_uuid)\n",
    "    \n",
    "    if len(site_downloads) > 0:\n",
    "        all_downloads.append(site_downloads)\n",
    "        print(f\"  Found {len(site_downloads)} downloads\")\n",
    "\n",
    "# Concatenate all downloads\n",
    "if all_downloads:\n",
    "    downloads_df = pl.concat(all_downloads)\n",
    "else:\n",
    "    downloads_df = pl.DataFrame()\n",
    "\n",
    "# Filter to video files with content_type \"scene\" and oshash (for exact matching)\n",
    "downloads_with_oshash = downloads_df.filter(\n",
    "    (pl.col(\"ce_downloads_file_type\") == \"video\") &\n",
    "    (pl.col(\"ce_downloads_content_type\") == \"scene\") &\n",
    "    (pl.col(\"ce_downloads_hash_oshash\").is_not_null())\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal downloads: {len(downloads_df)}\")\n",
    "print(f\"Video scene downloads with oshash: {len(downloads_with_oshash)}\")\n",
    "\n",
    "# Write debug output\n",
    "write_debug_json(downloads_with_oshash, \"downloads_with_oshash\")\n",
    "\n",
    "downloads_with_oshash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join_and_filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Culture Extractor downloads with Stashapp scenes on oshash (exact matching only)\n",
    "joined_df = scenes_df.join(\n",
    "    downloads_with_oshash,\n",
    "    left_on=\"oshash\",\n",
    "    right_on=\"ce_downloads_hash_oshash\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(f\"Matched scenes: {len(joined_df)}\")\n",
    "\n",
    "# Write debug output for joined data\n",
    "write_debug_json(joined_df, \"joined_df\")\n",
    "\n",
    "# Filter out scenes that already have the matching Culture Extractor stash ID\n",
    "scenes_to_update = []\n",
    "scenes_already_set = []\n",
    "\n",
    "for row in joined_df.iter_rows(named=True):\n",
    "    existing_stash_ids = row[\"existing_stash_ids\"] or []\n",
    "    existing_ce_stash_id = get_existing_ce_stash_id(existing_stash_ids)\n",
    "    ce_release_uuid = row[\"ce_downloads_release_uuid\"]\n",
    "\n",
    "    # Check if the same UUID is already set\n",
    "    if existing_ce_stash_id == ce_release_uuid:\n",
    "        scenes_already_set.append(row)\n",
    "    else:\n",
    "        scenes_to_update.append(row)\n",
    "\n",
    "# Create DataFrames for verification\n",
    "scenes_to_update_df = (\n",
    "    pl.DataFrame(scenes_to_update) if scenes_to_update else pl.DataFrame()\n",
    ")\n",
    "scenes_already_set_df = (\n",
    "    pl.DataFrame(scenes_already_set) if scenes_already_set else pl.DataFrame()\n",
    ")\n",
    "\n",
    "print(f\"\\nScenes that need updating: {len(scenes_to_update_df)}\")\n",
    "print(f\"Scenes already set (skipped): {len(scenes_already_set_df)}\")\n",
    "\n",
    "# Write debug outputs\n",
    "if len(scenes_to_update_df) > 0:\n",
    "    write_debug_json(scenes_to_update_df, \"scenes_to_update_df\")\n",
    "if len(scenes_already_set_df) > 0:\n",
    "    write_debug_json(scenes_already_set_df, \"scenes_already_set_df\")\n",
    "\n",
    "if len(scenes_already_set_df) > 0:\n",
    "    print(\"\\nScenes already set with matching Culture Extractor stash ID:\")\n",
    "    display(\n",
    "        scenes_already_set_df.select(\n",
    "            [\n",
    "                \"scene_id\",\n",
    "                \"scene_title\",\n",
    "                \"file_basename\",\n",
    "                \"ce_downloads_release_name\",\n",
    "                \"ce_downloads_release_uuid\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nScenes to be updated:\")\n",
    "\n",
    "scenes_to_update_df.select(\n",
    "    [\n",
    "        \"scene_id\",\n",
    "        \"scene_title\",\n",
    "        \"file_basename\",\n",
    "        \"ce_downloads_release_name\",\n",
    "        \"ce_downloads_release_uuid\",\n",
    "        \"ce_downloads_site_name\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_updates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply step: Update scenes with Culture Extractor release UUIDs as stash_ids\n",
    "# (preserving existing stash_ids from other endpoints)\n",
    "\n",
    "update_results = []\n",
    "\n",
    "for row in scenes_to_update_df.iter_rows(named=True):\n",
    "    scene_id = row[\"scene_id\"]\n",
    "    ce_release_uuid = row[\"ce_downloads_release_uuid\"]\n",
    "    scene_title = row[\"scene_title\"]\n",
    "    file_basename = row[\"file_basename\"]\n",
    "    ce_release_name = row[\"ce_downloads_release_name\"]\n",
    "    ce_site_name = row[\"ce_downloads_site_name\"]\n",
    "    existing_stash_ids = row[\"existing_stash_ids\"] or []\n",
    "\n",
    "    try:\n",
    "        # Merge stash_ids preserving existing ones\n",
    "        merged_stash_ids = merge_stash_ids(\n",
    "            existing_stash_ids, CULTURE_EXTRACTOR_ENDPOINT, ce_release_uuid\n",
    "        )\n",
    "\n",
    "        # Update the scene with merged stash_ids\n",
    "        result = stash_raw_client.update_scene(\n",
    "            {\n",
    "                \"id\": scene_id,\n",
    "                \"stash_ids\": merged_stash_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        update_results.append(\n",
    "            {\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_title\": scene_title,\n",
    "                \"file_basename\": file_basename,\n",
    "                \"ce_site_name\": ce_site_name,\n",
    "                \"ce_release_name\": ce_release_name,\n",
    "                \"ce_release_uuid\": ce_release_uuid,\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"✓ Updated scene {scene_id} ({scene_title}) with CE UUID {ce_release_uuid} \"\n",
    "            f\"from {ce_site_name} - {ce_release_name} (preserved {len(existing_stash_ids)} existing stash_ids)\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        update_results.append(\n",
    "            {\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_title\": scene_title,\n",
    "                \"file_basename\": file_basename,\n",
    "                \"ce_site_name\": ce_site_name,\n",
    "                \"ce_release_name\": ce_release_name,\n",
    "                \"ce_release_uuid\": ce_release_uuid,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"✗ Failed to update scene {scene_id} ({scene_title}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of apply step results\n",
    "if update_results:\n",
    "    update_results_df = pl.DataFrame(update_results)\n",
    "    print(f\"Total scenes processed: {len(update_results_df)}\")\n",
    "    print(\n",
    "        f\"Successful updates: {len(update_results_df.filter(pl.col('status') == 'success'))}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Failed updates: {len(update_results_df.filter(pl.col('status') == 'error'))}\"\n",
    "    )\n",
    "\n",
    "    # Write debug output\n",
    "    write_debug_json(update_results_df, \"update_results_df\")\n",
    "\n",
    "    # Show any errors\n",
    "    errors_df = update_results_df.filter(pl.col(\"status\") == \"error\")\n",
    "    if len(errors_df) > 0:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        display(errors_df)\n",
    "\n",
    "    # Show successful updates\n",
    "    success_df = update_results_df.filter(pl.col(\"status\") == \"success\")\n",
    "    print(\"\\nSuccessful updates:\")\n",
    "    success_df\n",
    "else:\n",
    "    print(\"No scenes needed updating.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CultureDataAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
