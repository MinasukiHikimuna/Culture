{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "from libraries.client_stashapp import StashAppClient, get_stashapp_client\n",
    "\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "stash_client = StashAppClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lum-studio\"\n",
    ")\n",
    "\n",
    "system_prompt_release_name = \"\"\"\n",
    "Extract a release name from these poorly named zip files.\n",
    "Zip names may contain studio name X-Art in some variations.\n",
    "It might contain quality like lrg which means large.\n",
    "File names can contain performer names like silvie here.\n",
    "There can be multiple performers.\n",
    "Performer names and qualities and similar things should be removed.\n",
    "Only answer with the likely release name, no other text.\n",
    "\n",
    "Examples:\n",
    "X-Art - 2013-01-09 - Susie & Clover - Warm Inside.zip => Warm Inside\n",
    "x-art hayden h the dressing room-lrg.zip => The Dressing Room\n",
    "TayTO-X-Art.13.12.02.Scarlet.Lucky.Man.IMAGESET.zip => Lucky Man\n",
    "x-art_leila_carmen_christmas_vacation-lrg.zip => Christmas Vacation\n",
    "X-Art.com_12.05.07.Carla.Golden.Blonde.XXX.IMAGESET-FuGLi.zip => Golden Blonde\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_release_date = \"\"\"\n",
    "Extract a release date from these poorly named zip files.\n",
    "Zip names may contain studio name X-Art in some variations.\n",
    "It might contain quality like lrg which means large.\n",
    "File names can contain performer names like silvie here.\n",
    "There can be multiple performers.\n",
    "Performer names and qualities and similar things should be removed.\n",
    "Only answer with the likely release date in YYYY-MM-DD format,\n",
    "no other text. If there is no date in the zip file name, return None.\n",
    "\n",
    "Examples:\n",
    "X-Art - 2013-01-09 - Susie & Clover - Warm Inside.zip => 2013-01-09\n",
    "x-art hayden h the dressing room-lrg.zip => None\n",
    "TayTO-X-Art.13.12.02.Scarlet.Lucky.Man.IMAGESET.zip => 2013-12-02\n",
    "x-art_leila_carmen_christmas_vacation-lrg.zip => None\n",
    "X-Art com 17 11 29 Susie 5 Reasons To Love Sex With Blondes XXX IMAGESET-FuGLi.zip => 2017-11-29\n",
    "X-Art.com_12.05.07.Carla.Golden.Blonde.XXX.IMAGESET-FuGLi.zip => 2012-05-07\n",
    "X-Art.com_12.08.30.Leila.Side.By.Side.XXX.IMAGESET-FuGLi.zip => 2012-08-30\n",
    "x-art hayden h summer plaything-lrg.zip => None\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_release_name(zip_file_name):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_release_name},\n",
    "            {\"role\": \"user\", \"content\": zip_file_name}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def get_release_date(zip_file_name):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama-3-8b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_release_date},\n",
    "            {\"role\": \"user\", \"content\": zip_file_name}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xart_models_df = pl.read_parquet(\"H:\\\\Parquet Data\\\\X-Art\\\\xart_models.parquet\")\n",
    "xart_models_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xart_indexxx_models_df = pl.read_parquet(\"H:\\\\Parquet Data\\\\X-Art\\\\xart_indexxx_models.parquet\")\n",
    "xart_indexxx_models_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xart_stashdb_models_df = pl.read_parquet(\"H:\\\\Parquet Data\\\\X-Art\\\\xart_stashdb_models.parquet\")\n",
    "xart_stashdb_models_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xart_indexxx_joined_models_df = (\n",
    "    xart_models_df.join(\n",
    "        xart_indexxx_models_df,\n",
    "        how=\"left\",\n",
    "        left_on=pl.col(\"name\").str.to_lowercase().str.replace(r\"\\.\", \"\"),\n",
    "        right_on=pl.col(\"site_name\").str.to_lowercase().str.replace(r\"\\.\", \"\"),\n",
    "        coalesce=False\n",
    "    )\n",
    ")\n",
    "\n",
    "unmatched_with_indexxx = xart_indexxx_joined_models_df.filter(pl.col(\"site_name\").is_null())\n",
    "if len(unmatched_with_indexxx) > 0:\n",
    "    print(unmatched_with_indexxx)\n",
    "\n",
    "xart_indexxx_joined_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_models_with_aliases(indexxx_df: pl.DataFrame, stashdb_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Match models between indexxx and stashdb dataframes, using both name and aliases\n",
    "    \"\"\"\n",
    "    # First do direct name matching\n",
    "    result_df = indexxx_df.join(\n",
    "        stashdb_df,\n",
    "        left_on=\"general_name\",\n",
    "        right_on=\"name\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # For unmatched rows, try matching against aliases\n",
    "    unmatched_mask = result_df[\"id\"].is_null()  # Use id to check for matches\n",
    "    unmatched_df = result_df.filter(unmatched_mask)\n",
    "\n",
    "    # Explode aliases into separate rows\n",
    "    stashdb_aliases = stashdb_df.explode(\"aliases\").select([\"name\", \"aliases\"])\n",
    "\n",
    "    # Match against aliases\n",
    "    alias_matches = unmatched_df.join(\n",
    "        stashdb_aliases,\n",
    "        left_on=\"general_name\",\n",
    "        right_on=\"aliases\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get the main stashdb data for alias matches\n",
    "    alias_matched = alias_matches.filter(pl.col(\"name\").is_not_null()).join(\n",
    "        stashdb_df,\n",
    "        left_on=\"name\",  # Use the name column from the alias match\n",
    "        right_on=\"name\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_right\"  # Add suffix to avoid column conflicts\n",
    "    )\n",
    "\n",
    "    # First drop the original columns that we want to replace with _right versions\n",
    "    columns_to_drop = [\"id\", \"disambiguation\", \"deleted\", \"aliases\", \"gender\", \"birth_date\", \"is_favorite\", \"images\", \"scenes\"]\n",
    "    keep_columns = [col for col in alias_matched.columns if col not in columns_to_drop]\n",
    "\n",
    "    # Create a mapping of right columns to their original names\n",
    "    column_mapping = {f\"{col}_right\": col for col in columns_to_drop}\n",
    "\n",
    "    # Select and rename columns\n",
    "    alias_matched = alias_matched.select([\n",
    "        *keep_columns,  # Keep columns we didn't drop\n",
    "        *(pl.col(right_col).alias(original_col) for right_col, original_col in column_mapping.items())\n",
    "    ])\n",
    "\n",
    "    # Make sure alias_matched has the same columns as result_df\n",
    "    alias_matched = alias_matched.select(result_df.columns)\n",
    "\n",
    "    # Update the original result with alias matches\n",
    "    result_df = pl.concat([\n",
    "        result_df.filter(~unmatched_mask),  # Keep direct matches\n",
    "        alias_matched  # Add alias matches\n",
    "    ])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Usage\n",
    "xart_indexxx_stashdb_joined_models_df = match_models_with_aliases(\n",
    "    xart_indexxx_models_df,\n",
    "    xart_stashdb_models_df\n",
    ")\n",
    "\n",
    "# Check match statistics\n",
    "total_models = len(xart_indexxx_stashdb_joined_models_df)\n",
    "matched_models = xart_indexxx_stashdb_joined_models_df.filter(\n",
    "    pl.col(\"id\").is_not_null()  # Use id to check for matches\n",
    ").height\n",
    "\n",
    "print(f\"\\nMatch Statistics:\")\n",
    "print(f\"Total models: {total_models}\")\n",
    "print(f\"Matched models: {matched_models}\")\n",
    "print(f\"Match rate: {matched_models/total_models:.1%}\")\n",
    "\n",
    "# See which models weren't matched\n",
    "unmatched = xart_indexxx_stashdb_joined_models_df.filter(\n",
    "    pl.col(\"id\").is_null()  # Use id to check for matches\n",
    ").select([\"site_name\", \"general_name\"])\n",
    "\n",
    "# See which models were matched via aliases\n",
    "alias_matches = xart_indexxx_stashdb_joined_models_df.filter(\n",
    "    pl.col(\"aliases\").is_not_null() &\n",
    "    pl.col(\"id\").is_not_null()  # Use id to check for matches\n",
    ").select([\"site_name\", \"general_name\", \"aliases\"])  # Remove duplicate general_name\n",
    "\n",
    "if len(unmatched) > 0:\n",
    "    print(\"\\nUnmatched models:\")\n",
    "    print(unmatched)\n",
    "if len(alias_matches) > 0:\n",
    "    print(\"\\nAlias matches:\")\n",
    "    print(alias_matches)\n",
    "\n",
    "xart_indexxx_stashdb_joined_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stash_performers = stash_client.get_performers()\n",
    "xart_indexxx_stashdb_stashapp_joined_models_df = (\n",
    "    xart_indexxx_stashdb_joined_models_df.join(\n",
    "        stash_performers,\n",
    "        how=\"left\",\n",
    "        left_on=\"id\",\n",
    "        right_on=\"stashapp_stashdb_id\",\n",
    "        suffix=\"stashapp_\",\n",
    "    )\n",
    ")\n",
    "xart_indexxx_stashdb_stashapp_joined_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xart_scenes = pl.read_parquet(\"H:\\\\Parquet Data\\\\X-Art\\\\xart_scenes.parquet\")\n",
    "xart_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenes = selected_scenes.slice(2, 1)\n",
    "selected_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_name = \"moving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_id = 7479\n",
    "print(stash.find_gallery(gallery_id)[\"files\"][0][\"basename\"])\n",
    "gallery_name = get_release_name(stash.find_gallery(gallery_id)[\"files\"][0][\"basename\"])\n",
    "print(gallery_name)\n",
    "# gallery_date = get_release_date(stash.find_gallery(gallery_id)[\"files\"][0][\"basename\"])\n",
    "# print(gallery_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter scenes by title, type and date if available\n",
    "selected_scenes = xart_scenes.sort(by=\"date\").filter(\n",
    "    pl.col(\"title\").str.to_lowercase().str.contains(gallery_name.lower()) &\n",
    "    pl.col(\"type\").str.contains(\"gallery\") # &\n",
    "    # (gallery_date is None or pl.col(\"date\") == gallery_date)  # Only apply date filter if gallery_date exists\n",
    ")\n",
    "selected_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scene performers\n",
    "scene_performers = (\n",
    "    selected_scenes\n",
    "    .select(pl.col(\"performers\"))\n",
    "    .explode(\"performers\")\n",
    "    .unique()\n",
    "    .filter(pl.col(\"performers\").is_not_null())\n",
    "    .sort(by=\"performers\")\n",
    ")\n",
    "\n",
    "scene_performers_joined = scene_performers.join(\n",
    "    xart_indexxx_stashdb_stashapp_joined_models_df,\n",
    "    how=\"left\",\n",
    "    left_on=pl.col(\"performers\").str.to_lowercase(),\n",
    "    right_on=pl.col(\"site_name\").str.to_lowercase(),\n",
    "    suffix=\"metadata_\",\n",
    "    coalesce=False\n",
    ")\n",
    "\n",
    "scene_performers_joined.select(\"general_name\", \"stashapp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performer_ids = scene_performers_joined.select(\"stashapp_id\").drop_nulls().to_series().to_list()\n",
    "\n",
    "refreshed_gallery = stash.find_gallery(gallery_id)\n",
    "stash.update_gallery({\n",
    "    \"id\": gallery_id,\n",
    "    \"title\": selected_scenes.select(pl.col(\"title\")).to_series().to_list()[0],\n",
    "    \"date\": selected_scenes.select(pl.col(\"date\")).to_series().to_list()[0],\n",
    "    \"details\": selected_scenes.select(pl.col(\"description\")).to_series().to_list()[0],\n",
    "    \"urls\": list(set(refreshed_gallery[\"urls\"] + [selected_scenes.select(pl.col(\"url\")).to_series().to_list()[0]])),\n",
    "    \"performer_ids\": performer_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting metadata from Indexxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\nfrom pathlib import Path\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nclass Release:\n    def __init__(self, url, title, date, models):\n        self.url = url\n        self.title = title\n        self.date = date\n        self.models = models  # List of (official_name, alias) tuples\n\n    def to_dict(self):\n        return {\n            \"url\": self.url,\n            \"title\": self.title,\n            \"date\": self.date,\n            \"models\": [{\"official_name\": m[0], \"alias\": m[1]} for m in self.models]\n        }\n\nclass Scraper:\n    def __init__(self, base_url=\"https://www.indexxx.com\", site_id=None, site_slug=None):\n        self.base_url = base_url\n        self.site_id = site_id\n        self.site_slug = site_slug\n        self.session = requests.Session()\n\n        # Add comprehensive headers\n        self.session.headers.update({\n            \"authority\": \"www.indexxx.com\",\n            \"accept\": (\n                \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n                \"image/avif,image/webp,image/apng,*/*;q=0.8,\"\n                \"application/signed-exchange;v=b3;q=0.7\"\n            ),\n            \"accept-encoding\": \"gzip, deflate, br, zstd\",\n            \"accept-language\": \"en-US,en;q=0.9,fi;q=0.8\",\n            \"cache-control\": \"no-cache\",\n            \"pragma\": \"no-cache\",\n            \"sec-ch-ua\": '\"Not(A:Brand\";v=\"99\", \"Microsoft Edge\";v=\"133\", \"Chromium\";v=\"133\"',\n            \"sec-ch-ua-mobile\": \"?0\",\n            \"sec-ch-ua-platform\": '\"Windows\"',\n            \"sec-fetch-dest\": \"document\",\n            \"sec-fetch-mode\": \"navigate\",\n            \"sec-fetch-site\": \"same-origin\",\n            \"sec-fetch-user\": \"?1\",\n            \"upgrade-insecure-requests\": \"1\",\n            \"user-agent\": (\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0\"\n            ),\n        })\n\n        # Add cookies\n        cf_clearance = (\n            \"gws_iACWaFPjTCrarTjqpMa82rEgRmXK45l4ndArCEY\"\n            \"-1738553527-1.2.1.1-Fn3Z5R5yOSuklpSjU6IAWyjX\"\n            \"MtrIzGuJ1Ex9VzAjwSrfoqlXCqxjnJ2JPjxqdbg34iLD\"\n            \"ziG0KE4EnkIv1PH3g6ydFu77SzoUo0IL04ojYme2kHf7u\"\n            \"czVsRTjk1MVaRBbX17a7SV2j2LyawuCMWIn3a2dqH0agR\"\n            \"ql0stNYOTT6MDHEpKkw8jA9_vP0L8hSeYcJ1YlD8g.tYm\"\n            \"SZmBYsWZMhiS6Ip7VQhqsMiNY3iHI8Be30kWItk6FMTw2\"\n            \"HcwMw4SeUkgOLv.GVjievGbzr4X4359LGWzold4LK1c9v\"\n            \".B0myA\"\n        )\n        self.session.cookies.update({\n            \"ageVerified\": \"true\",\n            \"cf_clearance\": cf_clearance,\n        })\n\n    def get_site_url(self, page=None):\n        \"\"\"Get URL for site's releases page\"\"\"\n        if not self.site_id or not self.site_slug:\n            raise ValueError(\"site_id and site_slug must be set\")\n\n        url = f\"{self.base_url}/websites/{self.site_id}/{self.site_slug}/sets/\"\n        if page is not None:\n            url += f\"?page={page}\"\n        return url\n\n    def get_soup(self, url, delay=1):\n        \"\"\"Get BeautifulSoup object for a URL with rate limiting\"\"\"\n        time.sleep(delay)  # Rate limiting\n\n        # Update referer for each request\n        self.session.headers.update({\n            \"referer\": \"/\".join(url.split(\"/\")[:-1]) if url.endswith(\"/\") else \"/\".join(url.split(\"/\")[:-2])\n        })\n\n        response = self.session.get(url)\n        if response.status_code != 200:\n            raise Exception(f\"Failed to fetch {url}: Status code {response.status_code}\")\n\n        return BeautifulSoup(response.text, \"html.parser\")\n\n    def parse_release_page(self, url):\n        \"\"\"Parse individual release page\"\"\"\n        soup = self.get_soup(url)\n\n        # Extract basic info\n        date = soup.find(\"span\", {\"itemprop\": \"datePublished\"}).text\n        title = soup.find(\"span\", {\"itemprop\": \"description\"}).text\n\n        # Extract models\n        models = []\n        model_section = soup.find(\"div\", {\"itemprop\": \"about\"})\n        if model_section:\n            for model in model_section.find_all(\"div\", {\"itemtype\": \"http://schema.org/Person\"}):\n                official_name = model.find(\"span\", {\"itemprop\": \"name\"}).text\n                alias_div = model.find(\"div\", text=lambda t: t and \"as:\" in t)\n                alias = alias_div.text.replace(\"as:\", \"\").strip() if alias_div else official_name\n                models.append((official_name, alias))\n\n        return Release(url, title, date, models)\n\n    def scrape_list_page(self, page_num):\n        \"\"\"Scrape a single list page\"\"\"\n        url = self.get_site_url(page_num)\n        soup = self.get_soup(url)\n\n        releases = []\n        for pset in soup.find_all(\"div\", {\"class\": \"pset\"}):\n            link = pset.find(\"div\", {\"class\": \"my-2\"}).find(\"a\")\n            if link:\n                release_url = self.base_url + link[\"href\"]\n                try:\n                    release = self.parse_release_page(release_url)\n                    releases.append(release)\n                    print(f\"Scraped: {release.title}\")\n                except Exception as e:\n                    print(f\"Error scraping {release_url}: {e}\")\n\n        return releases\n\n    def get_last_page(self):\n        \"\"\"Get the last page number from the pagination\"\"\"\n        url = self.get_site_url()\n        soup = self.get_soup(url)\n\n        # Find the first page number in pagination (which is the last page)\n        pagination = soup.find(\"ul\", {\"class\": \"pagination\"})\n        if pagination:\n            # Skip the \"newer\" button and get the first actual page number\n            pages = pagination.find_all(\"li\", {\"class\": \"page-item\"})\n            for page in pages:\n                if \"disabled\" not in page.get(\"class\", []):\n                    try:\n                        return int(page.find(\"a\").text)\n                    except (ValueError, AttributeError):\n                        continue\n\n        raise ValueError(\"Could not determine last page number\")\n\n    def scrape_all_pages(self, start_page=1, end_page=None):\n        \"\"\"Scrape all pages in range\"\"\"\n        if end_page is None:\n            end_page = self.get_last_page()\n            print(f\"Detected {end_page} total pages\")\n\n        all_releases = []\n\n        for page in range(start_page, end_page + 1):\n            print(f\"\\nScraping page {page}/{end_page}...\")\n            releases = self.scrape_list_page(page)\n            all_releases.extend(releases)\n\n            # Save progress after each page\n            self.save_releases(all_releases, f\"releases_progress_p{page}.json\")\n\n        return all_releases\n\n    def save_releases(self, releases, filename):\n        \"\"\"Save releases to JSON file\"\"\"\n        with Path(filename).open(\"w\", encoding=\"utf-8\") as f:\n            json.dump([r.to_dict() for r in releases], f, indent=2)\n\n# Example usage for x-art\nscraper = Scraper(\n    site_id=\"293\",\n    site_slug=\"x-art\"\n)\nreleases = scraper.scrape_all_pages()\nscraper.save_releases(releases, \"releases_final.json\")\n\n# Could also scrape other sites\n# scraper2 = Scraper(site_id=\"123\", site_slug=\"other-site\")\n# releases2 = scraper2.scrape_all_pages()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting models from StashDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "import polars as pl\n",
    "import requests\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Define the headers, including any necessary authentication\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"ApiKey\": os.getenv(\"ApiKey\"),\n",
    "}\n",
    "\n",
    "def gql_query(query, variables=None):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if os.getenv(\"STASHDB_API_KEY\"):\n",
    "        headers[\"Apikey\"] = os.getenv(\"STASHDB_API_KEY\")\n",
    "    response = requests.post(\n",
    "        \"https://stashdb.org/graphql\",\n",
    "        json={\"query\": query, \"variables\": variables},\n",
    "        headers=headers,\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Query failed with status code {response.status_code}: {response.text}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query = \"\"\"\n",
    "    query StudioPerformers(\n",
    "        $studioId: ID!, \n",
    "        $gender: GenderFilterEnum, \n",
    "        $favorite: Boolean, \n",
    "        $names: String, \n",
    "        $page: Int! = 1, \n",
    "        $per_page: Int! = 25, \n",
    "        $direction: SortDirectionEnum!, \n",
    "        $sort: PerformerSortEnum!\n",
    "    ) {\n",
    "        queryPerformers(\n",
    "            input: {\n",
    "                studio_id: $studioId,\n",
    "                gender: $gender,\n",
    "                is_favorite: $favorite,\n",
    "                names: $names,\n",
    "                page: $page,\n",
    "                per_page: $per_page,\n",
    "                direction: $direction,\n",
    "                sort: $sort\n",
    "            }\n",
    "        ) {\n",
    "            count\n",
    "            performers {\n",
    "                id\n",
    "                name\n",
    "                disambiguation\n",
    "                deleted\n",
    "                aliases\n",
    "                gender\n",
    "                birth_date\n",
    "                is_favorite\n",
    "                images {\n",
    "                    ...ImageFragment\n",
    "                }\n",
    "                scenes(input: {studio_id: $studioId}) {\n",
    "                    id\n",
    "                    title\n",
    "                    duration\n",
    "                    release_date\n",
    "                    production_date\n",
    "                    studio {\n",
    "                        id\n",
    "                        name\n",
    "                    }\n",
    "                    images {\n",
    "                        ...ImageFragment\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fragment ImageFragment on Image {\n",
    "        id\n",
    "        url\n",
    "        width\n",
    "        height\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "studio_id = \"d22943f5-9bb5-495e-8b01-6e12d2fffc80\"\n",
    "\n",
    "all_performers = []\n",
    "for page in range(1, 18):\n",
    "    variables = {\n",
    "        \"studioId\": studio_id,\n",
    "        \"page\": page,\n",
    "        \"per_page\": 25,\n",
    "        \"sort\": \"LAST_SCENE\",\n",
    "        \"direction\": \"DESC\"\n",
    "    }\n",
    "\n",
    "    data = gql_query(\n",
    "        query,\n",
    "        variables\n",
    "    )\n",
    "\n",
    "    all_performers.extend(data[\"data\"][\"queryPerformers\"][\"performers\"])\n",
    "\n",
    "all_performers_df = pl.DataFrame(all_performers)\n",
    "all_performers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_performers_df.write_parquet(\"H:\\\\Parquet Data\\\\xart_stashdb_all_performers.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}