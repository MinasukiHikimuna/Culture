{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29904f4f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))\n",
    "\n",
    "from libraries.client_stashapp import StashAppClient, get_stashapp_client\n",
    "\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "stash_client = StashAppClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00019a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oshash_and_duration_match_tag = stash.find_tag({ \"name\": \"Duplicate: OSHASH And Duration Match\" })\n",
    "duration_match_tag = stash.find_tag({ \"name\": \"Duplicate: Duration Match\" })\n",
    "scenes_with_multiple_versions = stash.find_tag({ \"name\": \"Scene: Multiple Versions\" })\n",
    "duration_mismatch_tag = stash.find_tag({ \"name\": \"Duplicate: Duration Mismatch\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_with_dupes = stash.find_scenes({\n",
    "  \"file_count\": {\n",
    "    \"modifier\": \"GREATER_THAN\",\n",
    "    \"value\": 1\n",
    "  },\n",
    "  \"tags\": {\n",
    "    \"value\": [],\n",
    "    \"modifier\": \"INCLUDES\",\n",
    "    \"excludes\": [scenes_with_multiple_versions[\"id\"]]\n",
    "  }\n",
    "}, fragment=\"id title date studio { name } files { id duration path width height size fingerprints { type value } }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all file records\n",
    "file_records = []\n",
    "\n",
    "for scene in scenes_with_dupes:\n",
    "    for i, file in enumerate(scene[\"files\"]):\n",
    "        # Extract fingerprints\n",
    "        oshash = next((fp[\"value\"] for fp in file[\"fingerprints\"] if fp[\"type\"] == \"oshash\"), None)\n",
    "        phash = next((fp[\"value\"] for fp in file[\"fingerprints\"] if fp[\"type\"] == \"phash\"), None)\n",
    "\n",
    "        # Create a record for each file\n",
    "        record = {\n",
    "            \"scene_id\": scene[\"id\"],\n",
    "            \"title\": scene[\"title\"],\n",
    "            \"date\": scene[\"date\"],\n",
    "            \"studio_name\": scene[\"studio\"][\"name\"] if scene[\"studio\"] else None,\n",
    "            \"file_id\": file[\"id\"],\n",
    "            \"file_path\": file[\"path\"],\n",
    "            \"resolution_width\": file[\"width\"],\n",
    "            \"resolution_height\": file[\"height\"],\n",
    "            \"size\": file[\"size\"],\n",
    "            \"duration\": file[\"duration\"],\n",
    "            \"oshash\": oshash,\n",
    "            \"phash\": phash,\n",
    "            \"is_primary\": i == 0  # True if this is the first file in the scene's files list\n",
    "        }\n",
    "        file_records.append(record)\n",
    "\n",
    "# Create Polars DataFrame\n",
    "scenes_with_multiple_files_df = pl.DataFrame(file_records)\n",
    "scenes_with_multiple_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f31c56",
   "metadata": {},
   "source": [
    "# Deleting files where OSHASH is identical to another file in the scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ae09e",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n\n# Filter to only show rows that have matching oshash values with other rows\nduplicate_files_df = scenes_with_multiple_files_df.filter(\n    pl.col(\"oshash\").is_in(\n        scenes_with_multiple_files_df.group_by(\"oshash\")\n        .agg(pl.len().alias(\"dupe_count\"))  # Changed to use len() and a unique alias\n        .filter(pl.col(\"dupe_count\") > 1)\n        .get_column(\"oshash\")\n    )\n).sort([\"oshash\", \"file_id\"])\n\nduplicate_files_df = duplicate_files_df.with_columns(\n    pl.col(\"file_path\")\n    .map_elements(lambda x: os.path.exists(x), return_dtype=pl.Boolean)\n    .alias(\"file_path_exists\")\n)\n\nprint(\"\\nFiles with duplicate oshash values:\")\nprint(duplicate_files_df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f5fde",
   "metadata": {},
   "outputs": [],
   "source": "# Group by scene_id and oshash to find duplicates within each scene\ngrouped_files = duplicate_files_df.group_by([\"scene_id\", \"oshash\"]).agg([\n    pl.col(\"file_id\").min().alias(\"keep_file_id\"),  # File to keep\n    pl.col(\"file_id\").alias(\"all_file_ids\"),        # All file IDs\n    pl.col(\"file_path\").alias(\"all_file_paths\"),    # All file paths\n    pl.col(\"size\").alias(\"all_file_sizes\"),         # File sizes\n    pl.col(\"file_path_exists\").alias(\"all_file_exists\"),  # Path existence check\n    pl.col(\"is_primary\").alias(\"all_is_primary\")    # Primary file flags\n])\n\n# Lists to store kept and deleted file information\nkept_files = []\nfiles_to_delete = []\n\n# Print summary and collect file information\nprint(\"\\nFiles to be deleted:\")\nfor row in grouped_files.iter_rows(named=True):\n    file_ids = row[\"all_file_ids\"]\n    file_paths = row[\"all_file_paths\"]\n    file_sizes = row[\"all_file_sizes\"]\n    is_primary = row[\"all_is_primary\"]\n\n    # Find primary file index if it exists\n    primary_indices = [i for i, p in enumerate(is_primary) if p]\n    if primary_indices:\n        # Keep the primary file\n        keep_index = primary_indices[0]\n        keep_file_id = file_ids[keep_index]\n    else:\n        # If no primary file, keep the one with lowest file_id\n        keep_file_id = row[\"keep_file_id\"]\n        keep_index = file_ids.index(keep_file_id)\n\n    kept_files.append({\n        \"scene_id\": row[\"scene_id\"],\n        \"file_id\": keep_file_id,\n        \"file_path\": file_paths[keep_index],\n        \"size\": file_sizes[keep_index],\n        \"is_primary\": is_primary[keep_index]\n    })\n\n    # Get indices of files to delete (all except kept file, never delete primary files)\n    delete_indices = [\n        i for i, (file_id, p) in enumerate(zip(file_ids, is_primary))\n        if file_id != keep_file_id and not p\n    ]\n\n    if delete_indices:  # Only show if there are files to delete\n        primary_status = \" (Primary)\" if is_primary[keep_index] else \"\"\n        keep_path = file_paths[keep_index]\n        keep_size = file_sizes[keep_index]\n        keep_exists = os.path.exists(keep_path)\n        print(\n            f\"\\nScene {row['scene_id']}\"\n            f\" - Keeping{primary_status}: {keep_path}\"\n            f\" (ID: {keep_file_id},\"\n            f\" Size: {keep_size:,} bytes)\"\n            f\" Exists: {keep_exists}\"\n        )\n        for idx in delete_indices:\n            del_exists = os.path.exists(file_paths[idx])\n            print(\n                f\"  Will delete: {file_paths[idx]}\"\n                f\" (ID: {file_ids[idx]},\"\n                f\" Size: {file_sizes[idx]:,} bytes)\"\n                f\" Exists: {del_exists}\"\n            )\n            files_to_delete.append({\n                \"scene_id\": row[\"scene_id\"],\n                \"file_id\": file_ids[idx],\n                \"file_path\": file_paths[idx],\n                \"size\": file_sizes[idx],\n                \"is_primary\": is_primary[idx]\n            })\n\n# Calculate total space that would be freed\ntotal_space = sum(file[\"size\"] for file in files_to_delete)\nprint(\n    f\"\\nTotal space that would be freed:\"\n    f\" {total_space:,} bytes ({total_space/1024/1024/1024:.2f} GB)\"\n)\nprint(f\"Number of files to delete: {len(files_to_delete)}\")\n\n# Create DataFrames for kept and deleted files\nkept_files_df = pl.DataFrame(kept_files)\ndelete_files_df = pl.DataFrame(files_to_delete)"
  },
  {
   "cell_type": "markdown",
   "id": "7d28794d",
   "metadata": {},
   "source": [
    "# Switch primary files to the ones with Culture Extractor UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95905bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Function to check if a filename ends with a UUID pattern before the extension\n",
    "def has_uuid_before_extension(path):\n",
    "    # Extract the filename from the path\n",
    "    filename = os.path.basename(path)\n",
    "    # UUID pattern: 8-4-4-4-12 hexadecimal characters, allowing for a space or dash before it\n",
    "    uuid_pattern = r\"[ -][0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\.[^.]+$\"\n",
    "    return bool(re.search(uuid_pattern, filename.lower()))\n",
    "\n",
    "# Filter files that have UUID pattern\n",
    "uuid_files_df = delete_files_df.filter(\n",
    "    pl.col(\"file_path\").map_elements(has_uuid_before_extension)\n",
    ")\n",
    "\n",
    "print(\"\\nFiles to keep (they have UUID):\")\n",
    "print(uuid_files_df)\n",
    "\n",
    "# Create a DataFrame of the corresponding primary files for these UUID files\n",
    "primary_files_df = pl.DataFrame(kept_files).filter(\n",
    "    pl.col(\"scene_id\").is_in(uuid_files_df.get_column(\"scene_id\"))\n",
    ")\n",
    "\n",
    "print(\"\\nCorresponding primary files that should be switched:\")\n",
    "print(primary_files_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98778a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in uuid_files_df.iter_rows(named=True):\n",
    "    print(row)\n",
    "    stash.update_scene({\n",
    "        \"id\": row[\"scene_id\"],\n",
    "        \"primary_file_id\": row[\"file_id\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257ad7f",
   "metadata": {},
   "source": [
    "# Delete files that have identical OSHASH and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb148b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in delete_files_df.iter_rows(named=True):\n",
    "    stash.destroy_files(row[\"file_id\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}