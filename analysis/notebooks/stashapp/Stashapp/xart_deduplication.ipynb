{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Art File Deduplication and Organization\n",
    "\n",
    "This notebook implements a multi-stage matching and deduplication workflow for X-Art content:\n",
    "\n",
    "1. **Phase 1**: Export Stashapp X-Art scenes without CE UUID pattern\n",
    "2. **Phase 2**: Export Culture Extractor X-Art downloads with hashes\n",
    "3. **Phase 3**: Multi-stage matching (oshash → phash → fuzzy title)\n",
    "4. **Phase 4**: Review and approve matches\n",
    "5. **Phase 5**: Execute approved file operations\n",
    "6. **Phase 6**: Verify and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add libraries to path (same pattern as other notebooks)\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from culture_cli.modules.ce.utils.tag_matcher import calculate_similarity\n",
    "from libraries.client_culture_extractor import ClientCultureExtractor\n",
    "from libraries.client_stashapp import StashAppClient\n",
    "\n",
    "\n",
    "def hamming_distance(hash1: str, hash2: str) -> int:\n",
    "    \"\"\"Calculate the Hamming distance between two hex strings (phashes).\"\"\"\n",
    "    bin1 = bin(int(hash1, 16))[2:].zfill(64)\n",
    "    bin2 = bin(int(hash2, 16))[2:].zfill(64)\n",
    "    return sum(c1 != c2 for c1, c2 in zip(bin1, bin2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for intermediate results\n",
    "OUTPUT_DIR = Path(\"/Users/thardas/Private/Code/Culture/analysis/notebooks/stashapp/Stashapp/output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# CE UUID pattern at end of filename (before extension)\n",
    "CE_UUID_PATTERN = re.compile(r\"[ -][0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\.[^.]+$\", re.IGNORECASE)\n",
    "\n",
    "# Matching thresholds\n",
    "PHASH_HAMMING_THRESHOLD = 16  # Maximum hamming distance for phash match\n",
    "DURATION_DIFF_THRESHOLD = 60  # Maximum duration difference in seconds\n",
    "TITLE_SIMILARITY_THRESHOLD = 0.85  # Minimum Levenshtein similarity for fuzzy title match\n",
    "\n",
    "# Dry run mode (set to False to execute actual file operations)\n",
    "DRY_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FileMatch:\n",
    "    stashapp_scene_id: int\n",
    "    stashapp_title: str\n",
    "    stashapp_date: date | None\n",
    "    stashapp_file_path: str\n",
    "    stashapp_oshash: str | None\n",
    "    stashapp_phash: str | None\n",
    "    stashapp_duration: timedelta | None\n",
    "    ce_release_uuid: str\n",
    "    ce_release_name: str\n",
    "    ce_release_date: date | None\n",
    "    ce_saved_filename: str\n",
    "    ce_oshash: str | None\n",
    "    ce_phash: str | None\n",
    "    match_type: str  # \"oshash\", \"phash_duration\", \"date_title_fuzzy\", \"date_title_partial\"\n",
    "    confidence: float  # 0.0 to 1.0\n",
    "    action: str  # \"RENAME_DELETE\", \"MOVE_NEXT_TO\", \"FLAG_FOR_REVIEW\"\n",
    "    reason: str  # Human-readable explanation\n",
    "    approved: bool = False  # User approval flag\n",
    "\n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        # Convert date and timedelta to strings for JSON serialization\n",
    "        if d[\"stashapp_date\"]:\n",
    "            d[\"stashapp_date\"] = str(d[\"stashapp_date\"])\n",
    "        if d[\"ce_release_date\"]:\n",
    "            d[\"ce_release_date\"] = str(d[\"ce_release_date\"])\n",
    "        if d[\"stashapp_duration\"]:\n",
    "            d[\"stashapp_duration\"] = d[\"stashapp_duration\"].total_seconds()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Export Stashapp X-Art Scenes Without CE UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u0001d\u0002Using stash (v0.30.1-0) endpoint at https://stash.chiefsclub.com:443/graphql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found X-Art studio with ID: 6\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stashapp client\n",
    "stash = StashAppClient()\n",
    "\n",
    "# Get X-Art studio ID\n",
    "studios_df = stash.get_studios()\n",
    "xart_studio = studios_df.filter(pl.col(\"stash_studios_name\") == \"X-Art\")\n",
    "\n",
    "if len(xart_studio) == 0:\n",
    "    raise ValueError(\"X-Art studio not found in Stashapp\")\n",
    "\n",
    "xart_studio_id = xart_studio[\"stash_studios_id\"][0]\n",
    "print(f\"Found X-Art studio with ID: {xart_studio_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying X-Art scenes from Stashapp...\n",
      "Found 1021 X-Art scenes\n",
      "Found 935 scenes without CE UUID pattern\n"
     ]
    }
   ],
   "source": [
    "# Query all X-Art scenes\n",
    "print(\"Querying X-Art scenes from Stashapp...\")\n",
    "xart_scenes_df = stash.find_scenes_by_studio([xart_studio_id])\n",
    "print(f\"Found {len(xart_scenes_df)} X-Art scenes\")\n",
    "\n",
    "# Check if filename has CE UUID pattern\n",
    "xart_scenes_df = xart_scenes_df.with_columns(\n",
    "    pl.col(\"stashapp_primary_file_basename\").map_elements(\n",
    "        lambda x: bool(CE_UUID_PATTERN.search(x)) if x else False,\n",
    "        return_dtype=pl.Boolean\n",
    "    ).alias(\"has_ce_uuid\")\n",
    ")\n",
    "\n",
    "# Filter for scenes WITHOUT CE UUID and select/rename relevant columns\n",
    "stashapp_df = xart_scenes_df.filter(~pl.col(\"has_ce_uuid\")).select([\n",
    "    pl.col(\"stashapp_id\").alias(\"scene_id\"),\n",
    "    pl.col(\"stashapp_title\").alias(\"title\"),\n",
    "    pl.col(\"stashapp_date\").alias(\"date\"),\n",
    "    pl.col(\"stashapp_primary_file_path\").alias(\"file_path\"),\n",
    "    pl.col(\"stashapp_primary_file_basename\").alias(\"basename\"),\n",
    "    pl.col(\"stashapp_primary_file_oshash\").alias(\"oshash\"),\n",
    "    pl.col(\"stashapp_primary_file_phash\").alias(\"phash\"),\n",
    "    pl.col(\"stashapp_primary_file_duration\").alias(\"duration\"),\n",
    "    pl.col(\"has_ce_uuid\"),\n",
    "])\n",
    "\n",
    "print(f\"Found {len(stashapp_df)} scenes without CE UUID pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stashapp DataFrame shape: (935, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>scene_id</th><th>title</th><th>date</th><th>file_path</th><th>basename</th><th>oshash</th><th>phash</th><th>duration</th><th>has_ce_uuid</th></tr><tr><td>i64</td><td>str</td><td>date</td><td>str</td><td>str</td><td>str</td><td>str</td><td>duration[ms]</td><td>bool</td></tr></thead><tbody><tr><td>158</td><td>&quot;A Long Time Cumming&quot;</td><td>2018-05-24</td><td>&quot;Z:\\Culture\\Videos\\Sites\\Malibu…</td><td>&quot;Malibu Media꞉ X-Art – 2018-05-…</td><td>&quot;50b16d5b4cc22782&quot;</td><td>&quot;cac478f840e97ba5&quot;</td><td>18m 30s 600ms</td><td>false</td></tr><tr><td>290</td><td>&quot;Blondes Have More Fun&quot;</td><td>2016-06-08</td><td>&quot;Z:\\Culture\\Videos\\Sites\\Malibu…</td><td>&quot;Malibu Media꞉ X-Art – 2016-06-…</td><td>&quot;170166e84c21588d&quot;</td><td>&quot;fbda9da194b048c3&quot;</td><td>17m 10s 260ms</td><td>false</td></tr><tr><td>415</td><td>&quot;Let Me Take Your Picture&quot;</td><td>2013-09-14</td><td>&quot;Z:\\Culture\\Videos\\Sites\\Malibu…</td><td>&quot;Malibu Media꞉ X-Art – 2013-09-…</td><td>&quot;eb4174867c51b83e&quot;</td><td>&quot;ddc3094d991166ee&quot;</td><td>17m 5s 700ms</td><td>false</td></tr><tr><td>624</td><td>&quot;Coming Late&quot;</td><td>2014-06-19</td><td>&quot;Z:\\Culture\\Videos\\Sites\\Malibu…</td><td>&quot;Malibu Media꞉ X-Art – 2014-06-…</td><td>&quot;ea62317d7272a7a3&quot;</td><td>&quot;d4e0f233b4f2985c&quot;</td><td>24m 45s 440ms</td><td>false</td></tr><tr><td>724</td><td>&quot;Czechmates&quot;</td><td>2016-04-01</td><td>&quot;Z:\\Culture\\Videos\\Sites\\Malibu…</td><td>&quot;Malibu Media꞉ X-Art – 2016-04-…</td><td>&quot;105772790cc6245f&quot;</td><td>&quot;e5acd29cac964963&quot;</td><td>28m 29s 190ms</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ scene_id ┆ title     ┆ date      ┆ file_path ┆ … ┆ oshash    ┆ phash     ┆ duration  ┆ has_ce_uu │\n",
       "│ ---      ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ id        │\n",
       "│ i64      ┆ str       ┆ date      ┆ str       ┆   ┆ str       ┆ str       ┆ duration[ ┆ ---       │\n",
       "│          ┆           ┆           ┆           ┆   ┆           ┆           ┆ ms]       ┆ bool      │\n",
       "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 158      ┆ A Long    ┆ 2018-05-2 ┆ Z:\\Cultur ┆ … ┆ 50b16d5b4 ┆ cac478f84 ┆ 18m 30s   ┆ false     │\n",
       "│          ┆ Time      ┆ 4         ┆ e\\Videos\\ ┆   ┆ cc22782   ┆ 0e97ba5   ┆ 600ms     ┆           │\n",
       "│          ┆ Cumming   ┆           ┆ Sites\\Mal ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆           ┆           ┆ ibu…      ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 290      ┆ Blondes   ┆ 2016-06-0 ┆ Z:\\Cultur ┆ … ┆ 170166e84 ┆ fbda9da19 ┆ 17m 10s   ┆ false     │\n",
       "│          ┆ Have More ┆ 8         ┆ e\\Videos\\ ┆   ┆ c21588d   ┆ 4b048c3   ┆ 260ms     ┆           │\n",
       "│          ┆ Fun       ┆           ┆ Sites\\Mal ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆           ┆           ┆ ibu…      ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 415      ┆ Let Me    ┆ 2013-09-1 ┆ Z:\\Cultur ┆ … ┆ eb4174867 ┆ ddc3094d9 ┆ 17m 5s    ┆ false     │\n",
       "│          ┆ Take Your ┆ 4         ┆ e\\Videos\\ ┆   ┆ c51b83e   ┆ 91166ee   ┆ 700ms     ┆           │\n",
       "│          ┆ Picture   ┆           ┆ Sites\\Mal ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆           ┆           ┆ ibu…      ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 624      ┆ Coming    ┆ 2014-06-1 ┆ Z:\\Cultur ┆ … ┆ ea62317d7 ┆ d4e0f233b ┆ 24m 45s   ┆ false     │\n",
       "│          ┆ Late      ┆ 9         ┆ e\\Videos\\ ┆   ┆ 272a7a3   ┆ 4f2985c   ┆ 440ms     ┆           │\n",
       "│          ┆           ┆           ┆ Sites\\Mal ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆           ┆           ┆ ibu…      ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 724      ┆ Czechmate ┆ 2016-04-0 ┆ Z:\\Cultur ┆ … ┆ 105772790 ┆ e5acd29ca ┆ 28m 29s   ┆ false     │\n",
       "│          ┆ s         ┆ 1         ┆ e\\Videos\\ ┆   ┆ cc6245f   ┆ c964963   ┆ 190ms     ┆           │\n",
       "│          ┆           ┆           ┆ Sites\\Mal ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆           ┆           ┆ ibu…      ┆   ┆           ┆           ┆           ┆           │\n",
       "└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Stashapp DataFrame\n",
    "print(f\"Stashapp DataFrame shape: {stashapp_df.shape}\")\n",
    "stashapp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Stashapp scenes to: /Users/thardas/Private/Code/Culture/analysis/notebooks/stashapp/Stashapp/output/stashapp_xart_scenes_without_ce_uuid.json\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON for inspection\n",
    "output_file = OUTPUT_DIR / \"stashapp_xart_scenes_without_ce_uuid.json\"\n",
    "stashapp_df.write_json(output_file)\n",
    "print(f\"Saved Stashapp scenes to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Export Culture Extractor X-Art Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found X-Art site with UUID: 019c4c0b-ceda-7706-8658-01ff934bced5\n"
     ]
    }
   ],
   "source": [
    "# Initialize CE client\n",
    "ce_connection_string = (\n",
    "    f\"dbname={os.environ.get('CE_DB_NAME')} \"\n",
    "    f\"user={os.environ.get('CE_DB_USERNAME')} \"\n",
    "    f\"password={os.environ.get('CE_DB_PASSWORD')} \"\n",
    "    f\"host={os.environ.get('CE_DB_HOST')} \"\n",
    "    f\"port={os.environ.get('CE_DB_PORT')}\"\n",
    ")\n",
    "ce_client = ClientCultureExtractor(ce_connection_string)\n",
    "\n",
    "# Get X-Art site UUID\n",
    "sites_df = ce_client.get_sites()\n",
    "xart_site = sites_df.filter(pl.col(\"ce_sites_name\") == \"X-Art\")\n",
    "\n",
    "if len(xart_site) == 0:\n",
    "    raise ValueError(\"X-Art site not found in Culture Extractor database\")\n",
    "\n",
    "xart_site_uuid = xart_site[\"ce_sites_uuid\"][0]\n",
    "print(f\"Found X-Art site with UUID: {xart_site_uuid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying X-Art downloads from Culture Extractor...\n",
      "Found 1915 total downloads\n",
      "Found 514 video downloads\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 27)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ce_downloads_site_uuid</th><th>ce_downloads_site_name</th><th>ce_downloads_sub_site_name</th><th>ce_downloads_release_uuid</th><th>ce_downloads_release_date</th><th>ce_downloads_release_short_name</th><th>ce_downloads_release_name</th><th>ce_downloads_release_url</th><th>ce_downloads_release_description</th><th>ce_downloads_release_created</th><th>ce_downloads_release_last_updated</th><th>ce_downloads_release_available_files</th><th>ce_downloads_release_json_document</th><th>ce_downloads_uuid</th><th>ce_downloads_downloaded_at</th><th>ce_downloads_file_type</th><th>ce_downloads_content_type</th><th>ce_downloads_variant</th><th>ce_downloads_available_file</th><th>ce_downloads_original_filename</th><th>ce_downloads_saved_filename</th><th>ce_downloads_file_metadata</th><th>ce_downloads_performers</th><th>ce_downloads_tags</th><th>ce_downloads_hash_oshash</th><th>ce_downloads_hash_phash</th><th>ce_downloads_hash_sha256</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>date</td><td>str</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[4]]</td><td>list[struct[4]]</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;019c4c0b-ceda-7706-8658-01ff93…</td><td>&quot;X-Art&quot;</td><td>null</td><td>&quot;019c5161-9d4e-7103-986b-50d794…</td><td>2025-12-19</td><td>&quot;a-very-xart-xmas&quot;</td><td>&quot;A Very X-Art Xmas&quot;</td><td>&quot;https://www.x-art.com/members/…</td><td>&quot;Set against the glow of twinkl…</td><td>2026-02-12 10:24:37.717489</td><td>2026-02-12 10:24:37.717501</td><td>&quot;[{&quot;file_type&quot;: &quot;video&quot;, &quot;conte…</td><td>&quot;{&quot;slug&quot;: &quot;a-very-xart-xmas&quot;, &quot;…</td><td>&quot;019c5162-fbad-7649-9075-53c14d…</td><td>2026-02-12 12:26:07.405065</td><td>&quot;video&quot;</td><td>&quot;scene&quot;</td><td>&quot;4k&quot;</td><td>&quot;{&quot;file_type&quot;: &quot;video&quot;, &quot;conten…</td><td>&quot;xart_novella_night_a_very_x-ar…</td><td>&quot;X-Art - 2025-12-19 - a-very-xa…</td><td>&quot;{&quot;$type&quot;: &quot;VideoFileMetadata&quot;,…</td><td>[]</td><td>[]</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;019c4c0b-ceda-7706-8658-01ff93…</td><td>&quot;X-Art&quot;</td><td>null</td><td>&quot;019c5163-1d12-77df-b51b-7c450b…</td><td>2025-08-26</td><td>&quot;first-time-anal-adventure&quot;</td><td>&quot;First Time Anal Adventure&quot;</td><td>&quot;https://www.x-art.com/members/…</td><td>&quot;The Adventure continues... in …</td><td>2026-02-12 10:26:15.959220</td><td>2026-02-12 10:26:15.959240</td><td>&quot;[{&quot;file_type&quot;: &quot;video&quot;, &quot;conte…</td><td>&quot;{&quot;slug&quot;: &quot;first-time-anal-adve…</td><td>&quot;019c5164-3c0f-75bb-a64c-5d461e…</td><td>2026-02-12 12:27:29.424015</td><td>&quot;video&quot;</td><td>&quot;scene&quot;</td><td>&quot;4k&quot;</td><td>&quot;{&quot;file_type&quot;: &quot;video&quot;, &quot;conten…</td><td>&quot;xart_cherry_kiss_charles_first…</td><td>&quot;X-Art - 2025-08-26 - first-tim…</td><td>&quot;{&quot;$type&quot;: &quot;VideoFileMetadata&quot;,…</td><td>[]</td><td>[]</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;019c4c0b-ceda-7706-8658-01ff93…</td><td>&quot;X-Art&quot;</td><td>null</td><td>&quot;019c5164-4069-730d-96e6-a0e7c7…</td><td>2025-08-28</td><td>&quot;xart-tryout&quot;</td><td>&quot;X-Art Tryout&quot;</td><td>&quot;https://www.x-art.com/members/…</td><td>&quot;Gorgeous model Nikki Hill emai…</td><td>2026-02-12 10:27:30.541229</td><td>2026-02-12 10:27:30.541249</td><td>&quot;[{&quot;file_type&quot;: &quot;video&quot;, &quot;conte…</td><td>&quot;{&quot;slug&quot;: &quot;xart-tryout&quot;, &quot;title…</td><td>&quot;019c5165-039c-71c7-9835-bb4af0…</td><td>2026-02-12 12:28:20.508622</td><td>&quot;video&quot;</td><td>&quot;scene&quot;</td><td>&quot;4k&quot;</td><td>&quot;{&quot;file_type&quot;: &quot;video&quot;, &quot;conten…</td><td>&quot;xart_nikki_hill_tryout_solo_4k…</td><td>&quot;X-Art - 2025-08-28 - xart-tryo…</td><td>&quot;{&quot;$type&quot;: &quot;VideoFileMetadata&quot;,…</td><td>[]</td><td>[]</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;019c4c0b-ceda-7706-8658-01ff93…</td><td>&quot;X-Art&quot;</td><td>null</td><td>&quot;019c5165-07c4-72ba-af12-73d5e8…</td><td>2025-09-12</td><td>&quot;morning-delight&quot;</td><td>&quot;Morning Delight&quot;</td><td>&quot;https://www.x-art.com/members/…</td><td>&quot;&quot;</td><td>2026-02-12 10:28:21.577122</td><td>2026-02-12 10:28:21.577169</td><td>&quot;[{&quot;file_type&quot;: &quot;video&quot;, &quot;conte…</td><td>&quot;{&quot;slug&quot;: &quot;morning-delight&quot;, &quot;t…</td><td>&quot;019c5166-1042-7740-87c1-1fa342…</td><td>2026-02-12 12:29:29.282546</td><td>&quot;video&quot;</td><td>&quot;scene&quot;</td><td>&quot;4k&quot;</td><td>&quot;{&quot;file_type&quot;: &quot;video&quot;, &quot;conten…</td><td>&quot;xart_sybil_sasha_alina_morning…</td><td>&quot;X-Art - 2025-09-12 - morning-d…</td><td>&quot;{&quot;$type&quot;: &quot;VideoFileMetadata&quot;,…</td><td>[]</td><td>[]</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;019c4c0b-ceda-7706-8658-01ff93…</td><td>&quot;X-Art&quot;</td><td>null</td><td>&quot;019c5166-24dc-752e-835a-7b3c68…</td><td>2025-09-20</td><td>&quot;the-studio-part-4&quot;</td><td>&quot;The Studio Part 4&quot;</td><td>&quot;https://www.x-art.com/members/…</td><td>&quot;What&#x27;s your greatest wish? May…</td><td>2026-02-12 10:29:34.559336</td><td>2026-02-12 10:29:34.559349</td><td>&quot;[{&quot;file_type&quot;: &quot;video&quot;, &quot;conte…</td><td>&quot;{&quot;slug&quot;: &quot;the-studio-part-4&quot;, …</td><td>&quot;019c5167-13a0-7594-a2ec-9a1028…</td><td>2026-02-12 12:30:35.680461</td><td>&quot;video&quot;</td><td>&quot;scene&quot;</td><td>&quot;4k&quot;</td><td>&quot;{&quot;file_type&quot;: &quot;video&quot;, &quot;conten…</td><td>&quot;xart_angelica_red_fox_the_stud…</td><td>&quot;X-Art - 2025-09-20 - the-studi…</td><td>&quot;{&quot;$type&quot;: &quot;VideoFileMetadata&quot;,…</td><td>[]</td><td>[]</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 27)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ ce_downlo ┆ ce_downlo ┆ ce_downlo ┆ ce_downlo ┆ … ┆ ce_downlo ┆ ce_downlo ┆ ce_downlo ┆ ce_downl │\n",
       "│ ads_site_ ┆ ads_site_ ┆ ads_sub_s ┆ ads_relea ┆   ┆ ads_tags  ┆ ads_hash_ ┆ ads_hash_ ┆ oads_has │\n",
       "│ uuid      ┆ name      ┆ ite_name  ┆ se_uuid   ┆   ┆ ---       ┆ oshash    ┆ phash     ┆ h_sha256 │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ list[stru ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ ct[4]]    ┆ str       ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 019c4c0b- ┆ X-Art     ┆ null      ┆ 019c5161- ┆ … ┆ []        ┆ null      ┆ null      ┆ null     │\n",
       "│ ceda-7706 ┆           ┆           ┆ 9d4e-7103 ┆   ┆           ┆           ┆           ┆          │\n",
       "│ -8658-01f ┆           ┆           ┆ -986b-50d ┆   ┆           ┆           ┆           ┆          │\n",
       "│ f93…      ┆           ┆           ┆ 794…      ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 019c4c0b- ┆ X-Art     ┆ null      ┆ 019c5163- ┆ … ┆ []        ┆ null      ┆ null      ┆ null     │\n",
       "│ ceda-7706 ┆           ┆           ┆ 1d12-77df ┆   ┆           ┆           ┆           ┆          │\n",
       "│ -8658-01f ┆           ┆           ┆ -b51b-7c4 ┆   ┆           ┆           ┆           ┆          │\n",
       "│ f93…      ┆           ┆           ┆ 50b…      ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 019c4c0b- ┆ X-Art     ┆ null      ┆ 019c5164- ┆ … ┆ []        ┆ null      ┆ null      ┆ null     │\n",
       "│ ceda-7706 ┆           ┆           ┆ 4069-730d ┆   ┆           ┆           ┆           ┆          │\n",
       "│ -8658-01f ┆           ┆           ┆ -96e6-a0e ┆   ┆           ┆           ┆           ┆          │\n",
       "│ f93…      ┆           ┆           ┆ 7c7…      ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 019c4c0b- ┆ X-Art     ┆ null      ┆ 019c5165- ┆ … ┆ []        ┆ null      ┆ null      ┆ null     │\n",
       "│ ceda-7706 ┆           ┆           ┆ 07c4-72ba ┆   ┆           ┆           ┆           ┆          │\n",
       "│ -8658-01f ┆           ┆           ┆ -af12-73d ┆   ┆           ┆           ┆           ┆          │\n",
       "│ f93…      ┆           ┆           ┆ 5e8…      ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 019c4c0b- ┆ X-Art     ┆ null      ┆ 019c5166- ┆ … ┆ []        ┆ null      ┆ null      ┆ null     │\n",
       "│ ceda-7706 ┆           ┆           ┆ 24dc-752e ┆   ┆           ┆           ┆           ┆          │\n",
       "│ -8658-01f ┆           ┆           ┆ -835a-7b3 ┆   ┆           ┆           ┆           ┆          │\n",
       "│ f93…      ┆           ┆           ┆ c68…      ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query all X-Art downloads\n",
    "print(\"Querying X-Art downloads from Culture Extractor...\")\n",
    "ce_downloads_df = ce_client.get_downloads(xart_site_uuid)\n",
    "print(f\"Found {len(ce_downloads_df)} total downloads\")\n",
    "\n",
    "# Filter for video files only\n",
    "ce_videos_df = ce_downloads_df.filter(pl.col(\"ce_downloads_file_type\") == \"video\")\n",
    "print(f\"Found {len(ce_videos_df)} video downloads\")\n",
    "\n",
    "ce_videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CE downloads to: /Users/thardas/Private/Code/Culture/analysis/notebooks/stashapp/Stashapp/output/ce_xart_downloads.json\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON for inspection\n",
    "output_file = OUTPUT_DIR / \"ce_xart_downloads.json\"\n",
    "ce_videos_df.write_json(output_file)\n",
    "print(f\"Saved CE downloads to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Multi-Stage Matching\n",
    "\n",
    "Priority order:\n",
    "1. **OSHash match** (confidence: 1.0) → RENAME_DELETE\n",
    "2. **Phash + duration match** (confidence: 0.95) → MOVE_NEXT_TO\n",
    "3. **Date + fuzzy title match** (confidence: 0.80) → MOVE_NEXT_TO\n",
    "4. **Date + partial title match** (confidence: 0.70) → FLAG_FOR_REVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to normalize titles for comparison\n",
    "def normalize_title(title: str) -> str:\n",
    "    \"\"\"Normalize title for comparison by lowercasing and removing special characters.\"\"\"\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", title.lower())\n",
    "\n",
    "# Helper function to check if one title contains the other\n",
    "def partial_title_match(title1: str, title2: str) -> bool:\n",
    "    \"\"\"Check if one title is a substring of the other.\"\"\"\n",
    "    if not title1 or not title2:\n",
    "        return False\n",
    "    norm1 = normalize_title(title1)\n",
    "    norm2 = normalize_title(title2)\n",
    "    return norm1 in norm2 or norm2 in norm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: OSHash Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 OSHash matches\n",
      "Created 0 OSHash match records\n"
     ]
    }
   ],
   "source": [
    "matches: list[FileMatch] = []\n",
    "\n",
    "# Filter out null oshashes\n",
    "stashapp_with_oshash = stashapp_df.filter(pl.col(\"oshash\").is_not_null())\n",
    "ce_with_oshash = ce_videos_df.filter(pl.col(\"ce_downloads_hash_oshash\").is_not_null())\n",
    "\n",
    "# Join on oshash\n",
    "oshash_matches = stashapp_with_oshash.join(\n",
    "    ce_with_oshash,\n",
    "    left_on=\"oshash\",\n",
    "    right_on=\"ce_downloads_hash_oshash\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(oshash_matches)} OSHash matches\")\n",
    "\n",
    "# Convert to FileMatch objects\n",
    "for row in oshash_matches.iter_rows(named=True):\n",
    "    matches.append(FileMatch(\n",
    "        stashapp_scene_id=row[\"scene_id\"],\n",
    "        stashapp_title=row[\"title\"],\n",
    "        stashapp_date=row[\"date\"],\n",
    "        stashapp_file_path=row[\"file_path\"],\n",
    "        stashapp_oshash=row[\"oshash\"],\n",
    "        stashapp_phash=row[\"phash\"],\n",
    "        stashapp_duration=row[\"duration\"],\n",
    "        ce_release_uuid=row[\"ce_downloads_release_uuid\"],\n",
    "        ce_release_name=row[\"ce_downloads_release_name\"],\n",
    "        ce_release_date=row[\"ce_downloads_release_date\"],\n",
    "        ce_saved_filename=row[\"ce_downloads_saved_filename\"],\n",
    "        ce_oshash=row[\"ce_downloads_hash_oshash\"],\n",
    "        ce_phash=row[\"ce_downloads_hash_phash\"],\n",
    "        match_type=\"oshash\",\n",
    "        confidence=1.0,\n",
    "        action=\"RENAME_DELETE\",\n",
    "        reason=\"Identical file (same OSHash) - rename Stashapp file to CE naming, delete CE duplicate\"\n",
    "    ))\n",
    "\n",
    "print(f\"Created {len(matches)} OSHash match records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Phash + Duration Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Stashapp scenes: 935\n",
      "Remaining CE downloads: 0\n",
      "Found 0 Phash matches\n",
      "Total matches so far: 0\n"
     ]
    }
   ],
   "source": [
    "# Get already matched scene IDs and CE release UUIDs\n",
    "matched_scene_ids = {m.stashapp_scene_id for m in matches}\n",
    "matched_ce_uuids = {m.ce_release_uuid for m in matches}\n",
    "\n",
    "# Filter out already matched records\n",
    "remaining_stashapp = stashapp_df.filter(\n",
    "    ~pl.col(\"scene_id\").is_in(matched_scene_ids) &\n",
    "    pl.col(\"phash\").is_not_null() &\n",
    "    pl.col(\"duration\").is_not_null()\n",
    ")\n",
    "\n",
    "remaining_ce = ce_videos_df.filter(\n",
    "    ~pl.col(\"ce_downloads_release_uuid\").is_in(matched_ce_uuids) &\n",
    "    pl.col(\"ce_downloads_hash_phash\").is_not_null()\n",
    ")\n",
    "\n",
    "print(f\"Remaining Stashapp scenes: {len(remaining_stashapp)}\")\n",
    "print(f\"Remaining CE downloads: {len(remaining_ce)}\")\n",
    "\n",
    "# Manual phash comparison (Polars doesn't have built-in hamming distance)\n",
    "phash_matches = []\n",
    "for stash_row in remaining_stashapp.iter_rows(named=True):\n",
    "    stash_phash = stash_row[\"phash\"]\n",
    "    stash_duration = stash_row[\"duration\"]\n",
    "\n",
    "    if not stash_phash or not stash_duration:\n",
    "        continue\n",
    "\n",
    "    for ce_row in remaining_ce.iter_rows(named=True):\n",
    "        ce_phash = ce_row[\"ce_downloads_hash_phash\"]\n",
    "\n",
    "        if not ce_phash:\n",
    "            continue\n",
    "\n",
    "        # Calculate hamming distance\n",
    "        try:\n",
    "            hamming_dist = hamming_distance(stash_phash, ce_phash)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if hamming_dist <= PHASH_HAMMING_THRESHOLD:\n",
    "            # Check duration difference (CE doesn't store duration, so skip this check)\n",
    "            # Note: CE database doesn't have duration in downloads table\n",
    "            phash_matches.append((stash_row, ce_row, hamming_dist))\n",
    "            break  # Match found, move to next Stashapp scene\n",
    "\n",
    "print(f\"Found {len(phash_matches)} Phash matches\")\n",
    "\n",
    "# Convert to FileMatch objects\n",
    "for stash_row, ce_row, hamming_dist in phash_matches:\n",
    "    matches.append(FileMatch(\n",
    "        stashapp_scene_id=stash_row[\"scene_id\"],\n",
    "        stashapp_title=stash_row[\"title\"],\n",
    "        stashapp_date=stash_row[\"date\"],\n",
    "        stashapp_file_path=stash_row[\"file_path\"],\n",
    "        stashapp_oshash=stash_row[\"oshash\"],\n",
    "        stashapp_phash=stash_row[\"phash\"],\n",
    "        stashapp_duration=stash_row[\"duration\"],\n",
    "        ce_release_uuid=ce_row[\"ce_downloads_release_uuid\"],\n",
    "        ce_release_name=ce_row[\"ce_downloads_release_name\"],\n",
    "        ce_release_date=ce_row[\"ce_downloads_release_date\"],\n",
    "        ce_saved_filename=ce_row[\"ce_downloads_saved_filename\"],\n",
    "        ce_oshash=ce_row[\"ce_downloads_hash_oshash\"],\n",
    "        ce_phash=ce_row[\"ce_downloads_hash_phash\"],\n",
    "        match_type=\"phash_duration\",\n",
    "        confidence=0.95,\n",
    "        action=\"MOVE_NEXT_TO\",\n",
    "        reason=f\"Similar content (Phash hamming distance: {hamming_dist}) - likely different quality/encoding\"\n",
    "    ))\n",
    "\n",
    "print(f\"Total matches so far: {len(matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Date + Fuzzy Title Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update matched IDs\n",
    "matched_scene_ids = {m.stashapp_scene_id for m in matches}\n",
    "matched_ce_uuids = {m.ce_release_uuid for m in matches}\n",
    "\n",
    "# Filter out already matched records\n",
    "remaining_stashapp = stashapp_df.filter(\n",
    "    ~pl.col(\"scene_id\").is_in(matched_scene_ids) &\n",
    "    pl.col(\"date\").is_not_null() &\n",
    "    pl.col(\"title\").is_not_null()\n",
    ")\n",
    "\n",
    "remaining_ce = ce_videos_df.filter(\n",
    "    ~pl.col(\"ce_downloads_release_uuid\").is_in(matched_ce_uuids) &\n",
    "    pl.col(\"ce_downloads_release_date\").is_not_null() &\n",
    "    pl.col(\"ce_downloads_release_name\").is_not_null()\n",
    ")\n",
    "\n",
    "print(f\"Remaining Stashapp scenes: {len(remaining_stashapp)}\")\n",
    "print(f\"Remaining CE downloads: {len(remaining_ce)}\")\n",
    "\n",
    "# Manual fuzzy title matching\n",
    "fuzzy_matches = []\n",
    "for stash_row in remaining_stashapp.iter_rows(named=True):\n",
    "    stash_date = stash_row[\"date\"]\n",
    "    stash_title = stash_row[\"title\"]\n",
    "\n",
    "    if not stash_date or not stash_title:\n",
    "        continue\n",
    "\n",
    "    for ce_row in remaining_ce.iter_rows(named=True):\n",
    "        ce_date = ce_row[\"ce_downloads_release_date\"]\n",
    "        ce_title = ce_row[\"ce_downloads_release_name\"]\n",
    "\n",
    "        if not ce_date or not ce_title:\n",
    "            continue\n",
    "\n",
    "        # Check exact date match\n",
    "        if stash_date != ce_date:\n",
    "            continue\n",
    "\n",
    "        # Calculate title similarity\n",
    "        similarity = calculate_similarity(stash_title, ce_title)\n",
    "\n",
    "        if similarity >= TITLE_SIMILARITY_THRESHOLD:\n",
    "            fuzzy_matches.append((stash_row, ce_row, similarity))\n",
    "            break  # Match found, move to next Stashapp scene\n",
    "\n",
    "print(f\"Found {len(fuzzy_matches)} fuzzy title matches\")\n",
    "\n",
    "# Convert to FileMatch objects\n",
    "for stash_row, ce_row, similarity in fuzzy_matches:\n",
    "    matches.append(FileMatch(\n",
    "        stashapp_scene_id=stash_row[\"scene_id\"],\n",
    "        stashapp_title=stash_row[\"title\"],\n",
    "        stashapp_date=stash_row[\"date\"],\n",
    "        stashapp_file_path=stash_row[\"file_path\"],\n",
    "        stashapp_oshash=stash_row[\"oshash\"],\n",
    "        stashapp_phash=stash_row[\"phash\"],\n",
    "        stashapp_duration=stash_row[\"duration\"],\n",
    "        ce_release_uuid=ce_row[\"ce_downloads_release_uuid\"],\n",
    "        ce_release_name=ce_row[\"ce_downloads_release_name\"],\n",
    "        ce_release_date=ce_row[\"ce_downloads_release_date\"],\n",
    "        ce_saved_filename=ce_row[\"ce_downloads_saved_filename\"],\n",
    "        ce_oshash=ce_row[\"ce_downloads_hash_oshash\"],\n",
    "        ce_phash=ce_row[\"ce_downloads_hash_phash\"],\n",
    "        match_type=\"date_title_fuzzy\",\n",
    "        confidence=0.80,\n",
    "        action=\"MOVE_NEXT_TO\",\n",
    "        reason=f\"Same date and similar title (similarity: {similarity:.2f})\"\n",
    "    ))\n",
    "\n",
    "print(f\"Total matches so far: {len(matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Date + Partial Title Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update matched IDs\n",
    "matched_scene_ids = {m.stashapp_scene_id for m in matches}\n",
    "matched_ce_uuids = {m.ce_release_uuid for m in matches}\n",
    "\n",
    "# Filter out already matched records\n",
    "remaining_stashapp = stashapp_df.filter(\n",
    "    ~pl.col(\"scene_id\").is_in(matched_scene_ids) &\n",
    "    pl.col(\"date\").is_not_null() &\n",
    "    pl.col(\"title\").is_not_null()\n",
    ")\n",
    "\n",
    "remaining_ce = ce_videos_df.filter(\n",
    "    ~pl.col(\"ce_downloads_release_uuid\").is_in(matched_ce_uuids) &\n",
    "    pl.col(\"ce_downloads_release_date\").is_not_null() &\n",
    "    pl.col(\"ce_downloads_release_name\").is_not_null()\n",
    ")\n",
    "\n",
    "print(f\"Remaining Stashapp scenes: {len(remaining_stashapp)}\")\n",
    "print(f\"Remaining CE downloads: {len(remaining_ce)}\")\n",
    "\n",
    "# Manual partial title matching\n",
    "partial_matches = []\n",
    "for stash_row in remaining_stashapp.iter_rows(named=True):\n",
    "    stash_date = stash_row[\"date\"]\n",
    "    stash_title = stash_row[\"title\"]\n",
    "\n",
    "    if not stash_date or not stash_title:\n",
    "        continue\n",
    "\n",
    "    for ce_row in remaining_ce.iter_rows(named=True):\n",
    "        ce_date = ce_row[\"ce_downloads_release_date\"]\n",
    "        ce_title = ce_row[\"ce_downloads_release_name\"]\n",
    "\n",
    "        if not ce_date or not ce_title:\n",
    "            continue\n",
    "\n",
    "        # Check exact date match\n",
    "        if stash_date != ce_date:\n",
    "            continue\n",
    "\n",
    "        # Check partial title match\n",
    "        if partial_title_match(stash_title, ce_title):\n",
    "            partial_matches.append((stash_row, ce_row))\n",
    "            break  # Match found, move to next Stashapp scene\n",
    "\n",
    "print(f\"Found {len(partial_matches)} partial title matches\")\n",
    "\n",
    "# Convert to FileMatch objects\n",
    "for stash_row, ce_row in partial_matches:\n",
    "    matches.append(FileMatch(\n",
    "        stashapp_scene_id=stash_row[\"scene_id\"],\n",
    "        stashapp_title=stash_row[\"title\"],\n",
    "        stashapp_date=stash_row[\"date\"],\n",
    "        stashapp_file_path=stash_row[\"file_path\"],\n",
    "        stashapp_oshash=stash_row[\"oshash\"],\n",
    "        stashapp_phash=stash_row[\"phash\"],\n",
    "        stashapp_duration=stash_row[\"duration\"],\n",
    "        ce_release_uuid=ce_row[\"ce_downloads_release_uuid\"],\n",
    "        ce_release_name=ce_row[\"ce_downloads_release_name\"],\n",
    "        ce_release_date=ce_row[\"ce_downloads_release_date\"],\n",
    "        ce_saved_filename=ce_row[\"ce_downloads_saved_filename\"],\n",
    "        ce_oshash=ce_row[\"ce_downloads_hash_oshash\"],\n",
    "        ce_phash=ce_row[\"ce_downloads_hash_phash\"],\n",
    "        match_type=\"date_title_partial\",\n",
    "        confidence=0.70,\n",
    "        action=\"FLAG_FOR_REVIEW\",\n",
    "        reason=\"Same date and partial title match - requires manual verification\"\n",
    "    ))\n",
    "\n",
    "print(f\"Total matches: {len(matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "matches_by_type = {}\n",
    "matches_by_action = {}\n",
    "matches_by_confidence = {\"high\": 0, \"medium\": 0, \"low\": 0}\n",
    "\n",
    "for match in matches:\n",
    "    matches_by_type[match.match_type] = matches_by_type.get(match.match_type, 0) + 1\n",
    "    matches_by_action[match.action] = matches_by_action.get(match.action, 0) + 1\n",
    "\n",
    "    if match.confidence >= 0.95:\n",
    "        matches_by_confidence[\"high\"] += 1\n",
    "    elif match.confidence >= 0.75:\n",
    "        matches_by_confidence[\"medium\"] += 1\n",
    "    else:\n",
    "        matches_by_confidence[\"low\"] += 1\n",
    "\n",
    "print(\"\\n=== MATCHING SUMMARY ===\")\n",
    "print(f\"\\nTotal matches: {len(matches)}\")\n",
    "print(\"\\nBy match type:\")\n",
    "for match_type, count in matches_by_type.items():\n",
    "    print(f\"  {match_type}: {count}\")\n",
    "print(\"\\nBy action:\")\n",
    "for action, count in matches_by_action.items():\n",
    "    print(f\"  {action}: {count}\")\n",
    "print(\"\\nBy confidence:\")\n",
    "for level, count in matches_by_confidence.items():\n",
    "    print(f\"  {level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save matches to JSON\n",
    "output_file = OUTPUT_DIR / \"xart_matches.json\"\n",
    "with output_file.open(\"w\") as f:\n",
    "    json.dump([m.to_dict() for m in matches], f, indent=2)\n",
    "print(f\"\\nSaved matches to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Review and Approve Matches\n",
    "\n",
    "Display matches grouped by confidence for user review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_match(match: FileMatch, index: int):\n",
    "    \"\"\"Display a single match for user review.\"\"\"\n",
    "    print(f\"\\n--- Match #{index + 1} ---\")\n",
    "    print(f\"Match Type: {match.match_type}\")\n",
    "    print(f\"Confidence: {match.confidence:.2f}\")\n",
    "    print(f\"Action: {match.action}\")\n",
    "    print(f\"Reason: {match.reason}\")\n",
    "    print(\"\\nStashapp Scene:\")\n",
    "    print(f\"  ID: {match.stashapp_scene_id}\")\n",
    "    print(f\"  Title: {match.stashapp_title}\")\n",
    "    print(f\"  Date: {match.stashapp_date}\")\n",
    "    print(f\"  File: {match.stashapp_file_path}\")\n",
    "    print(f\"  OSHash: {match.stashapp_oshash}\")\n",
    "    print(\"\\nCE Download:\")\n",
    "    print(f\"  UUID: {match.ce_release_uuid}\")\n",
    "    print(f\"  Title: {match.ce_release_name}\")\n",
    "    print(f\"  Date: {match.ce_release_date}\")\n",
    "    print(f\"  File: {match.ce_saved_filename}\")\n",
    "    print(f\"  OSHash: {match.ce_oshash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display high-confidence matches (>= 0.95)\n",
    "high_confidence = [m for m in matches if m.confidence >= 0.95]\n",
    "print(f\"\\n=== HIGH CONFIDENCE MATCHES ({len(high_confidence)}) ===\")\n",
    "print(\"These matches can typically be approved in bulk.\\n\")\n",
    "\n",
    "for i, match in enumerate(high_confidence[:5]):  # Show first 5\n",
    "    display_match(match, i)\n",
    "\n",
    "if len(high_confidence) > 5:\n",
    "    print(f\"\\n... and {len(high_confidence) - 5} more high-confidence matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display medium-confidence matches (0.75 - 0.94)\n",
    "medium_confidence = [m for m in matches if 0.75 <= m.confidence < 0.95]\n",
    "print(f\"\\n=== MEDIUM CONFIDENCE MATCHES ({len(medium_confidence)}) ===\")\n",
    "print(\"These matches should be reviewed individually.\\n\")\n",
    "\n",
    "for i, match in enumerate(medium_confidence[:5]):  # Show first 5\n",
    "    display_match(match, i)\n",
    "\n",
    "if len(medium_confidence) > 5:\n",
    "    print(f\"\\n... and {len(medium_confidence) - 5} more medium-confidence matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display low-confidence matches (< 0.75)\n",
    "low_confidence = [m for m in matches if m.confidence < 0.75]\n",
    "print(f\"\\n=== LOW CONFIDENCE MATCHES ({len(low_confidence)}) ===\")\n",
    "print(\"These matches require careful manual verification.\\n\")\n",
    "\n",
    "for i, match in enumerate(low_confidence[:5]):  # Show first 5\n",
    "    display_match(match, i)\n",
    "\n",
    "if len(low_confidence) > 5:\n",
    "    print(f\"\\n... and {len(low_confidence) - 5} more low-confidence matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approval Interface\n",
    "\n",
    "Manually set `approved=True` for matches you want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Approve all high-confidence matches\n",
    "# Uncomment to approve:\n",
    "# for match in high_confidence:\n",
    "#     match.approved = True\n",
    "\n",
    "# Example: Approve specific matches by index\n",
    "# matches[0].approved = True\n",
    "# matches[5].approved = True\n",
    "\n",
    "approved_matches = [m for m in matches if m.approved]\n",
    "print(f\"\\nApproved matches: {len(approved_matches)} / {len(matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Execute Approved File Operations\n",
    "\n",
    "**WARNING**: This will modify files on disk. Set `DRY_RUN = False` to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def execute_rename_delete(match: FileMatch, dry_run: bool = True) -> dict:\n",
    "    \"\"\"Execute RENAME_DELETE action for exact duplicates.\"\"\"\n",
    "    result = {\n",
    "        \"success\": False,\n",
    "        \"action\": \"RENAME_DELETE\",\n",
    "        \"match\": match.to_dict(),\n",
    "        \"operations\": [],\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get CE basename for new Stashapp filename\n",
    "        ce_basename = Path(match.ce_saved_filename).name\n",
    "        stash_dir = str(Path(match.stashapp_file_path).parent)\n",
    "        new_stash_path = str(Path(stash_dir) / ce_basename)\n",
    "\n",
    "        # Operation 1: Rename Stashapp file\n",
    "        if dry_run:\n",
    "            result[\"operations\"].append(f\"[DRY RUN] Rename: {match.stashapp_file_path} -> {new_stash_path}\")\n",
    "        else:\n",
    "            Path(match.stashapp_file_path).rename(new_stash_path)\n",
    "            result[\"operations\"].append(f\"Renamed: {match.stashapp_file_path} -> {new_stash_path}\")\n",
    "\n",
    "        # Operation 2: Trigger Stashapp scan\n",
    "        if dry_run:\n",
    "            result[\"operations\"].append(f\"[DRY RUN] Scan directory: {stash_dir}\")\n",
    "        else:\n",
    "            stash.metadata_scan(paths=[stash_dir])\n",
    "            result[\"operations\"].append(f\"Scanned directory: {stash_dir}\")\n",
    "\n",
    "        # Operation 3: Delete CE video file (keep images)\n",
    "        if dry_run:\n",
    "            result[\"operations\"].append(f\"[DRY RUN] Delete CE video: {match.ce_saved_filename}\")\n",
    "        elif Path(match.ce_saved_filename).exists():\n",
    "            Path(match.ce_saved_filename).unlink()\n",
    "            result[\"operations\"].append(f\"Deleted CE video: {match.ce_saved_filename}\")\n",
    "\n",
    "        result[\"success\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"errors\"].append(str(e))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def execute_move_next_to(match: FileMatch, dry_run: bool = True) -> dict:\n",
    "    \"\"\"Execute MOVE_NEXT_TO action for similar files.\"\"\"\n",
    "    result = {\n",
    "        \"success\": False,\n",
    "        \"action\": \"MOVE_NEXT_TO\",\n",
    "        \"match\": match.to_dict(),\n",
    "        \"operations\": [],\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get target directory from Stashapp file\n",
    "        target_dir = str(Path(match.stashapp_file_path).parent)\n",
    "        ce_basename = Path(match.ce_saved_filename).name\n",
    "        new_ce_path = str(Path(target_dir) / ce_basename)\n",
    "\n",
    "        # Operation 1: Move CE video file\n",
    "        if dry_run:\n",
    "            result[\"operations\"].append(f\"[DRY RUN] Move: {match.ce_saved_filename} -> {new_ce_path}\")\n",
    "        else:\n",
    "            shutil.move(match.ce_saved_filename, new_ce_path)\n",
    "            result[\"operations\"].append(f\"Moved: {match.ce_saved_filename} -> {new_ce_path}\")\n",
    "\n",
    "        # Operation 2: Move preview images from same release directory\n",
    "        ce_release_dir = str(Path(match.ce_saved_filename).parent)\n",
    "        for img_file in Path(ce_release_dir).glob(\"*.jpg\"):\n",
    "            img_target = str(Path(target_dir) / img_file.name)\n",
    "            if dry_run:\n",
    "                result[\"operations\"].append(f\"[DRY RUN] Move image: {img_file} -> {img_target}\")\n",
    "            else:\n",
    "                shutil.move(str(img_file), img_target)\n",
    "                result[\"operations\"].append(f\"Moved image: {img_file} -> {img_target}\")\n",
    "\n",
    "        # Operation 3: Trigger Stashapp scan\n",
    "        if dry_run:\n",
    "            result[\"operations\"].append(f\"[DRY RUN] Scan directory: {target_dir}\")\n",
    "        else:\n",
    "            stash.metadata_scan(paths=[target_dir])\n",
    "            result[\"operations\"].append(f\"Scanned directory: {target_dir}\")\n",
    "\n",
    "        result[\"success\"] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"errors\"].append(str(e))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute approved operations\n",
    "execution_results = []\n",
    "\n",
    "print(f\"\\n=== EXECUTING APPROVED OPERATIONS (DRY_RUN={DRY_RUN}) ===\")\n",
    "print(f\"Processing {len(approved_matches)} approved matches...\\n\")\n",
    "\n",
    "for i, match in enumerate(approved_matches):\n",
    "    print(f\"\\nProcessing match #{i + 1}/{len(approved_matches)}...\")\n",
    "    print(f\"  Stashapp: {match.stashapp_title}\")\n",
    "    print(f\"  CE: {match.ce_release_name}\")\n",
    "    print(f\"  Action: {match.action}\")\n",
    "\n",
    "    if match.action == \"RENAME_DELETE\":\n",
    "        result = execute_rename_delete(match, dry_run=DRY_RUN)\n",
    "    elif match.action == \"MOVE_NEXT_TO\":\n",
    "        result = execute_move_next_to(match, dry_run=DRY_RUN)\n",
    "    else:\n",
    "        result = {\n",
    "            \"success\": False,\n",
    "            \"action\": match.action,\n",
    "            \"match\": match.to_dict(),\n",
    "            \"operations\": [],\n",
    "            \"errors\": [f\"Unknown action: {match.action}\"]\n",
    "        }\n",
    "\n",
    "    execution_results.append(result)\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        print(\"  ✓ Success\")\n",
    "        for op in result[\"operations\"]:\n",
    "            print(f\"    - {op}\")\n",
    "    else:\n",
    "        print(\"  ✗ Failed\")\n",
    "        for error in result[\"errors\"]:\n",
    "            print(f\"    - ERROR: {error}\")\n",
    "\n",
    "print(\"\\n=== EXECUTION COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Verify and Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_operations = len(execution_results)\n",
    "successful_operations = sum(1 for r in execution_results if r[\"success\"])\n",
    "failed_operations = total_operations - successful_operations\n",
    "\n",
    "rename_delete_count = sum(1 for r in execution_results if r[\"action\"] == \"RENAME_DELETE\")\n",
    "move_next_to_count = sum(1 for r in execution_results if r[\"action\"] == \"MOVE_NEXT_TO\")\n",
    "\n",
    "print(\"\\n=== EXECUTION SUMMARY ===\")\n",
    "print(f\"\\nTotal operations: {total_operations}\")\n",
    "print(f\"Successful: {successful_operations}\")\n",
    "print(f\"Failed: {failed_operations}\")\n",
    "print(\"\\nBy action type:\")\n",
    "print(f\"  RENAME_DELETE: {rename_delete_count}\")\n",
    "print(f\"  MOVE_NEXT_TO: {move_next_to_count}\")\n",
    "\n",
    "if DRY_RUN:\n",
    "    print(\"\\n⚠️  DRY RUN MODE - No files were actually modified\")\n",
    "    print(\"Set DRY_RUN = False to execute actual file operations\")\n",
    "else:\n",
    "    print(\"\\n✓ File operations executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save execution log\n",
    "log_file = OUTPUT_DIR / \"execution_log.json\"\n",
    "with log_file.open(\"w\") as f:\n",
    "    json.dump({\n",
    "        \"dry_run\": DRY_RUN,\n",
    "        \"total_operations\": total_operations,\n",
    "        \"successful\": successful_operations,\n",
    "        \"failed\": failed_operations,\n",
    "        \"results\": execution_results\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nExecution log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display failed operations\n",
    "failed = [r for r in execution_results if not r[\"success\"]]\n",
    "if failed:\n",
    "    print(\"\\n=== FAILED OPERATIONS ===\")\n",
    "    for i, result in enumerate(failed):\n",
    "        print(f\"\\n#{i + 1}:\")\n",
    "        print(f\"  Stashapp: {result['match']['stashapp_title']}\")\n",
    "        print(f\"  CE: {result['match']['ce_release_name']}\")\n",
    "        print(\"  Errors:\")\n",
    "        for error in result[\"errors\"]:\n",
    "            print(f\"    - {error}\")\n",
    "else:\n",
    "    print(\"\\n✓ No failed operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}