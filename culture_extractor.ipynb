{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "user = os.environ.get(\"CE_DB_USERNAME\")\n",
    "pw = os.environ.get(\"CE_DB_PASSWORD\")\n",
    "host = os.environ.get(\"CE_DB_HOST\")\n",
    "port = os.environ.get(\"CE_DB_PORT\")\n",
    "db = os.environ.get(\"CE_DB_NAME\")\n",
    "\n",
    "connection_string = f\"dbname={db} user={user} password={pw} host={host} port={port}\"\n",
    "\n",
    "def hex_to_binary(hex_string):\n",
    "    return bin(int(hex_string, 16))[2:].zfill(64)\n",
    "\n",
    "def calculate_hamming_distance(phash1, phash2):\n",
    "    # Convert hexadecimal phashes to binary\n",
    "    binary1 = hex_to_binary(phash1)\n",
    "    binary2 = hex_to_binary(phash2)\n",
    "    \n",
    "    # Ensure both binary strings are of equal length\n",
    "    if len(binary1) != len(binary2):\n",
    "        raise ValueError(\"Binary strings must be of equal length\")\n",
    "    \n",
    "    # Calculate Hamming distance\n",
    "    return sum(c1 != c2 for c1, c2 in zip(binary1, binary2))\n",
    "\n",
    "# Example usage:\n",
    "# phash1 = \"951428607cf7cb8f\"\n",
    "# phash2 = \"951428607cf7cb8e\"\n",
    "# distance = calculate_hamming_distance(phash1, phash2)\n",
    "# print(f\"Hamming distance between {phash1} and {phash2}: {distance}\")\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    from Levenshtein import distance\n",
    "    return distance(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query\n",
    "    query = \"SELECT * FROM sites ORDER BY name\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    matched_performers = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_sites = pd.DataFrame(matched_performers, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scenes to Stash\n",
    "# import base64\n",
    "# \n",
    "# for _, release in df_tickling_submissions_joined.iterrows():\n",
    "#     scene_data = {\n",
    "#         \"id\": release[\"stash_scene\"][\"id\"],\n",
    "#         \"code\": release[\"culture_extractor_release_short_name\"],\n",
    "#         \"title\": release[\"culture_extractor_release_name\"],\n",
    "#         \"date\": release[\"culture_extractor_release_date\"].strftime(\"%Y-%m-%d\"),\n",
    "#         \"details\": release[\"description\"],\n",
    "#         \"performer_ids\": release[\"stash_performer_ids\"],\n",
    "#         \"urls\": [release[\"culture_extractor_release_url\"]],\n",
    "#         \"stash_ids\": [{\"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": release[\"culture_extractor_release_uuid\"]}]\n",
    "#     }\n",
    "#     \n",
    "#     # Find jpg file in a directory like F:\\Ripping\\Tickling Submission\\Metadata\\0192221b-5f5b-75ac-a715-e8292b4262e7\\\n",
    "#     metadata_path = f\"F:\\\\Ripping\\\\Tickling Submission\\\\Metadata\\\\{release['culture_extractor_release_uuid']}\\\\\"\n",
    "#     jpg_file = next((f for f in os.listdir(metadata_path) if f.endswith('.jpg')), None)\n",
    "#     if jpg_file:\n",
    "#         with open(f\"F:\\\\Ripping\\\\Tickling Submission\\\\Metadata\\\\{release['culture_extractor_release_uuid']}\\\\\" + jpg_file, \"rb\") as image_file:\n",
    "#             encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "#             scene_data[\"cover_image\"] = f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "#     \n",
    "#     stash.update_scene(scene_data)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query with COALESCE to handle -infinity dates\n",
    "    query = \"\"\"\n",
    "   SELECT \n",
    "       sites.name AS site_name,\n",
    "       sub_sites.name AS sub_site_name,\n",
    "       releases.uuid AS release_uuid,\n",
    "       COALESCE(NULLIF(releases.release_date, '-infinity'), '1970-01-01'::date) as release_date,\n",
    "       releases.short_name AS release_short_name, \n",
    "       releases.name AS release_name, \n",
    "       releases.url AS release_url,\n",
    "       releases.description AS release_description,\n",
    "       releases.json_document::text AS release_json_document,\n",
    "       downloads.uuid AS downloads_uuid,\n",
    "       downloads.file_type,\n",
    "       downloads.content_type,\n",
    "       downloads.saved_filename,\n",
    "       downloads.file_metadata::text AS file_metadata,\n",
    "       -- Aggregate performers into a JSON array of objects\n",
    "       json_agg(\n",
    "           DISTINCT jsonb_build_object(\n",
    "               'uuid', performers.uuid,\n",
    "               'name', performers.name,\n",
    "               'short_name', performers.short_name,\n",
    "               'url', performers.url\n",
    "           ) \n",
    "       ) FILTER (WHERE performers.uuid IS NOT NULL) as performers,\n",
    "       -- Aggregate tags into an array  \n",
    "       array_agg(DISTINCT tags.name) FILTER (WHERE tags.name IS NOT NULL) as tag_names\n",
    "   FROM releases\n",
    "   JOIN sites ON releases.site_uuid = sites.uuid\n",
    "   JOIN downloads ON releases.uuid = downloads.release_uuid\n",
    "   LEFT JOIN sub_sites ON releases.sub_site_uuid = sub_sites.uuid\n",
    "   -- Left join performers through junction table\n",
    "   LEFT JOIN release_entity_site_performer_entity rep ON releases.uuid = rep.releases_uuid\n",
    "   LEFT JOIN performers ON rep.performers_uuid = performers.uuid\n",
    "   -- Left join tags through junction table\n",
    "   LEFT JOIN release_entity_site_tag_entity ret ON releases.uuid = ret.releases_uuid\n",
    "   LEFT JOIN tags ON ret.tags_uuid = tags.uuid\n",
    "   WHERE sites.name = 'Nubile Films'\n",
    "   GROUP BY\n",
    "       sites.name,\n",
    "       sub_sites.name,\n",
    "       releases.uuid,\n",
    "       releases.release_date,\n",
    "       releases.short_name,\n",
    "       releases.name,\n",
    "       releases.url,\n",
    "       releases.description,\n",
    "       releases.json_document::text,\n",
    "       downloads.uuid,\n",
    "       downloads.file_type,\n",
    "       downloads.content_type,\n",
    "       downloads.saved_filename,\n",
    "       downloads.file_metadata::text\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    matched_performers = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_releases = pd.DataFrame(matched_performers, columns=column_names)\n",
    "    # Convert string representation of JSON to dict\n",
    "    df_releases['file_metadata'] = df_releases['file_metadata'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    df_releases['sha256'] = df_releases['file_metadata'].apply(lambda x: x['sha256Sum'] if isinstance(x, dict) and 'sha256Sum' in x else None)\n",
    "    df_releases['phash'] = df_releases['file_metadata'].apply(lambda x: x['phash'] if isinstance(x, dict) and 'phash' in x else None)\n",
    "    df_releases['oshash'] = df_releases['file_metadata'].apply(lambda x: x['oshash'] if isinstance(x, dict) and 'oshash' in x else None)\n",
    "    \n",
    "    df_releases = df_releases.copy().add_prefix('culture_extractor_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stash app client\n",
    "\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import os\n",
    "from libraries.client_stashapp import get_stashapp_client\n",
    "from libraries.StashDbClient import StashDbClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "STASHAPP_URL = os.getenv(\"STASHAPP_URL\")\n",
    "\n",
    "stashbox_client = StashDbClient(\n",
    "    os.getenv(\"STASHDB_ENDPOINT\"),\n",
    "    os.getenv(\"STASHDB_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_with_stash_scenes = []\n",
    "\n",
    "for _, release in df_releases.iterrows():\n",
    "    oshash = release[\"culture_extractor_oshash\"]\n",
    "    if oshash is None:\n",
    "        continue\n",
    "\n",
    "    stash_scene = stash.find_scene_by_hash({ \"oshash\": oshash })\n",
    "    if stash_scene:\n",
    "        releases_with_stash_scenes.append({\n",
    "            **release.to_dict(),\n",
    "            \"stash_scene\": stash_scene\n",
    "        })\n",
    "\n",
    "df_releases_with_stash_scenes = pd.DataFrame(releases_with_stash_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store scene data\n",
    "scene_data = []\n",
    "\n",
    "# Create list of scene objects with filename, phash and duration\n",
    "scene_objects = df_releases_with_stash_scenes[['stash_scene', 'culture_extractor_phash', 'culture_extractor_file_metadata']].dropna().apply(\n",
    "    lambda x: {\n",
    "        'filename': x['stash_scene']['files'][0]['basename'],\n",
    "        'phash': x['culture_extractor_phash'], \n",
    "        'duration': x['culture_extractor_file_metadata'].get('duration')\n",
    "    },\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0, len(scene_objects), batch_size):\n",
    "    batch = scene_objects[i:i+batch_size]\n",
    "    stashdb_scenes = stashbox_client.query_scenes_by_phash(batch)\n",
    "    \n",
    "    for scene_obj in batch:\n",
    "        phash = scene_obj['phash']\n",
    "        if phash in stashdb_scenes:\n",
    "            release_row = df_releases_with_stash_scenes[df_releases_with_stash_scenes['culture_extractor_phash'] == phash].iloc[0]\n",
    "            \n",
    "            stashdb_row_data = {\n",
    "                **release_row.to_dict(),\n",
    "                'stashdb_scene': stashdb_scenes[phash]\n",
    "            }\n",
    "            scene_data.append(stashdb_row_data)\n",
    "\n",
    "df_scenes = pd.DataFrame(scene_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create verification columns\n",
    "def extract_duration(file_metadata):\n",
    "   if isinstance(file_metadata, dict) and 'duration' in file_metadata:\n",
    "       return file_metadata['duration']\n",
    "   return None\n",
    "\n",
    "def calculate_duration_difference(row):\n",
    "   ce_duration = extract_duration(row['culture_extractor_file_metadata'])\n",
    "   stashdb_duration = row.get('stashdb_scene', {}).get('duration')\n",
    "   \n",
    "   if ce_duration and stashdb_duration:\n",
    "       diff = abs(ce_duration - stashdb_duration)\n",
    "       # Return difference as percentage of the longer duration\n",
    "       max_duration = max(ce_duration, stashdb_duration)\n",
    "       return (diff / max_duration) * 100\n",
    "   return None\n",
    "\n",
    "def calculate_title_similarity(row):\n",
    "   ce_title = row.get('culture_extractor_release_name')\n",
    "   stashdb_title = row.get('stashdb_scene', {}).get('title')\n",
    "   \n",
    "   if ce_title and stashdb_title:\n",
    "       return levenshtein(ce_title, stashdb_title)\n",
    "   return None\n",
    "\n",
    "def get_date_difference_days(row):\n",
    "   ce_date = row.get('culture_extractor_release_date')\n",
    "   stashdb_date = row.get('stashdb_scene', {}).get('date')\n",
    "   \n",
    "   if ce_date and stashdb_date:\n",
    "       try:\n",
    "           ce_date = pd.to_datetime(ce_date)\n",
    "           stashdb_date = pd.to_datetime(stashdb_date)\n",
    "           return abs((ce_date - stashdb_date).days)\n",
    "       except:\n",
    "           return None\n",
    "   return None\n",
    "\n",
    "# Create verification dataframe\n",
    "df_verification = df_scenes[df_scenes['stashdb_scene'].notna()].copy()\n",
    "\n",
    "# Add verification columns at the beginning\n",
    "verification_columns = {\n",
    "   'duration_diff_pct': df_verification.apply(calculate_duration_difference, axis=1),\n",
    "   'title_levenshtein': df_verification.apply(calculate_title_similarity, axis=1),\n",
    "   'date_diff_days': df_verification.apply(get_date_difference_days, axis=1),\n",
    "   'ce_duration': df_verification['culture_extractor_file_metadata'].apply(extract_duration),\n",
    "   'stashdb_duration': df_verification['stashdb_scene'].apply(lambda x: x.get('duration') if x else None),\n",
    "   'ce_title': df_verification['culture_extractor_release_name'],\n",
    "   'stashdb_title': df_verification['stashdb_scene'].apply(lambda x: x.get('title') if x else None),\n",
    "   'ce_date': df_verification['culture_extractor_release_date'],\n",
    "   'stashdb_date': df_verification['stashdb_scene'].apply(lambda x: x.get('date') if x else None)\n",
    "}\n",
    "\n",
    "# Create new dataframe with verification columns first\n",
    "df_verification = pd.concat([\n",
    "   pd.DataFrame(verification_columns),\n",
    "   df_verification.drop(columns=verification_columns.keys(), errors='ignore')\n",
    "], axis=1)\n",
    "\n",
    "# Add warning flags\n",
    "df_verification['duration_warning'] = df_verification['duration_diff_pct'] > 5  # Warning if >5% difference\n",
    "df_verification['title_warning'] = df_verification['title_levenshtein'] > 5    # Warning if Levenshtein distance >5\n",
    "df_verification['date_warning'] = df_verification['date_diff_days'] > 7        # Warning if >7 days difference\n",
    "\n",
    "# Sort by warnings to show potential mismatches first\n",
    "df_verification = df_verification.sort_values(\n",
    "   by=['duration_warning', 'title_warning', 'date_warning', 'duration_diff_pct'],\n",
    "   ascending=[False, False, False, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_verification.iloc[1]['culture_extractor_performers']\n",
    "import json\n",
    "with open('tests/data/culture_extractor_stashdb_performers.sample02.json', 'w') as f:\n",
    "    json.dump({ \n",
    "        'culture_extractor_performers': df_verification.iloc[1]['culture_extractor_performers'],\n",
    "        'stashdb_performers': df_verification.iloc[1]['stashdb_scene']['performers']\n",
    "    }, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.performer_matcher import PerformerMatcher\n",
    "\n",
    "df_sample_scene = df_verification.iloc[1]\n",
    "matched_performers = PerformerMatcher.match_all_performers(df_sample_scene['culture_extractor_performers'], df_sample_scene['stashdb_scene']['performers'])\n",
    "\n",
    "for ce_performer, stashdb_performer in matched_performers:\n",
    "    print(ce_performer)\n",
    "    print(stashdb_performer)\n",
    "    \n",
    "    # refreshed_performer = stash.find_performers({ 'stash_id_endpoint': { 'endpoint': 'https://stashdb.org/graphql', 'stash_id': stashdb_performer['performer']['id'], 'modifier': 'EQUALS' } })\n",
    "    # if len(refreshed_performer) == 0:\n",
    "    #     print(f\"No performer found with id {stashdb_performer['performer']['id']}\")\n",
    "    #     continue\n",
    "    # if len(refreshed_performer) > 1:\n",
    "    #     print(f\"Multiple performers found with id {stashdb_performer['performer']['id']}\")\n",
    "    #     continue\n",
    "    # \n",
    "    # refreshed_performer = refreshed_performer[0]\n",
    "    # print(refreshed_performer)\n",
    "    # print(refreshed_performer['stash_ids'])\n",
    "    # \n",
    "    # # Check if performer already has Culture Extractor stash ID\n",
    "    # has_ce_stash_id = any(\n",
    "    #     stash_id['endpoint'] == 'https://culture.extractor/graphql' and \n",
    "    #     stash_id['stash_id'] == ce_performer['uuid']\n",
    "    #     for stash_id in refreshed_performer['stash_ids']\n",
    "    # )\n",
    "    # if not has_ce_stash_id:\n",
    "    #     new_stash_ids = refreshed_performer['stash_ids'] + [{ 'endpoint': 'https://culture.extractor/graphql', 'stash_id': ce_performer['uuid'] }]\n",
    "    #     stash.update_performer({ 'id': refreshed_performer['id'], 'stash_ids': new_stash_ids })\n",
    "    #     print(f\"Updated performer {refreshed_performer['name']} with Culture Extractor stash ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database schema information\n",
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query to get table information\n",
    "    table_query = \"\"\"\n",
    "        SELECT DISTINCT table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' \n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_name;\n",
    "    \"\"\"\n",
    "    cursor.execute(table_query)\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    print(\"Database Schema:\")\n",
    "    for (table_name,) in tables:\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "        \n",
    "        # Get column information for each table\n",
    "        column_query = \"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                column_default,\n",
    "                is_nullable,\n",
    "                character_maximum_length\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = 'public'\n",
    "            AND table_name = %s\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\"\n",
    "        cursor.execute(column_query, (table_name,))\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        for column in columns:\n",
    "            column_name, data_type, default, nullable, max_length = column\n",
    "            nullable_str = \"NULL\" if nullable == 'YES' else \"NOT NULL\"\n",
    "            default_str = f\"DEFAULT {default}\" if default else \"\"\n",
    "            length_str = f\"({max_length})\" if max_length else \"\"\n",
    "            print(f\"  {column_name}: {data_type}{length_str} {nullable_str} {default_str}\")\n",
    "\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query with COALESCE to handle -infinity dates\n",
    "    query = \"\"\"\n",
    "        SELECT sites.name AS site_name,\n",
    "               releases.uuid AS release_uuid,\n",
    "               COALESCE(NULLIF(releases.release_date, '-infinity'), '1970-01-01'::date) as release_date,\n",
    "               releases.short_name AS release_short_name, \n",
    "               releases.name AS release_name, \n",
    "               releases.url AS release_url,\n",
    "               releases.json_document AS release_json_document,\n",
    "               downloads.uuid AS downloads_uuid,\n",
    "               downloads.file_type,\n",
    "               downloads.content_type,\n",
    "               downloads.saved_filename,\n",
    "               downloads.file_metadata\n",
    "        FROM releases\n",
    "        JOIN sites ON releases.site_uuid = sites.uuid\n",
    "        JOIN downloads ON releases.uuid = downloads.release_uuid\n",
    "        WHERE file_type = 'zip' AND content_type = 'gallery' AND sites.name = 'Femjoy'\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    matched_performers = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_releases = pd.DataFrame(matched_performers, columns=column_names)\n",
    "    # df_releases['sha256'] = df_releases['file_metadata'].apply(lambda x: x['sha256Sum'] if isinstance(x, dict) and 'sha256Sum' in x else None)\n",
    "    # df_releases['oshash'] = df_releases['file_metadata'].apply(lambda x: x['oshash'] if isinstance(x, dict) and 'oshash' in x else None)\n",
    "    \n",
    "    \n",
    "    df_releases = df_releases.copy().add_prefix('culture_extractor_')\n",
    "\n",
    "# Extract model names from json_document\n",
    "def extract_model_names(json_doc):\n",
    "    try:\n",
    "        if isinstance(json_doc, str):\n",
    "            import json\n",
    "            json_doc = json.loads(json_doc)\n",
    "            \n",
    "        if isinstance(json_doc, dict) and 'models' in json_doc:\n",
    "            return [model['name'] for model in json_doc['models']]\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Add new column with model names\n",
    "df_releases['culture_extractor_models'] = df_releases['culture_extractor_release_json_document'].apply(extract_model_names)\n",
    "\n",
    "# If you want the names as a comma-separated string instead of a list\n",
    "df_releases['culture_extractor_models_string'] = df_releases['culture_extractor_models'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "df_releases = df_releases[['culture_extractor_release_uuid', 'culture_extractor_release_date', 'culture_extractor_release_name', 'culture_extractor_models', 'culture_extractor_models_string']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_uuid = '01928448-5251-716f-8499-d5c7a99a1e48'\n",
    "df_releases[df_releases['culture_extractor_release_uuid'].astype(str) == target_uuid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy culture_extractor_release_uuid from releases which have a specific model name\n",
    "model_name = \"Marria Leeah\"\n",
    "df_releases_with_model_name = df_releases[df_releases['culture_extractor_models_string'].str.contains(model_name)]\n",
    "df_releases_with_model_name = df_releases_with_model_name[['culture_extractor_release_uuid', 'culture_extractor_release_name', 'culture_extractor_models_string']]\n",
    "df_releases_with_model_name.columns = ['release_uuid', 'release_name', 'model_name']\n",
    "\n",
    "df_releases_with_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_releases_with_model_name[\"release_uuid\"].to_clipboard(index=False, header=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query\n",
    "    query = \"\"\"\n",
    "        SELECT sub_sites.*, sites.name AS site_name\n",
    "        FROM sub_sites\n",
    "        JOIN sites ON sub_sites.site_uuid = sites.uuid\n",
    "        ORDER BY sub_sites.name\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    matched_performers = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_subsites = pd.DataFrame(matched_performers, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.client_stashapp import get_stashapp_client\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "\n",
    "def get_parent_studio_id(studio):\n",
    "    if studio[\"parent_studio\"] is not None:\n",
    "        return studio[\"parent_studio\"][\"id\"]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "studios = stash.find_studios({})\n",
    "df_stash_studios = pd.DataFrame(studios)\n",
    "df_stash_studios[\"parent_studio_id\"] = df_stash_studios.apply(get_parent_studio_id, axis=1)\n",
    "\n",
    "# Lookup functions\n",
    "def get_studio_by_id(studio_id):\n",
    "    return df_stash_studios[df_stash_studios[\"id\"] == studio_id]\n",
    "\n",
    "def get_studio_by_name(studio_name):\n",
    "    return df_stash_studios[df_stash_studios[\"name\"] == studio_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes with prefixed column names\n",
    "df_sites_prefixed = df_sites.add_prefix('culture_extractor_')\n",
    "df_stash_studios_prefixed = df_stash_studios.add_prefix('stash_')\n",
    "\n",
    "# Merge the prefixed dataframes\n",
    "df_matched_studios = pd.merge(df_stash_studios_prefixed, df_sites_prefixed, \n",
    "                              left_on='stash_name', \n",
    "                              right_on='culture_extractor_name', \n",
    "                              how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Culture Extractor UUID in Stash studio\n",
    "name = \"SexArt\"\n",
    "\n",
    "df_matched_studio = df_matched_studios[df_matched_studios[\"stash_name\"] == name]\n",
    "if not df_matched_studio.empty:\n",
    "    df_matched_studio = df_matched_studio.iloc[0]\n",
    "else:\n",
    "    print(f\"No studio found with name: {name}\")\n",
    "    raise ValueError(f\"No studio found with name: {name}\")\n",
    "\n",
    "refreshed_studio = stash.find_studio(name)\n",
    "assert refreshed_studio is not None, f\"No studio found with name: {name}\"\n",
    "\n",
    "stashbox_ids = refreshed_studio[\"stash_ids\"]\n",
    "culture_extractor_endpoint = \"https://culture.extractor/graphql\"\n",
    "culture_extractor_uuid = str(df_matched_studio[\"culture_extractor_uuid\"])\n",
    "existing_stash_id = next((stash_id for stash_id in stashbox_ids if stash_id[\"endpoint\"] == culture_extractor_endpoint), None)\n",
    "\n",
    "if existing_stash_id:\n",
    "    if existing_stash_id[\"stash_id\"] != culture_extractor_uuid:\n",
    "        existing_stash_id[\"stash_id\"] = culture_extractor_uuid\n",
    "        stash.update_studio({\"id\": refreshed_studio[\"id\"], \"stash_ids\": stashbox_ids})\n",
    "        print(f\"Updated stash_id for {culture_extractor_endpoint}\")\n",
    "else:\n",
    "    stashbox_ids.append({\"endpoint\": culture_extractor_endpoint, \"stash_id\": culture_extractor_uuid})\n",
    "    stash.update_studio({\"id\": refreshed_studio[\"id\"], \"stash_ids\": stashbox_ids})\n",
    "    print(f\"Added new stash_id for {culture_extractor_endpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get studio for scene matching\n",
    "stash_site_name = 'Slayed'\n",
    "\n",
    "current_studio = get_studio_by_name(stash_site_name)\n",
    "\n",
    "if isinstance(current_studio, pd.DataFrame) and not current_studio.empty:\n",
    "    current_studio = current_studio.iloc[0].to_dict()\n",
    "\n",
    "culture_extractor_site_uuid = None\n",
    "if isinstance(current_studio, dict) and \"stash_ids\" in current_studio:\n",
    "    for stash_id in current_studio[\"stash_ids\"]:\n",
    "        if isinstance(stash_id, dict) and stash_id.get(\"endpoint\") == \"https://culture.extractor/graphql\":\n",
    "            culture_extractor_site_uuid = stash_id.get(\"stash_id\")\n",
    "            break\n",
    "\n",
    "assert culture_extractor_site_uuid is not None, f\"No Culture Extractor site uuid found for {stash_site_name}\"\n",
    "print(f\"Matched Stash studio {stash_site_name} to Culture Extractor site uuid {culture_extractor_site_uuid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_uuid = \"018b8e83-e2e3-718e-966d-c4f745149c79\"\n",
    "\n",
    "with psycopg.connect(connection_string) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DELETE FROM releases WHERE uuid = %s\", (delete_uuid,))\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenes from Culture Extractor\n",
    "query_template = \"\"\"\n",
    "    SELECT\n",
    "        sites.uuid AS site_uuid,\n",
    "        sites.short_name AS site_short_name,\n",
    "        sites.name AS site_name,\n",
    "        \n",
    "        releases.uuid AS release_uuid,\n",
    "        releases.release_date AS release_date,\n",
    "        releases.short_name AS release_short_name,\n",
    "        releases.name AS release_name,\n",
    "        releases.url AS release_url,\n",
    "        releases.json_document AS release_json_document,\n",
    "        downloads.uuid AS downloads_uuid,\n",
    "        downloads.downloaded_at AS downloads_downloaded_at,\n",
    "        downloads.variant AS downloads_variant,\n",
    "        downloads.file_type AS downloads_file_type,\n",
    "        downloads.content_type AS downloads_content_type,\n",
    "        downloads.file_metadata AS downloads_file_metadata\n",
    "    FROM releases\n",
    "    JOIN sites ON releases.site_uuid = sites.uuid\n",
    "    JOIN downloads ON releases.uuid = downloads.release_uuid\n",
    "    WHERE\n",
    "        sites.uuid = '%s' AND\n",
    "        downloads.file_type = 'video' AND\n",
    "        downloads.content_type = 'scene' AND\n",
    "        (downloads.variant NOT IN ('480x270', '270p', '320p', '360p', '270p MOBILE'));\n",
    "    \"\"\"\n",
    "query = query_template % culture_extractor_site_uuid\n",
    "\n",
    "with psycopg.connect(connection_string) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    matched_performers = cursor.fetchall()\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    cursor.close()\n",
    "\n",
    "df_culture_extractor_scenes = pd.DataFrame(matched_performers, columns=column_names)\n",
    "df_culture_extractor_scenes[\"culture_extractor_duration\"] = df_culture_extractor_scenes[\"downloads_file_metadata\"].apply(lambda x: x[\"duration\"] if isinstance(x, dict) and \"duration\" in x else None)\n",
    "df_culture_extractor_scenes[\"culture_extractor_phash\"] = df_culture_extractor_scenes[\"downloads_file_metadata\"].apply(lambda x: x[\"phash\"] if isinstance(x, dict) and \"phash\" in x else None)\n",
    "df_culture_extractor_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonunique_release_short_name = df_culture_extractor_scenes[df_culture_extractor_scenes.duplicated(subset=['release_short_name'], keep=False)]\n",
    "df_nonunique_release_short_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenes from Stash\n",
    "stash_scenes = stash.find_scenes(\n",
    "    # {\n",
    "    #     \"studios\": { \"value\": [current_studio[\"id\"]], \"excludes\": [], \"modifier\": \"INCLUDES\", \"depth\": -1 }\n",
    "    # },\n",
    "    # filter={\n",
    "    #     \"per_page\": 500, \"page\": 1, \"sort\": \"path\", \"direction\": \"DESC\"\n",
    "    # },\n",
    "    fragment=\"id title code date files { id path basename fingerprints { type value } format width height video_codec frame_rate duration } studio { id name tags { id name } } performers { id name gender tags { id name} } stash_ids { endpoint stash_id } urls\"\n",
    ")\n",
    "df_stash_scenes = pd.DataFrame(stash_scenes)\n",
    "\n",
    "def get_endpoint_stash_id(stash_ids, endpoint):\n",
    "    for stash_id in stash_ids:\n",
    "        if stash_id[\"endpoint\"] == endpoint:\n",
    "            return stash_id[\"stash_id\"]\n",
    "    return None\n",
    "\n",
    "def get_tpdb_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://theporndb.net/graphql\")\n",
    "\n",
    "def get_stashdb_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://stashdb.org/graphql\")\n",
    "\n",
    "def get_culture_extractor_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://culture.extractor/graphql\")\n",
    "\n",
    "df_stash_scenes[\"date\"] = pd.to_datetime(df_stash_scenes[\"date\"])\n",
    "df_stash_scenes[\"stashdb_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_stashdb_id)\n",
    "df_stash_scenes[\"tpdb_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_tpdb_id)\n",
    "df_stash_scenes[\"culture_extractor_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_culture_extractor_id)\n",
    "df_stash_scenes[\"stash_duration\"] = df_stash_scenes[\"files\"].apply(lambda x: x[0][\"duration\"])\n",
    "df_stash_scenes[\"stash_phash\"] = df_stash_scenes[\"files\"].apply(lambda x: next((y[\"value\"] for y in x[0][\"fingerprints\"] if y[\"type\"] == \"phash\"), None))\n",
    "df_stash_scenes[\"stash_oshash\"] = df_stash_scenes[\"files\"].apply(lambda x: next((y[\"value\"] for y in x[0][\"fingerprints\"] if y[\"type\"] == \"oshash\"), None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate scenes based on StashDB ID\n",
    "df_duplicate_stash_scenes = df_stash_scenes[\n",
    "    (df_stash_scenes['stashdb_id'].notna()) &\n",
    "    (df_stash_scenes.duplicated(subset=['stashdb_id'], keep=False))\n",
    "]\n",
    "df_duplicate_stash_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the duplicate tag to the scenes\n",
    "duplicate_stashdb_ids_tag = stash.find_tag(\"StashDB: Duplicate Scenes Based On ID\")\n",
    "duplicate_stashdb_ids_tag\n",
    "\n",
    "for index, row in df_duplicate_stash_scenes.iterrows():\n",
    "    refreshed_scene = stash.find_scene(row[\"id\"])\n",
    "    existing_tag_ids = [tag[\"id\"] for tag in refreshed_scene[\"tags\"]]\n",
    "    if duplicate_stashdb_ids_tag[\"id\"] not in existing_tag_ids:\n",
    "        updated_tag_ids = existing_tag_ids + [duplicate_stashdb_ids_tag[\"id\"]]\n",
    "        stash.update_scene({ \"id\": row[\"id\"], \"tag_ids\": updated_tag_ids })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching existing scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Stash and Culture Extractor scenes based on oshash\n",
    "df_merged_by_oshash = pd.merge(df_stash_scenes, df_releases, \n",
    "                               left_on='stash_oshash', right_on='culture_extractor_oshash', \n",
    "                               how='inner')\n",
    "\n",
    "df_merged_by_oshash = df_merged_by_oshash[\n",
    "    df_merged_by_oshash['culture_extractor_id'].str.strip() != df_merged_by_oshash['culture_extractor_release_uuid'].astype(str).str.strip()\n",
    "]\n",
    "len(df_merged_by_oshash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match first x rows from df_merged_by_oshash to df_stash_scenes\n",
    "for index, row in df_merged_by_oshash[df_merged_by_oshash['culture_extractor_site_name'] == \"It's POV\"].iterrows():\n",
    "    refreshed_scene = stash.find_scene(row[\"id\"])\n",
    "\n",
    "    release_uuid = row[\"culture_extractor_release_uuid\"]\n",
    "    if pd.isnull(release_uuid):\n",
    "        continue\n",
    "\n",
    "    existing_stash_ids = refreshed_scene[\"stash_ids\"]\n",
    "    if \"https://culture.extractor/graphql\" not in [stash_id_obj[\"endpoint\"] for stash_id_obj in existing_stash_ids]:\n",
    "        existing_stash_ids.append({ \"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": str(release_uuid) })\n",
    "        id = row[\"id\"]\n",
    "        stash.update_scene({ \"id\": id, \"stash_ids\": existing_stash_ids })\n",
    "        print(f\"Updated scene {id} with Culture Extractor ID {release_uuid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Stash and Culture Extractor scenes based on phash\n",
    "df_culture_extractor_scenes['release_date'] = pd.to_datetime(df_culture_extractor_scenes['release_date'])\n",
    "\n",
    "# Merge the dataframes\n",
    "df_merged_scenes = pd.merge(df_stash_scenes, df_culture_extractor_scenes, \n",
    "                            left_on='stash_phash', right_on='culture_extractor_phash', \n",
    "                            how='left')\n",
    "\n",
    "df_merged_matched_scenes = df_merged_scenes.copy()\n",
    "df_merged_matched_scenes = df_merged_matched_scenes[df_merged_matched_scenes[\"release_uuid\"].notnull()]\n",
    "\n",
    "df_merged_matched_scenes[\"duration_difference\"] = df_merged_matched_scenes[\"stash_duration\"] - df_merged_matched_scenes[\"culture_extractor_duration\"]\n",
    "df_merged_matched_scenes[\"phash_distance\"] = df_merged_matched_scenes.apply(lambda row: calculate_hamming_distance(row[\"stash_phash\"], row[\"culture_extractor_phash\"]), axis=1)\n",
    "df_merged_matched_scenes[\"title_levenshtein\"] = df_merged_matched_scenes.apply(lambda row: levenshtein(row[\"title\"], row[\"release_name\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Stash scenes with Culture Extractor ID\n",
    "for index, row in df_merged_matched_scenes.iterrows():\n",
    "    refreshed_scene = stash.find_scene(row[\"id\"])\n",
    "\n",
    "    release_uuid = row[\"release_uuid\"]\n",
    "    if pd.isnull(release_uuid):\n",
    "        continue\n",
    "\n",
    "    existing_stash_ids = refreshed_scene[\"stash_ids\"]\n",
    "    if \"https://culture.extractor/graphql\" not in [stash_id_obj[\"endpoint\"] for stash_id_obj in existing_stash_ids]:\n",
    "        existing_stash_ids.append({ \"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": str(release_uuid) })\n",
    "        id = row[\"id\"]\n",
    "        code = row[\"release_short_name\"]\n",
    "        stash.update_scene({ \"id\": id, \"code\": code, \"stash_ids\": existing_stash_ids })\n",
    "        print(f\"Updated scene {id} with Culture Extractor ID {release_uuid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'new_urls' where 'release_url' is appended to 'urls' if not already present\n",
    "df_merged_scenes['new_urls'] = df_merged_scenes.apply(lambda row: row['urls'] + [row['release_url']] if row['release_url'] not in row['urls'] else row['urls'], axis=1)\n",
    "\n",
    "df_merged_scenes[[\"id\", \"title\", \"new_urls\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'new_urls' where 'release_url' is appended to 'urls' if not already present\n",
    "df_merged_scenes['new_stash_ids'] = df_merged_scenes.apply(lambda row:\n",
    "                                                           row['stash_ids'] + [{ \"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": str(row[\"release_uuid\"]) }] if \"https://culture.extractor/graphql\" not in [stash_id_obj[\"endpoint\"] for stash_id_obj in row['stash_ids']] else row['stash_ids'],\n",
    "                                                           axis=1)\n",
    "df_merged_scenes[[\"new_stash_ids\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_merged_scenes.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    old_urls = row[\"urls\"]\n",
    "    new_urls = row[\"new_urls\"]\n",
    "    old_urls_set = set(old_urls)\n",
    "    new_urls_set = set(new_urls)\n",
    "    if old_urls_set != new_urls_set:\n",
    "        print(row[\"title\"])\n",
    "        print(old_urls_set)\n",
    "        print(new_urls_set)\n",
    "        stash.update_scene({ \"id\": id, \"urls\": new_urls })\n",
    "\n",
    "    # stash_ids = row[\"stash_ids\"]\n",
    "    # new_stash_ids = row[\"new_stash_ids\"]\n",
    "    # stash_ids_set = set(stash_id_obj[\"stash_id\"] for stash_id_obj in row[\"stash_ids\"])\n",
    "    # new_stash_ids_set = set(stash_id_obj[\"stash_id\"] for stash_id_obj in row[\"new_stash_ids\"])\n",
    "    # if stash_ids_set != new_stash_ids_set:\n",
    "    #     print(row[\"title\"])\n",
    "    #     print(stash_ids_set)\n",
    "    #     print(new_stash_ids_set)\n",
    "    #     stash.update_scene({ \"id\": id, \"stash_ids\": new_stash_ids })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = Path(r\"F:\\Ripping\\Tickling Submission\\Metadata\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for release_dir in root_dir.iterdir():\n",
    "    if release_dir.is_dir():\n",
    "        release_uuid = release_dir.name\n",
    "        preview_image = None\n",
    "        full_scene = None\n",
    "        trailer = None\n",
    "\n",
    "        # Check each file in the release directory\n",
    "        for file in release_dir.iterdir():\n",
    "            if file.suffix.lower() == '.jpg':\n",
    "                preview_image = file.name\n",
    "            elif file.suffix.lower() in ['.wmv', '.mp4']:\n",
    "                try:\n",
    "                    file_stem = file.stem  # Get filename without extension\n",
    "                    if file_stem.endswith(release_uuid):\n",
    "                        trailer = file.name\n",
    "                    elif re.search(r'- \\d+x\\d+$', file_stem):\n",
    "                        full_scene = file.name\n",
    "                except OSError as e:\n",
    "                    print(f\"Error accessing file: {file}. Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            'release_uuid': release_uuid,\n",
    "            'preview_image': preview_image,\n",
    "            'full_scene': full_scene,\n",
    "            'trailer': trailer\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df_files = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_files, df_culture_extractor_scenes, \n",
    "                     left_on='release_uuid', \n",
    "                     right_on='release_uuid', \n",
    "                     how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get release_uuid values where full_scene is missing\n",
    "missing_full_scene = df_merged[df_merged['full_scene'].isnull()]['release_uuid'].tolist()\n",
    "\n",
    "# Format the list for VS Code breakpoint condition\n",
    "breakpoint_condition = f\"release_id in {missing_full_scene}\"\n",
    "\n",
    "print(\"VS Code breakpoint condition:\")\n",
    "print(breakpoint_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_full_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_name = \"Vixen\"\n",
    "\n",
    "studio_for_trailers = stash.find_studio(studio_name)\n",
    "scenes_for_trailers = stash.find_scenes(\n",
    "    {\n",
    "        \"studios\": {\n",
    "            \"value\": [studio_for_trailers[\"id\"]],\n",
    "            \"excludes\": [],\n",
    "            \"modifier\": \"INCLUDES\",\n",
    "            \"depth\": -1\n",
    "        }\n",
    "    },\n",
    "    fragment=\"id title code date files { id path basename fingerprints { type value } format width height video_codec frame_rate duration } studio { id name tags { id name } } performers { id name gender tags { id name} } stash_ids { endpoint stash_id } urls\"\n",
    ")\n",
    "\n",
    "df_stash_scenes_for_trailers = pd.DataFrame(scenes_for_trailers)\n",
    "df_stash_scenes_for_trailers[\"stash_oshash\"] = df_stash_scenes_for_trailers[\"files\"].apply(lambda x: next((y[\"value\"] for y in x[0][\"fingerprints\"] if y[\"type\"] == \"oshash\"), None))\n",
    "df_stash_scenes_for_trailers[\"culture_extractor_uuid\"] = df_stash_scenes_for_trailers[\"stash_ids\"].apply(lambda x: get_culture_extractor_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailer_tag = stash.find_tag(\"Trailer Associated\", create=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Source paths\n",
    "culture_extractor_trailer_base_path_d = f\"D:\\\\Ripping\\\\{studio_name}\\\\Metadata\\\\\"\n",
    "culture_extractor_trailer_base_path_f = f\"F:\\\\Ripping\\\\{studio_name}\\\\Metadata\\\\\"\n",
    "\n",
    "# Target path\n",
    "stash_trailer_base_path = \"H:\\\\Stash\\\\generated\\\\trailers\"\n",
    "\n",
    "for index, row in df_stash_scenes_for_trailers.iterrows():\n",
    "    scene_id = row[\"id\"]\n",
    "    scene_oshash = row[\"stash_oshash\"]\n",
    "    culture_extractor_uuid = row[\"culture_extractor_uuid\"]\n",
    "\n",
    "    stash_trailer_path = os.path.join(stash_trailer_base_path, f\"{scene_oshash}.mp4\")\n",
    "    if os.path.exists(stash_trailer_path):\n",
    "        refreshed_scene = stash.find_scene(scene_id)\n",
    "        existing_tag_ids = [tag[\"id\"] for tag in refreshed_scene[\"tags\"]]\n",
    "        if trailer_tag[\"id\"] not in existing_tag_ids:\n",
    "            updated_tag_ids = existing_tag_ids + [trailer_tag[\"id\"]]\n",
    "            stash.update_scene({ \"id\": scene_id, \"tag_ids\": updated_tag_ids })\n",
    "        \n",
    "        print(f\"Stash: Trailer already exists for scene {scene_id} at {stash_trailer_path}\")\n",
    "        continue\n",
    "    \n",
    "    trailer_filename_candidates = [\"trailer_2160.mp4\", \"trailer_1080.mp4\"]\n",
    "    trailer_candidate_paths_d = [os.path.join(culture_extractor_trailer_base_path_d, f\"{culture_extractor_uuid}\", filename) for filename in trailer_filename_candidates]\n",
    "    trailer_candidate_paths_f = [os.path.join(culture_extractor_trailer_base_path_f, f\"{culture_extractor_uuid}\", filename) for filename in trailer_filename_candidates]    \n",
    "    matching_trailer_path = next((path for path in trailer_candidate_paths_d + trailer_candidate_paths_f if os.path.exists(path)), None)\n",
    "    if matching_trailer_path:\n",
    "        shutil.copy(matching_trailer_path, stash_trailer_path)\n",
    "        print(f\"Copied trailer for scene {scene_id} to {stash_trailer_path}\")\n",
    "        refreshed_scene = stash.find_scene(scene_id)\n",
    "        existing_tag_ids = [tag[\"id\"] for tag in refreshed_scene[\"tags\"]]\n",
    "        if trailer_tag[\"id\"] not in existing_tag_ids:\n",
    "            updated_tag_ids = existing_tag_ids + [trailer_tag[\"id\"]]\n",
    "            stash.update_scene({ \"id\": scene_id, \"tag_ids\": updated_tag_ids })\n",
    "    else:\n",
    "        print(f\"No trailer found for scene {scene_id}\")\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
