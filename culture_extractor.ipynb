{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "user = os.environ.get(\"CE_DB_USERNAME\")\n",
    "pw = os.environ.get(\"CE_DB_PASSWORD\")\n",
    "host = os.environ.get(\"CE_DB_HOST\")\n",
    "port = os.environ.get(\"CE_DB_PORT\")\n",
    "db = os.environ.get(\"CE_DB_NAME\")\n",
    "\n",
    "connection_string = f\"dbname={db} user={user} password={pw} host={host} port={port}\"\n",
    "\n",
    "def hex_to_binary(hex_string):\n",
    "    return bin(int(hex_string, 16))[2:].zfill(64)\n",
    "\n",
    "def calculate_hamming_distance(phash1, phash2):\n",
    "    # Convert hexadecimal phashes to binary\n",
    "    binary1 = hex_to_binary(phash1)\n",
    "    binary2 = hex_to_binary(phash2)\n",
    "    \n",
    "    # Ensure both binary strings are of equal length\n",
    "    if len(binary1) != len(binary2):\n",
    "        raise ValueError(\"Binary strings must be of equal length\")\n",
    "    \n",
    "    # Calculate Hamming distance\n",
    "    return sum(c1 != c2 for c1, c2 in zip(binary1, binary2))\n",
    "\n",
    "# Example usage:\n",
    "# phash1 = \"951428607cf7cb8f\"\n",
    "# phash2 = \"951428607cf7cb8e\"\n",
    "# distance = calculate_hamming_distance(phash1, phash2)\n",
    "# print(f\"Hamming distance between {phash1} and {phash2}: {distance}\")\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    from Levenshtein import distance\n",
    "    return distance(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query\n",
    "    query = \"SELECT * FROM sites ORDER BY name\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_sites = pd.DataFrame(results, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(connection_string) as conn:\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the query\n",
    "    query = \"\"\"\n",
    "        SELECT sub_sites.*, sites.name AS site_name\n",
    "        FROM sub_sites\n",
    "        JOIN sites ON sub_sites.site_uuid = sites.uuid\n",
    "        ORDER BY sub_sites.name\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all the results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Get the column names from cursor.description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Close the cursor\n",
    "    cursor.close()\n",
    "\n",
    "    # No need to manually close the connection when using 'with'\n",
    "\n",
    "    # Convert the results to a data frame with column names\n",
    "    df_subsites = pd.DataFrame(results, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.client_stashapp import get_stashapp_client\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "\n",
    "def get_parent_studio_id(studio):\n",
    "    if studio[\"parent_studio\"] is not None:\n",
    "        return studio[\"parent_studio\"][\"id\"]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "studios = stash.find_studios({})\n",
    "df_stash_studios = pd.DataFrame(studios)\n",
    "df_stash_studios[\"parent_studio_id\"] = df_stash_studios.apply(get_parent_studio_id, axis=1)\n",
    "\n",
    "# Lookup functions\n",
    "def get_studio_by_id(studio_id):\n",
    "    return df_stash_studios[df_stash_studios[\"id\"] == studio_id]\n",
    "\n",
    "def get_studio_by_name(studio_name):\n",
    "    return df_stash_studios[df_stash_studios[\"name\"] == studio_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes with prefixed column names\n",
    "df_sites_prefixed = df_sites.add_prefix('culture_extractor_')\n",
    "df_stash_studios_prefixed = df_stash_studios.add_prefix('stash_')\n",
    "\n",
    "# Merge the prefixed dataframes\n",
    "df_matched_studios = pd.merge(df_stash_studios_prefixed, df_sites_prefixed, \n",
    "                              left_on='stash_name', \n",
    "                              right_on='culture_extractor_name', \n",
    "                              how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Culture Extractor UUID in Stash studio\n",
    "name = \"SexArt\"\n",
    "\n",
    "df_matched_studio = df_matched_studios[df_matched_studios[\"stash_name\"] == name]\n",
    "if not df_matched_studio.empty:\n",
    "    df_matched_studio = df_matched_studio.iloc[0]\n",
    "else:\n",
    "    print(f\"No studio found with name: {name}\")\n",
    "    raise ValueError(f\"No studio found with name: {name}\")\n",
    "\n",
    "refreshed_studio = stash.find_studio(name)\n",
    "assert refreshed_studio is not None, f\"No studio found with name: {name}\"\n",
    "\n",
    "stashbox_ids = refreshed_studio[\"stash_ids\"]\n",
    "culture_extractor_endpoint = \"https://culture.extractor/graphql\"\n",
    "culture_extractor_uuid = str(df_matched_studio[\"culture_extractor_uuid\"])\n",
    "existing_stash_id = next((stash_id for stash_id in stashbox_ids if stash_id[\"endpoint\"] == culture_extractor_endpoint), None)\n",
    "\n",
    "if existing_stash_id:\n",
    "    if existing_stash_id[\"stash_id\"] != culture_extractor_uuid:\n",
    "        existing_stash_id[\"stash_id\"] = culture_extractor_uuid\n",
    "        stash.update_studio({\"id\": refreshed_studio[\"id\"], \"stash_ids\": stashbox_ids})\n",
    "        print(f\"Updated stash_id for {culture_extractor_endpoint}\")\n",
    "else:\n",
    "    stashbox_ids.append({\"endpoint\": culture_extractor_endpoint, \"stash_id\": culture_extractor_uuid})\n",
    "    stash.update_studio({\"id\": refreshed_studio[\"id\"], \"stash_ids\": stashbox_ids})\n",
    "    print(f\"Added new stash_id for {culture_extractor_endpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get studio for scene matching\n",
    "stash_site_name = 'SexArt'\n",
    "\n",
    "current_studio = get_studio_by_name(stash_site_name)\n",
    "\n",
    "if isinstance(current_studio, pd.DataFrame) and not current_studio.empty:\n",
    "    current_studio = current_studio.iloc[0].to_dict()\n",
    "\n",
    "culture_extractor_site_uuid = None\n",
    "if isinstance(current_studio, dict) and \"stash_ids\" in current_studio:\n",
    "    for stash_id in current_studio[\"stash_ids\"]:\n",
    "        if isinstance(stash_id, dict) and stash_id.get(\"endpoint\") == \"https://culture.extractor/graphql\":\n",
    "            culture_extractor_site_uuid = stash_id.get(\"stash_id\")\n",
    "            break\n",
    "\n",
    "assert culture_extractor_site_uuid is not None, f\"No Culture Extractor site uuid found for {stash_site_name}\"\n",
    "print(f\"Matched Stash studio {stash_site_name} to Culture Extractor site uuid {culture_extractor_site_uuid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_uuid = \"018b8e83-e2e3-718e-966d-c4f745149c79\"\n",
    "\n",
    "with psycopg.connect(connection_string) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DELETE FROM releases WHERE uuid = %s\", (delete_uuid,))\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenes from Culture Extractor\n",
    "query_template = \"\"\"\n",
    "    SELECT\n",
    "        sites.uuid AS site_uuid,\n",
    "        sites.short_name AS site_short_name,\n",
    "        sites.name AS site_name,\n",
    "        \n",
    "        releases.uuid AS release_uuid,\n",
    "        releases.release_date AS release_date,\n",
    "        releases.short_name AS release_short_name,\n",
    "        releases.name AS release_name,\n",
    "        releases.url AS release_url,\n",
    "        releases.json_document AS release_json_document,\n",
    "        downloads.uuid AS downloads_uuid,\n",
    "        downloads.downloaded_at AS downloads_downloaded_at,\n",
    "        downloads.variant AS downloads_variant,\n",
    "        downloads.file_type AS downloads_file_type,\n",
    "        downloads.content_type AS downloads_content_type,\n",
    "        downloads.file_metadata AS downloads_file_metadata\n",
    "    FROM releases\n",
    "    JOIN sites ON releases.site_uuid = sites.uuid\n",
    "    JOIN downloads ON releases.uuid = downloads.release_uuid\n",
    "    WHERE\n",
    "        sites.uuid = '%s' AND\n",
    "        downloads.file_type = 'video' AND\n",
    "        downloads.content_type = 'scene' AND\n",
    "        (downloads.variant NOT IN ('480x270', '270p', '320p', '360p', '270p MOBILE'));\n",
    "    \"\"\"\n",
    "query = query_template % culture_extractor_site_uuid\n",
    "\n",
    "with psycopg.connect(connection_string) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    cursor.close()\n",
    "\n",
    "df_culture_extractor_scenes = pd.DataFrame(results, columns=column_names)\n",
    "df_culture_extractor_scenes[\"culture_extractor_duration\"] = df_culture_extractor_scenes[\"downloads_file_metadata\"].apply(lambda x: x[\"duration\"] if isinstance(x, dict) and \"duration\" in x else None)\n",
    "df_culture_extractor_scenes[\"culture_extractor_phash\"] = df_culture_extractor_scenes[\"downloads_file_metadata\"].apply(lambda x: x[\"phash\"] if isinstance(x, dict) and \"phash\" in x else None)\n",
    "df_culture_extractor_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonunique_release_short_name = df_culture_extractor_scenes[df_culture_extractor_scenes.duplicated(subset=['release_short_name'], keep=False)]\n",
    "df_nonunique_release_short_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scenes from Stash\n",
    "stash_scenes = stash.find_scenes(\n",
    "    {\n",
    "        \"studios\": { \"value\": [current_studio[\"id\"]], \"excludes\": [], \"modifier\": \"INCLUDES\", \"depth\": -1 }\n",
    "    },\n",
    "    # filter={\n",
    "    #     \"per_page\": 500, \"page\": 1, \"sort\": \"path\", \"direction\": \"DESC\"\n",
    "    # },\n",
    "    fragment=\"id title code date files { id path basename fingerprints { type value } format width height video_codec frame_rate duration } studio { id name tags { id name } } performers { id name gender tags { id name} } stash_ids { endpoint stash_id } urls\"\n",
    ")\n",
    "df_stash_scenes = pd.DataFrame(stash_scenes)\n",
    "\n",
    "def get_endpoint_stash_id(stash_ids, endpoint):\n",
    "    for stash_id in stash_ids:\n",
    "        if stash_id[\"endpoint\"] == endpoint:\n",
    "            return stash_id[\"stash_id\"]\n",
    "    return None\n",
    "\n",
    "def get_tpdb_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://theporndb.net/graphql\")\n",
    "\n",
    "def get_stashdb_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://stashdb.org/graphql\")\n",
    "\n",
    "def get_culture_extractor_id(stash_ids):\n",
    "    return get_endpoint_stash_id(stash_ids, \"https://culture.extractor/graphql\")\n",
    "\n",
    "df_stash_scenes[\"date\"] = pd.to_datetime(df_stash_scenes[\"date\"])\n",
    "df_stash_scenes[\"stashdb_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_stashdb_id)\n",
    "df_stash_scenes[\"tpdb_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_tpdb_id)\n",
    "df_stash_scenes[\"culture_extractor_id\"] = df_stash_scenes[\"stash_ids\"].apply(get_culture_extractor_id)\n",
    "df_stash_scenes[\"stash_duration\"] = df_stash_scenes[\"files\"].apply(lambda x: x[0][\"duration\"])\n",
    "df_stash_scenes[\"stash_phash\"] = df_stash_scenes[\"files\"].apply(lambda x: next((y[\"value\"] for y in x[0][\"fingerprints\"] if y[\"type\"] == \"phash\"), None))\n",
    "df_stash_scenes[\"stash_oshash\"] = df_stash_scenes[\"files\"].apply(lambda x: next((y[\"value\"] for y in x[0][\"fingerprints\"] if y[\"type\"] == \"oshash\"), None))\n",
    "\n",
    "df_stash_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate scenes based on StashDB ID\n",
    "df_duplicate_stash_scenes = df_stash_scenes[\n",
    "    (df_stash_scenes['stashdb_id'].notna()) &\n",
    "    (df_stash_scenes.duplicated(subset=['stashdb_id'], keep=False))\n",
    "]\n",
    "df_duplicate_stash_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the duplicate tag to the scenes\n",
    "duplicate_stashdb_ids_tag = stash.find_tag(\"StashDB: Duplicate Scenes Based On ID\")\n",
    "duplicate_stashdb_ids_tag\n",
    "\n",
    "for index, row in df_duplicate_stash_scenes.iterrows():\n",
    "    refreshed_scene = stash.find_scene(row[\"id\"])\n",
    "    existing_tag_ids = [tag[\"id\"] for tag in refreshed_scene[\"tags\"]]\n",
    "    if duplicate_stashdb_ids_tag[\"id\"] not in existing_tag_ids:\n",
    "        updated_tag_ids = existing_tag_ids + [duplicate_stashdb_ids_tag[\"id\"]]\n",
    "        stash.update_scene({ \"id\": row[\"id\"], \"tag_ids\": updated_tag_ids })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching existing scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Stash and Culture Extractor scenes based on phash\n",
    "df_culture_extractor_scenes['release_date'] = pd.to_datetime(df_culture_extractor_scenes['release_date'])\n",
    "\n",
    "# Merge the dataframes\n",
    "df_merged_scenes = pd.merge(df_stash_scenes, df_culture_extractor_scenes, \n",
    "                            left_on='stash_phash', right_on='culture_extractor_phash', \n",
    "                            how='left')\n",
    "\n",
    "df_merged_matched_scenes = df_merged_scenes.copy()\n",
    "df_merged_matched_scenes = df_merged_matched_scenes[df_merged_matched_scenes[\"release_uuid\"].notnull()]\n",
    "\n",
    "df_merged_matched_scenes[\"duration_difference\"] = df_merged_matched_scenes[\"stash_duration\"] - df_merged_matched_scenes[\"culture_extractor_duration\"]\n",
    "df_merged_matched_scenes[\"phash_distance\"] = df_merged_matched_scenes.apply(lambda row: calculate_hamming_distance(row[\"stash_phash\"], row[\"culture_extractor_phash\"]), axis=1)\n",
    "df_merged_matched_scenes[\"title_levenshtein\"] = df_merged_matched_scenes.apply(lambda row: levenshtein(row[\"title\"], row[\"release_name\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Stash scenes with Culture Extractor ID\n",
    "for index, row in df_merged_matched_scenes.iterrows():\n",
    "    refreshed_scene = stash.find_scene(row[\"id\"])\n",
    "\n",
    "    release_uuid = row[\"release_uuid\"]\n",
    "    if pd.isnull(release_uuid):\n",
    "        continue\n",
    "\n",
    "    existing_stash_ids = refreshed_scene[\"stash_ids\"]\n",
    "    if \"https://culture.extractor/graphql\" not in [stash_id_obj[\"endpoint\"] for stash_id_obj in existing_stash_ids]:\n",
    "        existing_stash_ids.append({ \"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": str(release_uuid) })\n",
    "        id = row[\"id\"]\n",
    "        code = row[\"release_short_name\"]\n",
    "        stash.update_scene({ \"id\": id, \"code\": code, \"stash_ids\": existing_stash_ids })\n",
    "        print(f\"Updated scene {id} with Culture Extractor ID {release_uuid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'new_urls' where 'release_url' is appended to 'urls' if not already present\n",
    "df_merged_scenes['new_urls'] = df_merged_scenes.apply(lambda row: row['urls'] + [row['release_url']] if row['release_url'] not in row['urls'] else row['urls'], axis=1)\n",
    "\n",
    "df_merged_scenes[[\"id\", \"title\", \"new_urls\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'new_urls' where 'release_url' is appended to 'urls' if not already present\n",
    "df_merged_scenes['new_stash_ids'] = df_merged_scenes.apply(lambda row:\n",
    "                                                           row['stash_ids'] + [{ \"endpoint\": \"https://culture.extractor/graphql\", \"stash_id\": str(row[\"release_uuid\"]) }] if \"https://culture.extractor/graphql\" not in [stash_id_obj[\"endpoint\"] for stash_id_obj in row['stash_ids']] else row['stash_ids'],\n",
    "                                                           axis=1)\n",
    "df_merged_scenes[[\"new_stash_ids\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_merged_scenes.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    old_urls = row[\"urls\"]\n",
    "    new_urls = row[\"new_urls\"]\n",
    "    old_urls_set = set(old_urls)\n",
    "    new_urls_set = set(new_urls)\n",
    "    if old_urls_set != new_urls_set:\n",
    "        print(row[\"title\"])\n",
    "        print(old_urls_set)\n",
    "        print(new_urls_set)\n",
    "        stash.update_scene({ \"id\": id, \"urls\": new_urls })\n",
    "\n",
    "    # stash_ids = row[\"stash_ids\"]\n",
    "    # new_stash_ids = row[\"new_stash_ids\"]\n",
    "    # stash_ids_set = set(stash_id_obj[\"stash_id\"] for stash_id_obj in row[\"stash_ids\"])\n",
    "    # new_stash_ids_set = set(stash_id_obj[\"stash_id\"] for stash_id_obj in row[\"new_stash_ids\"])\n",
    "    # if stash_ids_set != new_stash_ids_set:\n",
    "    #     print(row[\"title\"])\n",
    "    #     print(stash_ids_set)\n",
    "    #     print(new_stash_ids_set)\n",
    "    #     stash.update_scene({ \"id\": id, \"stash_ids\": new_stash_ids })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import new scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = Path(r\"F:\\Ripping\\Tickling Submission\\Metadata\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for release_dir in root_dir.iterdir():\n",
    "    if release_dir.is_dir():\n",
    "        release_uuid = release_dir.name\n",
    "        preview_image = None\n",
    "        full_scene = None\n",
    "        trailer = None\n",
    "\n",
    "        # Check each file in the release directory\n",
    "        for file in release_dir.iterdir():\n",
    "            if file.suffix.lower() == '.jpg':\n",
    "                preview_image = file.name\n",
    "            elif file.suffix.lower() in ['.wmv', '.mp4']:\n",
    "                try:\n",
    "                    file_stem = file.stem  # Get filename without extension\n",
    "                    if file_stem.endswith(release_uuid):\n",
    "                        trailer = file.name\n",
    "                    elif re.search(r'- \\d+x\\d+$', file_stem):\n",
    "                        full_scene = file.name\n",
    "                except OSError as e:\n",
    "                    print(f\"Error accessing file: {file}. Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            'release_uuid': release_uuid,\n",
    "            'preview_image': preview_image,\n",
    "            'full_scene': full_scene,\n",
    "            'trailer': trailer\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df_files = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_files, df_culture_extractor_scenes, \n",
    "                     left_on='release_uuid', \n",
    "                     right_on='release_uuid', \n",
    "                     how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get release_uuid values where full_scene is missing\n",
    "missing_full_scene = df_merged[df_merged['full_scene'].isnull()]['release_uuid'].tolist()\n",
    "\n",
    "# Format the list for VS Code breakpoint condition\n",
    "breakpoint_condition = f\"release_id in {missing_full_scene}\"\n",
    "\n",
    "print(\"VS Code breakpoint condition:\")\n",
    "print(breakpoint_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_full_scene)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
